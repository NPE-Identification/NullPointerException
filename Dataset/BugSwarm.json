[
    {
        "ID Number":101,
        "Patch":"public class OnDeployExecutorImpl implements OnDeployExecutor {\n        ResourceResolver resourceResolver = null;\n        Session session = null;\n        try {\n            try {\n                Map<String, Object> userParams = new HashMap<>();\n                userParams.put(ResourceResolverFactory.SUBSERVICE, \"onDeployScripts\");\n                resourceResolver = resourceResolverFactory.getServiceResourceResolver(userParams);\n            } catch (LoginException le2) {\n                logger.error(\"On-deploy scripts cannot be run because the system cannot log in with the appropriate service user\");\n                throw new OnDeployEarlyTerminationException(le2);\n            }\n            resourceResolver = logIn();\n            session = resourceResolver.adaptTo(Session.class);\n            runScripts(resourceResolver, session, scripts);\n        } finally {\n@@ -172,7 +176,7 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n    protected Node getOrCreateStatusTrackingNode(Session session, String statusNodePath) {\n        try {\n            return JcrUtil.createPath(statusNodePath, \"nt:unstructured\", \"nt:unstructured\", session, false);\n            return JcrUtil.createPath(statusNodePath, JcrConstants.NT_UNSTRUCTURED, JcrConstants.NT_UNSTRUCTURED, session, false);\n        } catch (RepositoryException re) {\n            logger.error(\"On-deploy script cannot be run because the system could not find or create the script status node: {}\", statusNodePath);\n            throw new OnDeployEarlyTerminationException(re);\n@@ -182,30 +186,41 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n    protected String getScriptStatus(ResourceResolver resourceResolver, Node statusNode, String statusNodePath) {\n        try {\n            Resource resource = resourceResolver.getResource(statusNode.getPath());\n            return resource.getValueMap().get(\"status\", (String) null);\n            return resource.getValueMap().get(SCRIPT_STATUS, (String) null);\n        } catch (RepositoryException re) {\n            logger.error(\"On-deploy script cannot be run because the system read the script status node: {}\", statusNodePath);\n            throw new OnDeployEarlyTerminationException(re);\n        }\n    }\n    protected ResourceResolver logIn() {\n        try {\n            Map<String, Object> userParams = new HashMap<>();\n            userParams.put(ResourceResolverFactory.SUBSERVICE, SERVICE_NAME);\n            return resourceResolverFactory.getServiceResourceResolver(userParams);\n        } catch (LoginException le2) {\n            logger.error(\"On-deploy scripts cannot be run because the system cannot log in with the appropriate service user\");\n            throw new OnDeployEarlyTerminationException(le2);\n        }\n    }\n    protected void runScript(ResourceResolver resourceResolver, Session session, OnDeployScript script) {\n        String statusNodePath = \"\/var\/acs-commons\/on-deploy-scripts-status\/\" + script.getClass().getName();\n        String statusNodePath = SCRIPT_STATUS_JCR_FOLDER + \"\/\" + script.getClass().getName();\n        Node statusNode = getOrCreateStatusTrackingNode(session, statusNodePath);\n        String status = getScriptStatus(resourceResolver, statusNode, statusNodePath);\n        if (status == null || status.equals(\"fail\")) {\n        if (status == null || status.equals(SCRIPT_STATUS_FAIL)) {\n            trackScriptStart(session, statusNode, statusNodePath);\n            try {\n                script.execute(resourceResolver, queryBuilder);\n                logger.info(\"On-deploy script completed successfully: {}\", statusNodePath);\n                trackScriptEnd(session, statusNode, statusNodePath, \"success\");\n                trackScriptEnd(session, statusNode, statusNodePath, SCRIPT_STATUS_SUCCESS);\n            } catch (Exception e) {\n                String errMsg = \"On-deploy script failed: \" + script.getClass().getName();\n                logger.error(errMsg, e);\n                trackScriptEnd(session, statusNode, statusNodePath, \"fail\");\n                trackScriptEnd(session, statusNode, statusNodePath, SCRIPT_STATUS_FAIL);\n                throw new OnDeployEarlyTerminationException(new RuntimeException(errMsg));\n            }\n        } else if (!status.equals(\"success\")) {\n        } else if (!status.equals(SCRIPT_STATUS_SUCCESS)) {\n            String errMsg = \"On-deploy script is already running or in an otherwise unknown state: \" + script.getClass().getName() + \" - status: \" + status;\n            logger.error(errMsg);\n            throw new OnDeployEarlyTerminationException(new RuntimeException(errMsg));",
        "Stack-trace":"public class OnDeployExecutorImpl implements OnDeployExecutor {\n        ResourceResolver resourceResolver = null;\n        Session session = null;",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":102,
        "Patch":"private final Logger logger = LoggerFactory.getLogger(this.getClass());\n    @Reference\n@@ -108,14 +119,7 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n        ResourceResolver resourceResolver = null;\n        Session session = null;\n        try {\n            try {\n                Map<String, Object> userParams = new HashMap<>();\n                userParams.put(ResourceResolverFactory.SUBSERVICE, \"onDeployScripts\");\n                resourceResolver = resourceResolverFactory.getServiceResourceResolver(userParams);\n            } catch (LoginException le2) {\n                logger.error(\"On-deploy scripts cannot be run because the system cannot log in with the appropriate service user\");\n                throw new OnDeployEarlyTerminationException(le2);\n            }\n            resourceResolver = logIn();\n            session = resourceResolver.adaptTo(Session.class);\n            runScripts(resourceResolver, session, scripts);\n        } finally {\n@@ -172,7 +176,7 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n    protected Node getOrCreateStatusTrackingNode(Session session, String statusNodePath) {\n        try {\n            return JcrUtil.createPath(statusNodePath, \"nt:unstructured\", \"nt:unstructured\", session, false);\n            return JcrUtil.createPath(statusNodePath, JcrConstants.NT_UNSTRUCTURED, JcrConstants.NT_UNSTRUCTURED, session, false);\n        } catch (RepositoryException re) {\n            logger.error(\"On-deploy script cannot be run because the system could not find or create the script status node: {}\", statusNodePath);\n            throw new OnDeployEarlyTerminationException(re);\n@@ -182,30 +186,41 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n    protected String getScriptStatus(ResourceResolver resourceResolver, Node statusNode, String statusNodePath) {\n        try {\n            Resource resource = resourceResolver.getResource(statusNode.getPath());\n            return resource.getValueMap().get(\"status\", (String) null);\n            return resource.getValueMap().get(SCRIPT_STATUS, (String) null);\n        } catch (RepositoryException re) {\n            logger.error(\"On-deploy script cannot be run because the system read the script status node: {}\", statusNodePath);\n            throw new OnDeployEarlyTerminationException(re);\n        }\n    }\n    protected ResourceResolver logIn() {\n        try {\n            Map<String, Object> userParams = new HashMap<>();\n            userParams.put(ResourceResolverFactory.SUBSERVICE, SERVICE_NAME);\n            return resourceResolverFactory.getServiceResourceResolver(userParams);\n        } catch (LoginException le2) {\n            logger.error(\"On-deploy scripts cannot be run because the system cannot log in with the appropriate service user\");\n            throw new OnDeployEarlyTerminationException(le2);\n        }\n    }\n    protected void runScript(ResourceResolver resourceResolver, Session session, OnDeployScript script) {\n        String statusNodePath = \"\/var\/acs-commons\/on-deploy-scripts-status\/\" + script.getClass().getName();\n        String statusNodePath = SCRIPT_STATUS_JCR_FOLDER + \"\/\" + script.getClass().getName();\n        Node statusNode = getOrCreateStatusTrackingNode(session, statusNodePath);\n        String status = getScriptStatus(resourceResolver, statusNode, statusNodePath);\n        if (status == null || status.equals(\"fail\")) {\n        if (status == null || status.equals(SCRIPT_STATUS_FAIL)) {\n            trackScriptStart(session, statusNode, statusNodePath);\n            try {\n                script.execute(resourceResolver, queryBuilder);\n                logger.info(\"On-deploy script completed successfully: {}\", statusNodePath);\n                trackScriptEnd(session, statusNode, statusNodePath, \"success\");\n                trackScriptEnd(session, statusNode, statusNodePath, SCRIPT_STATUS_SUCCESS);\n            } catch (Exception e) {\n                String errMsg = \"On-deploy script failed: \" + script.getClass().getName();\n                logger.error(errMsg, e);\n                trackScriptEnd(session, statusNode, statusNodePath, \"fail\");\n                trackScriptEnd(session, statusNode, statusNodePath, SCRIPT_STATUS_FAIL);\n                throw new OnDeployEarlyTerminationException(new RuntimeException(errMsg));\n            }\n        } else if (!status.equals(\"success\")) {\n        } else if (!status.equals(SCRIPT_STATUS_SUCCESS)) {\n            String errMsg = \"On-deploy script is already running or in an otherwise unknown state: \" + script.getClass().getName() + \" - status: \" + status;\n            logger.error(errMsg);\n            throw new OnDeployEarlyTerminationException(new RuntimeException(errMsg));\n@@ -222,8 +237,8 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n    protected void trackScriptEnd(Session session, Node statusNode, String statusNodePath, String status) {\n        try {\n            statusNode.setProperty(\"status\", status);\n            statusNode.setProperty(\"endDate\", Calendar.getInstance());\n            statusNode.setProperty(SCRIPT_STATUS, status);\n            statusNode.setProperty(SCRIPT_DATE_END, Calendar.getInstance());\n            session.save();\n        } catch (RepositoryException e) {\n            logger.error(\"On-deploy script status node could not be updated: {} - status: {}\", statusNodePath, status);\n@@ -234,9 +249,9 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n    protected void trackScriptStart(Session session, Node statusNode, String statusNodePath) {\n        logger.info(\"Starting on-deploy script: {}\", statusNodePath);\n        try {\n            statusNode.setProperty(\"status\", \"running\");\n            statusNode.setProperty(\"startDate\", Calendar.getInstance());\n            statusNode.setProperty(\"endDate\", (Calendar) null);\n            statusNode.setProperty(SCRIPT_STATUS, SCRIPT_STATUS_RUNNING);\n            statusNode.setProperty(SCRIPT_DATE_START, Calendar.getInstance());\n            statusNode.setProperty(SCRIPT_DATE_END, (Calendar) null);\n            session.save();\n        } catch (RepositoryException e) {\n            logger.error(\"On-deploy script cannot be run because the system could not write to the script status node: {}\", statusNodePath);",
        "Stack-trace":" public class OnDeployExecutorImpl implements OnDeployExecutor {\n        ResourceResolver resourceResolver = null;\n        Session session = null;",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":103,
        "Patch":"public class OnDeployExecutorImpl implements OnDeployExecutor {\n    protected void trackScriptEnd(Session session, Node statusNode, String statusNodePath, String status) {\n        try {\n            statusNode.setProperty(\"status\", status);\n            statusNode.setProperty(\"endDate\", Calendar.getInstance());\n            statusNode.setProperty(SCRIPT_STATUS, status);\n            statusNode.setProperty(SCRIPT_DATE_END, Calendar.getInstance());\n            session.save();\n        } catch (RepositoryException e) {\n            logger.error(\"On-deploy script status node could not be updated: {} - status: {}\", statusNodePath, status);\n@@ -234,9 +249,9 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n    protected void trackScriptStart(Session session, Node statusNode, String statusNodePath) {\n        logger.info(\"Starting on-deploy script: {}\", statusNodePath);\n        try {\n            statusNode.setProperty(\"status\", \"running\");\n            statusNode.setProperty(\"startDate\", Calendar.getInstance());\n            statusNode.setProperty(\"endDate\", (Calendar) null);\n            statusNode.setProperty(SCRIPT_STATUS, SCRIPT_STATUS_RUNNING);\n            statusNode.setProperty(SCRIPT_DATE_START, Calendar.getInstance());\n            statusNode.setProperty(SCRIPT_DATE_END, (Calendar) null);\n            session.save();\n        } catch (RepositoryException e) {\n            logger.error(\"On-deploy script cannot be run because the system could not write to the script status node: {}\", statusNodePath);",
        "Stack-trace":" statusNode.setProperty(\"endDate\", (Calendar) null);\n            statusNode.setProperty(SCRIPT_STATUS, SCRIPT_STATUS_RUNNING);\n            statusNode.setProperty(SCRIPT_DATE_START, Calendar.getInstance());\n            statusNode.setProperty(SCRIPT_DATE_END, (Calendar) null);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":104,
        "Patch":"package com.adobe.acs.commons.ondeploy.impl;\nimport com.adobe.acs.commons.ondeploy.OnDeployExecutor;\nimport com.adobe.acs.commons.ondeploy.OnDeployScript;\nimport com.day.cq.commons.jcr.JcrConstants;\nimport com.day.cq.commons.jcr.JcrUtil;\nimport com.day.cq.search.QueryBuilder;\nimport org.apache.commons.lang3.StringUtils;\n@@ -81,6 +82,16 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n    )\n    private static final String PROP_SCRIPTS = \"scripts\";\n    private static final String SCRIPT_DATE_END = \"endDate\";\n    private static final String SCRIPT_DATE_START = \"startDate\";\n    private static final String SCRIPT_STATUS = \"status\";\n    private static final String SCRIPT_STATUS_JCR_FOLDER = \"\/var\/acs-commons\/on-deploy-scripts-status\";\n    private static final String SCRIPT_STATUS_FAIL = \"fail\";\n    private static final String SCRIPT_STATUS_RUNNING = \"running\";\n    private static final String SCRIPT_STATUS_SUCCESS = \"success\";\n    private static final String SERVICE_NAME = \"on-deploy-scripts\";\n    private final Logger logger = LoggerFactory.getLogger(this.getClass());\n    @Reference\n@@ -108,14 +119,7 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n        ResourceResolver resourceResolver = null;\n        Session session = null;\n        try {\n            try {\n                Map<String, Object> userParams = new HashMap<>();\n                userParams.put(ResourceResolverFactory.SUBSERVICE, \"onDeployScripts\");\n                resourceResolver = resourceResolverFactory.getServiceResourceResolver(userParams);\n            } catch (LoginException le2) {\n                logger.error(\"On-deploy scripts cannot be run because the system cannot log in with the appropriate service user\");\n                throw new OnDeployEarlyTerminationException(le2);\n            }\n            resourceResolver = logIn();\n            session = resourceResolver.adaptTo(Session.class);\n            runScripts(resourceResolver, session, scripts);\n        } finally {\n@@ -172,7 +176,7 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n    protected Node getOrCreateStatusTrackingNode(Session session, String statusNodePath) {\n        try {\n            return JcrUtil.createPath(statusNodePath, \"nt:unstructured\", \"nt:unstructured\", session, false);\n            return JcrUtil.createPath(statusNodePath, JcrConstants.NT_UNSTRUCTURED, JcrConstants.NT_UNSTRUCTURED, session, false);\n        } catch (RepositoryException re) {\n            logger.error(\"On-deploy script cannot be run because the system could not find or create the script status node: {}\", statusNodePath);\n            throw new OnDeployEarlyTerminationException(re);\n@@ -182,30 +186,41 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n    protected String getScriptStatus(ResourceResolver resourceResolver, Node statusNode, String statusNodePath) {\n        try {\n            Resource resource = resourceResolver.getResource(statusNode.getPath());\n            return resource.getValueMap().get(\"status\", (String) null);\n            return resource.getValueMap().get(SCRIPT_STATUS, (String) null);\n        } catch (RepositoryException re) {\n            logger.error(\"On-deploy script cannot be run because the system read the script status node: {}\", statusNodePath);\n            throw new OnDeployEarlyTerminationException(re);\n        }\n    }",
        "Stack-trace":"public class OnDeployExecutorImpl implements OnDeployExecutor {\n        ResourceResolver resourceResolver = null;\n        Session session = null;",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":105,
        "Patch":"private final Logger logger = LoggerFactory.getLogger(this.getClass());\n    @Reference\n@@ -108,14 +119,7 @@ public class OnDeployExecutorImpl implements OnDeployExecutor {\n        ResourceResolver resourceResolver = null;\n        Session session = null;\n        try {\n            try {\n                Map<String, Object> userParams = new HashMap<>();\n                userParams.put(ResourceResolverFactory.SUBSERVICE, \"onDeployScripts\");\n                resourceResolver = resourceResolverFactory.getServiceResourceResolver(userParams);\n            } catch (LoginException le2) {\n                logger.error(\"On-deploy scripts cannot be run because the system cannot log in with the appropriate service user\");\n                throw new OnDeployEarlyTerminationException(le2);\n            }\n            resourceResolver = logIn();\n            session = resourceResolver.adaptTo(Session.class);\n            runScripts(resourceResolver, session, scripts);\n        } finally {",
        "Stack-trace":"public class OnDeployExecutorImpl implements OnDeployExecutor {\n        ResourceResolver resourceResolver = null;\n        Session session = null;\n        try {\n            try {\n                Map<String, Object> userParams = new HashMap<>();",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":106,
        "Patch":"public class ProcessInstanceImpl implements ProcessInstance, Serializable {\n                });\n            }\n            batch.commitBatch();\n        } catch (RepositoryException | PersistenceException | LoginException ex) {\n        } catch (RepositoryException | PersistenceException | LoginException | NullPointerException ex) {\n            LOG.error(\"Unable to record errors\", ex);\n        }\n    }",
        "Stack-trace":"public class ProcessInstanceImpl implements ProcessInstance, Serializable {\n                });\n            }\n            batch.commitBatch();\n        } catch (RepositoryException | PersistenceException | LoginException ex) {\n        } catch (RepositoryException | PersistenceException | LoginException | NullPointerException ex)",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1083,
        "Patch":"public class UrlAssetImport extends AssetIngestor {\n        FileOrRendition file = new FileOrRendition(this::getHttpClient, name, source, folder, assetData);\n        file.setAsRenditionOfImage(\n                assetData.get(RENDITION_NAME).toString(),\n                assetData.get(ORIGINAL_FILE_NAME).toString()\n                assetData.get(RENDITION_NAME) == null ? null : assetData.get(RENDITION_NAME).toString(),\n                assetData.get(ORIGINAL_FILE_NAME) == null ? null : assetData.get(ORIGINAL_FILE_NAME).toString()\n        );\n        return file;\n    }\n    private String getTargetFolder(Map<String, CompositeVariant> assetData) {\n        String target = assetData.get(TARGET_FOLDER).toString();\n        String target = assetData.get(TARGET_FOLDER) == null ? null : assetData.get(TARGET_FOLDER).toString();\n        if (target == null || target.isEmpty()) {\n            return UNKNOWN_TARGET_FOLDER;\n        } else if (!target.startsWith(CONTENT_BASE)) {",
        "Stack-trace":"file.setAsRenditionOfImage(\n                assetData.get(RENDITION_NAME).toString(),\n                assetData.get(ORIGINAL_FILE_NAME).toString()\n                assetData.get(RENDITION_NAME) == null ? null : assetData.get(RENDITION_NAME).toString(),\n                assetData.get(ORIGINAL_FILE_NAME) == null ? null : assetData.get(ORIGINAL_FILE_NAME).toString()",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1111,
        "Patch":"import org.powermock.modules.junit4.PowerMockRunner;\n *\/\n@RunWith(PowerMockRunner.class)\npublic class ReorganizerTest {\n\/\/    @Test(expected = RepositoryException.class)\n    ReorganizerFactory factory = new ReorganizerFactory();\n    Reorganizer tool;\n    ProcessInstanceImpl instance;\n    ReplicatorQueue queue;\n    ResourceResolver rr;\n    @Before\n    public void setup() throws RepositoryException, PersistenceException, IllegalAccessException, LoginException {\n        queue = spy(new ReplicatorQueue());        \n        factory.setReplicator(queue);\n        tool = prepareProcessDefinition(factory.createProcessDefinition(), null);\n        instance = prepareProocessInstance(\n                new ProcessInstanceImpl(getControlledProcessManager(), tool, \"relocator test\")\n        );\n        rr = getEnhancedMockResolver();\n        when(rr.hasChanges()).thenReturn(true);\n    }\n    @Test(expected = RepositoryException.class)\n    public void testRequiredFields() throws LoginException, DeserializeException, RepositoryException, PersistenceException {\n        ResourceResolver rr = getEnhancedMockResolver();\n        Reorganizer tool = new ReorganizerFactory().createProcessDefinition();\n@@ -78,14 +99,8 @@ public class ReorganizerTest {\n        fail(\"That should have thrown an error\");\n    }\n\/\/    @Test\n    @Test\n    public void barebonesRun() throws LoginException, DeserializeException, RepositoryException, PersistenceException, IllegalAccessException {\n        final ResourceResolver rr = getEnhancedMockResolver();\n        Reorganizer tool = prepareProcessDefinition(new ReorganizerFactory().createProcessDefinition(), null);\n        ProcessInstanceImpl instance = prepareProocessInstance(\n                new ProcessInstanceImpl(getControlledProcessManager(), tool, \"relocator test\")\n        );\n        assertEquals(\"Reorganizer: relocator test\", instance.getName());\n        Map<String, Object> values = new HashMap<>();\n        values.put(\"sourceJcrPath\", \"\/content\/folderA\");\n@@ -99,17 +114,6 @@ public class ReorganizerTest {\n    @Test\n    public void noPublishTest() throws LoginException, DeserializeException, RepositoryException, PersistenceException, Exception {\n        final ResourceResolver rr = getEnhancedMockResolver();\n        ReorganizerFactory factory = new ReorganizerFactory();\n        ReplicatorQueue queue = spy(new ReplicatorQueue());\n        factory.setReplicator(queue);\n        Reorganizer tool = prepareProcessDefinition(factory.createProcessDefinition(), null);\n        ProcessInstanceImpl instance = prepareProocessInstance(\n                new ProcessInstanceImpl(getControlledProcessManager(), tool, \"relocator test\")\n        );\n        assertEquals(\"Reorganizer: relocator test\", instance.getName());\n        Map<String, Object> values = new HashMap<>();\n        values.put(\"sourceJcrPath\", \"\/content\/folderA\");\n@@ -119,18 +123,11 @@ public class ReorganizerTest {\n        instance.init(rr, values);\n        instance.run(rr);\n        assertTrue(\"Should unpublish the source folder\", queue.getDeactivateOperations().containsKey(\"\/content\/folderA\"));\n        assertTrue(queue.getActivateOperations().isEmpty());   \n        assertTrue(\"Should publish the moved source folder\", queue.getActivateOperations().containsKey(\"\/content\/republishA\"));\n    }    \n\/\/    @Test\n    @Test\n    public void testHaltingScenario() throws DeserializeException, LoginException, RepositoryException, InterruptedException, ExecutionException, PersistenceException, IllegalAccessException {\n        final ResourceResolver rr = getEnhancedMockResolver();\n        Reorganizer tool = prepareProcessDefinition(new ReorganizerFactory().createProcessDefinition(), null);\n        ProcessInstanceImpl instance = prepareProocessInstance(\n                new ProcessInstanceImpl(getControlledProcessManager(), tool, \"relocator test\")\n        );\n        assertEquals(\"Reorganizer: relocator test\", instance.getName());\n        Map<String, Object> values = new HashMap<>();\n        values.put(\"sourceJcrPath\", \"\/content\/folderA\");",
        "Stack-trace":" factory.setReplicator(queue);\n        tool = prepareProcessDefinition(factory.createProcessDefinition(), null);\n        instance = prepareProocessInstance(\n                new ProcessInstanceImpl(getControlledProcessManager(), tool, \"relocator test\")",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1129,
        "Patch":"import java.util.function.BiConsumer;\n@SuppressWarnings(\"squid:S00112\")\npublic interface CheckedBiConsumer<T, U> {\n    public static <T,U> CheckedBiConsumer<T,U> from(BiConsumer<T,U> handler) {\n        return (t, u) -> handler.accept(t, u);\n    static <T,U> CheckedBiConsumer<T,U> from(BiConsumer<T,U> handler) {\n        return handler == null ? null : (t, u) -> handler.accept(t, u);\n    }\n    \/**\n+3\u0095bundle\/src\/main\/java\/com\/adobe\/acs\/commons\/functions\/CheckedBiFunction.java\n@@ -36,6 +36,9 @@ import aQute.bnd.annotation.ConsumerType;\n@FunctionalInterface\n@SuppressWarnings(\"squid:S00112\")\npublic interface CheckedBiFunction<T, U, R> {\n    static <T,U,R> CheckedBiFunction<T, U, R> from(java.util.function.BiFunction<T,U,R> function) {\n        return function == null ? null : (t, u) -> function.apply(t, u);\n    }\n    \/**\n     * Applies this function to the given arguments.\n+3\u0095bundle\/src\/main\/java\/com\/adobe\/acs\/commons\/functions\/CheckedConsumer.java\n@@ -33,6 +33,9 @@ import aQute.bnd.annotation.ConsumerType;\n@FunctionalInterface\n@SuppressWarnings(\"squid:S00112\")\npublic interface CheckedConsumer<T> {\n    static <T> CheckedConsumer<T> from(java.util.function.Consumer<T> consumer) {\n        return consumer == null ? null : t -> consumer.accept(t);\n    }\n    void accept(T t) throws Exception;\n+1, -1\u0095bundle\/src\/main\/java\/com\/adobe\/acs\/commons\/functions\/CheckedFunction.java\n@@ -35,7 +35,7 @@ import java.util.function.Function;\npublic interface CheckedFunction<T, R> {\n    public static <T,R> CheckedFunction<T,R> from(Function<T,R> function) {\n        return t -> function.apply(t);\n        return function == null ? null : t -> function.apply(t);\n    }\n    \/**\n+2, -2\u0095bundle\/src\/main\/java\/com\/adobe\/acs\/commons\/mcp\/impl\/processes\/BrokenLinksReport.java\n@@ -132,8 +132,8 @@ public class BrokenLinksReport extends ProcessDefinition implements Serializable\n    public void buildReport(ActionManager manager) {\n        TreeFilteringResourceVisitor visitor = new TreeFilteringResourceVisitor();\n        visitor.setBreadthFirstMode();\n        visitor.setTraversalFilter(null);\n        visitor.setResourceVisitor((resource, depth) -> {\n        visitor.setTraversalFilterChecked(null);\n        visitor.setResourceVisitorChecked((resource, depth) -> {\n            manager.deferredWithResolver(rr -> {\n                Map<String, List<String>> brokenRefs = collectBrokenReferences(resource, regex, excludeList, deepCheckList);\n                for(Map.Entry<String, List<String>> ref : brokenRefs.entrySet()){\nShowing 1 to 180 of 3,373 entries",
        "Stack-trace":" public static <T,U> CheckedBiConsumer<T,U> from(BiConsumer<T,U> handler) {\n        return (t, u) -> handler.accept(t, u);\n    static <T,U> CheckedBiConsumer<T,U> from(BiConsumer<T,U> handler) {\n        return handler == null ? null : (t, u) -> handler.accept(t, u);\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1165,
        "Patch":"public abstract class AssetIngestor extends ProcessDefinition {\n                }\n                Asset asset = assetManager.createAsset(assetPath, source.getStream(), type, false);\n                if (asset == null) {\n                    throw new RepositoryException(\"Could not create asset, see more in logs.\");\n                } else {\n                if (asset != null) {\n                    saveMigrationInfo(source, asset);\n                }\n+19, -14\u0095bundle\/src\/main\/java\/com\/adobe\/acs\/commons\/mcp\/impl\/processes\/asset\/FileAssetIngestor.java\n@@ -99,31 +99,36 @@ public class FileAssetIngestor extends AssetIngestor {\n    HierarchicalElement baseFolder;\n    @Override\n    public void init() throws RepositoryException {\n        if (fileBasePath.toLowerCase().startsWith(\"sftp:\/\/\")) {\n    public void buildProcess(ProcessInstance instance, ResourceResolver rr) throws LoginException, RepositoryException {\n        baseFolder = getBaseFolder(fileBasePath);\n        instance.getInfo().setDescription(fileBasePath + \"->\" + jcrBasePath);\n        instance.defineCriticalAction(\"Create Folders\", rr, this::createFolders);\n        instance.defineCriticalAction(\"Import Assets\", rr, this::importAssets);\n    }\n    HierarchicalElement getBaseFolder(final String url) throws RepositoryException {\n        HierarchicalElement baseHierarchicalElement;\n        if (url.toLowerCase().startsWith(\"sftp:\/\/\")) {\n            try {\n                baseFolder = new SftpHierarchicalElement(fileBasePath);\n                baseFolder.isFolder(); \/\/ Forces a login and check status of base folder\n                baseHierarchicalElement = new SftpHierarchicalElement(url);\n                \/\/ Forces a login\n                ((SftpHierarchicalElement) baseHierarchicalElement).retrieveDetails();\n            } catch (URISyntaxException | UnsupportedEncodingException ex) {\n                Logger.getLogger(FileAssetIngestor.class.getName()).log(Level.SEVERE, null, ex);\n                throw new RepositoryException(\"Unable to process URL!\");\n            } catch (JSchException | SftpException ex) {\n                Logger.getLogger(FileAssetIngestor.class.getName()).log(Level.SEVERE, null, ex);\n                throw new RepositoryException(ex.getMessage());\n            }\n        } else {\n            File base = new File(fileBasePath);\n            File base = new File(url);\n            if (!base.exists()) {\n                throw new RepositoryException(\"Source folder does not exist!\");\n            }\n            baseFolder = new FileHierarchicalElement(base);\n            baseHierarchicalElement = new FileHierarchicalElement(base);\n        }\n        super.init();\n    }\n    @Override\n    public void buildProcess(ProcessInstance instance, ResourceResolver rr) throws LoginException, RepositoryException {\n        instance.getInfo().setDescription(fileBasePath + \"->\" + jcrBasePath);\n        instance.defineCriticalAction(\"Create Folders\", rr, this::createFolders);\n        instance.defineCriticalAction(\"Import Assets\", rr, this::importAssets);\n        return baseHierarchicalElement;\n    }\n    void createFolders(ActionManager manager) throws IOException {",
        "Stack-trace":"Asset asset = assetManager.createAsset(assetPath, source.getStream(), type, false);\n                if (asset == null) {\n                    throw new RepositoryException(\"Could not create asset, see more in logs.\");\n                } else {\n                if (asset != null)",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":40,
        "Patch":"public class UserServiceImpl implements UserService {\n            return null;\n        }\n        User user = User.find.query().where().eq(\"username\", username).findOne();\n        if (user == null) {\n            throw new ServiceException(\"user:\" + username + \" auth failed!\");\n        }\n        try {\n            byte[] pass = SecurityUtil.scramble411(password.getBytes(), seeds);\n            if (!SecurityUtil.scrambleServerAuth(pass, SecurityUtil.hexStr2Bytes(user.getPassword()), seeds)) {\n@@ -35,9 +38,7 @@ public class UserServiceImpl implements UserService {\n            throw new ServiceException(\"user:\" + user.getName() + \" auth failed!\");\n        }\n        if (user != null) {\n            user.setPassword(\"\");\n        }\n        user.setPassword(\"\");\n        return user;\n    }",
        "Stack-trace":" factory.setReplicator(queue);\n        tool = prepareProcessDefinition(factory.createProcessDefinition(), null);\n        instance = prepareProocessInstance(\n                new ProcessInstanceImpl(getControlledProcessManager(), tool, \"relocator test\")",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":4751,
        "Patch":"public class DefaultJSONParser implements Closeable {\n        lexer.nextToken(JSONToken.LITERAL_STRING);\n        if (this.context != null && this.context.level > 1024) {\n            throw new JSONException(\"array level > 1024\");\n        if (this.context != null && this.context.level > 512) {\n            throw new JSONException(\"array level > 512\");\n        }\n        ParseContext context = this.context;",
        "Stack-trace":" factory.setReplicator(queue);\n        tool = prepareProcessDefinition(factory.createProcessDefinition(), null);\n        instance = prepareProocessInstance(\n                new ProcessInstanceImpl(getControlledProcessManager(), tool, \"relocator test\")",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":6075,
        "Patch":"public abstract class JSONWriter\n    public abstract void writeZonedDateTime(ZonedDateTime dateTime);\n    public abstract void writeOffsetDateTime(OffsetDateTime dateTime);\n    public void writeInstant(Instant instant) {\n        if (instant == null) {\n            writeNull();",
        "Stack-trace":" factory.setReplicator(queue);\n        tool = prepareProcessDefinition(factory.createProcessDefinition(), null);\n        instance = prepareProocessInstance(\n                new ProcessInstanceImpl(getControlledProcessManager(), tool, \"relocator test\")",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":6075,
        "Patch":"final class JSONWriterJSONB\n        }\n    }\n    @Override\n    public void writeOffsetDateTime(OffsetDateTime dateTime) {\n        if (dateTime == null) {\n            writeNull();\n            return;\n        }\n        ensureCapacity(off + 8);\n        bytes[off++] = BC_TIMESTAMP_WITH_TIMEZONE;\n        int year = dateTime.getYear();\n        bytes[off++] = (byte) (year >>> 8);\n        bytes[off++] = (byte) year;\n        bytes[off++] = (byte) dateTime.getMonthValue();\n        bytes[off++] = (byte) dateTime.getDayOfMonth();\n        bytes[off++] = (byte) dateTime.getHour();\n        bytes[off++] = (byte) dateTime.getMinute();\n        bytes[off++] = (byte) dateTime.getSecond();\n        int nano = dateTime.getNano();\n        writeInt32(nano);\n        ZoneId zoneId = dateTime.getOffset();\n        String zoneIdStr = zoneId.getId();\n        switch (zoneIdStr) {\n            case OFFSET_8_ZONE_ID_NAME:\n                writeRaw(OFFSET_8_ZONE_ID_NAME_BYTES);\n                break;\n            default:\n                writeString(zoneIdStr);\n                break;\n        }\n    }\n    @Override\n    public void writeInstant(Instant instant) {\n        if (instant == null) {",
        "Stack-trace":" factory.setReplicator(queue);\n        tool = prepareProcessDefinition(factory.createProcessDefinition(), null);\n        instance = prepareProocessInstance(\n                new ProcessInstanceImpl(getControlledProcessManager(), tool, \"relocator test\")",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":6075,
        "Patch":"import com.alibaba.fastjson2.codec.DateTimeCodec;\nimport java.lang.reflect.Type;\nimport java.time.OffsetDateTime;\nimport java.time.ZonedDateTime;\nimport java.time.format.DateTimeFormatter;\nimport java.util.Locale;\n@@ -92,8 +91,7 @@ final class ObjectWriterImplOffsetDateTime\n        }\n        if (formatter == null) {\n            ZonedDateTime zdt = odt.toZonedDateTime();\n            jsonWriter.writeZonedDateTime(zdt);\n            jsonWriter.writeOffsetDateTime(odt);\n            return;\n        }",
        "Stack-trace":"final class ObjectWriterImplOffsetDateTime\n        }\n        if (formatter == null) {\n            ZonedDateTime zdt = odt.toZonedDateTime();",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":6075,
        "Patch":"class JSONWriterUTF16\n        char firstZoneChar = '\\0';\n        int zoneSize;\n        if (\"UTC\".equals(zoneId)) {\n        if (\"UTC\".equals(zoneId) || \"Z\".equals(zoneId)) {\n            zoneId = \"Z\";\n            zoneSize = 1;\n        } else if (zoneId.length() != 0 && ((firstZoneChar = zoneId.charAt(0)) == '+' || firstZoneChar == '-')) {\n            zoneSize = zoneId.length();\n        } else {\n            zoneSize = 2 + zoneId.length();\n        }\n        len += zoneSize;\n        int yearSize = IOUtils.stringSize(year);\n        len += yearSize;\n        int small;\n        if (nano % 1000_000_000 == 0) {\n            small = 0;\n        } else if (nano % 1000_000_00 == 0) {\n            len += 2;\n            small = nano \/ 1000_000_00;\n        } else if (nano % 1000_000_0 == 0) {\n            len += 3;\n            small = nano \/ 1000_000_0;\n        } else if (nano % 1000_000 == 0) {\n            len += 4;\n            small = nano \/ 1000_000;\n        } else if (nano % 1000_00 == 0) {\n            len += 5;\n            small = nano \/ 1000_00;\n        } else if (nano % 1000_0 == 0) {\n            len += 6;\n            small = nano \/ 1000_0;\n        } else if (nano % 1000 == 0) {\n            len += 7;\n            small = nano \/ 1000;\n        } else if (nano % 100 == 0) {\n            len += 8;\n            small = nano \/ 100;\n        } else if (nano % 10 == 0) {\n            len += 9;\n            small = nano \/ 10;\n        } else {\n            len += 10;\n            small = nano;\n        }\n        ensureCapacity(off + len);\n        chars[off] = quote;\n        Arrays.fill(chars, off + 1, off + len - 1, '0');\n        IOUtils.getChars(year, off + yearSize + 1, chars);\n        chars[off + yearSize + 1] = '-';\n        IOUtils.getChars(month, off + yearSize + 4, chars);\n        chars[off + yearSize + 4] = '-';\n        IOUtils.getChars(dayOfMonth, off + yearSize + 7, chars);\n        chars[off + yearSize + 7] = 'T';\n        IOUtils.getChars(hour, off + yearSize + 10, chars);\n        chars[off + yearSize + 10] = ':';\n        IOUtils.getChars(minute, off + yearSize + 13, chars);\n        chars[off + yearSize + 13] = ':';\n        IOUtils.getChars(second, off + yearSize + 16, chars);\n        if (small != 0) {\n            chars[off + yearSize + 16] = '.';\n            IOUtils.getChars(small, off + len - 1 - zoneSize, chars);\n        }\n        if (zoneSize == 1) {\n            chars[off + len - 2] = 'Z';\n        } else if (firstZoneChar == '+' || firstZoneChar == '-') {\n            zoneId.getChars(0, zoneId.length(), chars, off + len - zoneSize - 1);\n        } else {\n            chars[off + len - zoneSize - 1] = '[';\n            zoneId.getChars(0, zoneId.length(), chars, off + len - zoneSize);\n            chars[off + len - 2] = ']';\n        }\n        chars[off + len - 1] = quote;\n        off += len;\n    }\n    @Override\n    public final void writeOffsetDateTime(OffsetDateTime dateTime) {\n        if (dateTime == null) {\n            writeNull();\n            return;\n        }\n        int year = dateTime.getYear();\n        int month = dateTime.getMonthValue();\n        int dayOfMonth = dateTime.getDayOfMonth();\n        int hour = dateTime.getHour();\n        int minute = dateTime.getMinute();\n        int second = dateTime.getSecond();\n        int nano = dateTime.getNano();\n        String zoneId = dateTime.getOffset().getId();\n        int len = 17;\n        char firstZoneChar = '\\0';\n        int zoneSize;\n        if (\"UTC\".equals(zoneId) || \"Z\".equals(zoneId)) {\n            zoneId = \"Z\";\n            zoneSize = 1;\n        } else if (zoneId.length() != 0 && ((firstZoneChar = zoneId.charAt(0)) == '+' || firstZoneChar == '-')) {",
        "Stack-trace":"  if (small != 0) {\n            chars[off + yearSize + 16] = '.';\n            IOUtils.getChars(small, off + len - 1 - zoneSize, chars);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":7471,
        "Patch":"public interface JSON {\n        }\n    }\n    \/**\n     * Parses the json byte array as {@link T}. Returns\n     * {@code null} if received byte array is {@code null} or empty.\n     *\n     * @param chars the specified chars\n     * @param objectClass the specified actual type of {@link T}\n     * @param features the specified features is applied to parsing\n     * @return {@link T} or {@code null}\n     * @throws JSONException If a parsing error occurs\n     *\/\n    @SuppressWarnings(\"unchecked\")\n    static <T> T parseObject(char[] chars, Class<T> objectClass, JSONReader.Feature... features) {\n        if (chars == null || chars.length == 0) {\n            return null;\n        }\n        try (JSONReader reader = JSONReader.of(chars)) {\n            reader.context.config(features);\n            ObjectReader<T> objectReader = reader.getObjectReader(objectClass);\n            T object = objectReader.readObject(reader, objectClass, null, 0);\n            if (reader.resolveTasks != null) {\n                reader.handleResolveTasks(object);\n            }\n            if (reader.ch != EOI && (reader.context.features & IgnoreCheckClose.mask) == 0) {\n                throw new JSONException(reader.info(\"input not end\"));\n            }\n            return object;\n        }\n    }\n    \/**\n     * Parses the json byte array as {@link T}. Returns\n     * {@code null} if received byte array is {@code null} or empty.\n     *\n     * @param chars the specified chars\n     * @param type the specified actual type of {@link T}\n     * @param features the specified features is applied to parsing\n     * @return {@link T} or {@code null}\n     * @throws JSONException If a parsing error occurs\n     *\/\n    @SuppressWarnings(\"unchecked\")\n    static <T> T parseObject(char[] chars, Type type, JSONReader.Feature... features) {\n        if (chars == null || chars.length == 0) {\n            return null;\n        }\n        try (JSONReader reader = JSONReader.of(chars)) {\n            reader.context.config(features);\n            ObjectReader<T> objectReader = reader.getObjectReader(type);\n            T object = objectReader.readObject(reader, type, null, 0);\n            if (reader.resolveTasks != null) {\n                reader.handleResolveTasks(object);\n            }\n            if (reader.ch != EOI && (reader.context.features & IgnoreCheckClose.mask) == 0) {\n                throw new JSONException(reader.info(\"input not end\"));\n            }\n            return object;\n        }\n    }\n    \/**\n     * Parses the json byte array as {@link T}. Returns\n     * {@code null} if received byte array is {@code null} or empty.",
        "Stack-trace":" static <T> T parseObject(char[] chars, Class<T> objectClass, JSONReader.Feature... features) {\n        if (chars == null || chars.length == 0) {\n            return null;\n        }\n        try (JSONReader reader = JSONReader.of(chars)) {\n            reader.context.config(features);\n            ObjectReader<T> objectReader = reader.getObjectReader(objectClass);\n            T object = objectReader.readObject(reader, objectClass, null, 0);\n            if (reader.resolveTasks != null) {\n                reader.handleResolveTasks(object);\n            }\n            if (reader.ch != EOI && (reader.context.features & IgnoreCheckClose.mask) == 0) {\n                throw new JSONException(reader.info(\"input not end\"));\n            }\n            return object;\n        }\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":7471,
        "Patch":" public abstract class JSONReader\n        return null;\n    }\n    static char char1(int c) {\n    final char char1(int c) {\n        switch (c) {\n            case '0':\n                return '\\0';\n@@ -328,9 +328,10 @@ public abstract class JSONReader\n            case '@':\n            case '(':\n            case ')':\n            case '_':\n                return (char) c;\n            default:\n                throw new JSONException(\"unclosed.str.lit \" + (char) c);\n                throw new JSONException(info(\"unclosed.str '\\\\\" + (char) c));\n        }\n    }\n@@ -1667,7 +1668,11 @@ public abstract class JSONReader\n            Object name;\n            if (match || typeRedirect) {\n                name = readFieldName();\n                if (ch >= '1' && ch <= '9') {\n                    name = null;\n                } else {\n                    name = readFieldName();\n                }\n            } else {\n                name = getFieldName();\n                match = true;",
        "Stack-trace":" if (ch >= '1' && ch <= '9') {\n                    name = null;\n                } else {\n                    name = readFieldName();\n                }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":7471,
        "Patch":"class JSONReaderASCII\n    @Override\n    public final String readFieldName() {\n        if (ch != '\"' && ch != '\\'') {\n            if ((context.features & Feature.AllowUnQuotedFieldNames.mask) != 0 && isFirstIdentifier(ch)) {\n                return readFieldNameUnquote();\n            }\n            return null;\n        }\n@@ -1495,7 +1499,7 @@ class JSONReaderASCII\n                return null;\n            }\n            default:\n                throw new JSONException(\"TODO : \" + ch);\n                throw new JSONException(info(\"illegal input : \" + ch));\n        }\n    }\n}",
        "Stack-trace":"if (ch != '\"' && ch != '\\'') {\n            if ((context.features & Feature.AllowUnQuotedFieldNames.mask) != 0 && isFirstIdentifier(ch)) {\n                return readFieldNameUnquote();\n            }\n            return null;\n        }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":7471,
        "Patch":"class JSONReaderUTF8\n        this.end = length;\n        next();\n        while (ch == '\/') {\n            next();\n            if (ch == '\/') {\n                skipLineComment();\n            } else {\n                throw new JSONException(\"input not support \" + ch + \", offset \" + offset);\n            }\n        while (ch == '\/' && this.offset < this.bytes.length && this.bytes[this.offset] == '\/') {\n            skipLineComment();\n        }\n    }\n@@ -120,13 +110,8 @@ class JSONReaderUTF8\n        this.cacheItem = null;\n        next();\n        while (ch == '\/') {\n            next();\n            if (ch == '\/') {\n                skipLineComment();\n            } else {\n                throw new JSONException(\"input not support \" + ch + \", offset \" + offset);\n            }\n        while (ch == '\/' && this.offset < this.bytes.length && this.bytes[this.offset] == '\/') {\n            skipLineComment();\n        }\n    }\n@@ -1448,6 +1433,10 @@ class JSONReaderUTF8\n    @Override\n    public String readFieldName() {\n        if (ch != '\"' && ch != '\\'') {\n            if ((context.features & Feature.AllowUnQuotedFieldNames.mask) != 0 && isFirstIdentifier(ch)) {\n                return readFieldNameUnquote();\n            }\n            return null;\n        }\n@@ -4029,7 +4018,7 @@ class JSONReaderUTF8\n                return null;\n            }\n            default:\n                throw new JSONException(\"TODO : \" + ch);\n                throw new JSONException(info(\"illegal input : \" + ch));\n        }\n    }",
        "Stack-trace":"public String readFieldName() {\n        if (ch != '\"' && ch != '\\'') {\n            if ((context.features & Feature.AllowUnQuotedFieldNames.mask) != 0 && isFirstIdentifier(ch)) {\n                return readFieldNameUnquote();\n            }\n            return null;\n        }\n@@ -4029,7 +4018,7 @@ class JSONReaderUTF8\n                return null;\n            }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":6348,
        "Patch":"public class DateUtils {\n        }\n        char[] chars = new char[19];\n        byte[] bytes = new byte[19];\n        IOUtils.writeLocalDate(bytes, 0, year, month, dayOfMonth);\n        bytes[10] = ' ';\n        IOUtils.writeLocalDate(chars, 0, year, month, dayOfMonth);\n        chars[10] = ' ';\n        IOUtils.writeLocalTime(chars, 11, hour, minute, second);\n        if (STRING_CREATOR_JDK8 != null) {\n            return STRING_CREATOR_JDK8.apply(chars, Boolean.TRUE);",
        "Stack-trace":"IOUtils.writeLocalTime(chars, 11, hour, minute, second);\n        if (STRING_CREATOR_JDK8 != null) {\n            return STRING_CREATOR_JDK8.apply(chars, Boolean.TRUE);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":6955,
        "Patch":"public class ObjectReaderImplDate\n        }\n        long millis;\n        if (useSimpleFormatter) {\n        if (useSimpleFormatter || locale != null) {\n            String str = jsonReader.readString();\n            try {\n                return new SimpleDateFormat(format).parse(str);\n                SimpleDateFormat dateFormat;\n                if (locale != null) {\n                    dateFormat = new SimpleDateFormat(format, locale);\n                } else {\n                    dateFormat = new SimpleDateFormat(format);\n                }\n                return dateFormat.parse(str);\n            } catch (ParseException e) {\n                throw new JSONException(jsonReader.info(\"parse error : \" + str), e);\n            }",
        "Stack-trace":"if (useSimpleFormatter || locale != null) {\n            String str = jsonReader.readString();\n            try {\n                return new SimpleDateFormat(format).parse(str);\n                SimpleDateFormat dateFormat;\n                if (locale != null) {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":7128,
        "Patch":"public class ObjectReaderNoneDefaultConstructor<T>\n                Object fieldValue = valueMap.get(fieldReader.fieldNameHash);\n                if (fieldValue != null) {\n                    if (paramReader != null) {\n                    if (paramReader != null && (paramReader.fieldName == null || fieldReader.fieldName == null || !paramReader.fieldName.equals(fieldReader.fieldName))) {\n                        continue;\n                    }\n                    fieldReader.accept(object, fieldValue);",
        "Stack-trace":" if (fieldValue != null) {\n                    if (paramReader != null) {\n                    if (paramReader != null && (paramReader.fieldName == null || fieldReader.fieldName == null || !paramReader.fieldName.equals(fieldReader.fieldName))) {\n                        continue;\n                    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":7128,
        "Patch":"public class ObjectWriterBaseModule\n            if (objectClass != null) {\n                Class superclass = objectClass.getSuperclass();\n                Method supperMethod = BeanUtils.getMethod(superclass, method);\n                boolean ignore = fieldInfo.ignore;\n                if (supperMethod != null) {\n                    getFieldInfo(beanInfo, fieldInfo, superclass, supperMethod);\n                    boolean ignore = fieldInfo.ignore;\n                    Field field = BeanUtils.getField(objectClass, method);\n                    int supperMethodModifiers = supperMethod.getModifiers();\n                    if (ignore != fieldInfo.ignore\n                    if (null != field && ignore != fieldInfo.ignore\n                            && !Modifier.isAbstract(supperMethodModifiers)\n                            && !supperMethod.equals(method)\n                    ) {",
        "Stack-trace":"if (objectClass != null) {\n                Class superclass = objectClass.getSuperclass();\n                Method supperMethod = BeanUtils.getMethod(superclass, method);\n                boolean ignore = fieldInfo.ignore;\n                if (supperMethod != null) {\n                    getFieldInfo(beanInfo, fieldInfo, superclass, supperMethod);\n                    boolean ignore = fieldInfo.ignore;\n                    Field field = BeanUtils.getField(objectClass, method);\n                    int supperMethodModifiers = supperMethod.getModifiers();\n                    if (ignore != fieldInfo.ignore\n                    if (null != field && ignore != fieldInfo.ignore",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":7128,
        "Patch":"public class ObjectWriterCreatorASM\n                if (!record) {\n                    BeanUtils.declaredFields(objectClass, field -> {\n                        fieldInfo.init();\n                        fieldInfo.ignore = (field.getModifiers() & Modifier.PUBLIC) == 0 || (field.getModifiers() & Modifier.TRANSIENT) != 0;\n                        fieldInfo.ignore = ((field.getModifiers() & Modifier.PUBLIC) == 0 || (field.getModifiers() & Modifier.TRANSIENT) != 0);\n                        FieldWriter fieldWriter = creteFieldWriter(objectClass, writerFieldFeatures, provider, beanInfo, fieldInfo, field);\n                        if (fieldWriter != null) {",
        "Stack-trace":"if (!record) {\n                    BeanUtils.declaredFields(objectClass, field -> {\n                        fieldInfo.init();\n                        fieldInfo.ignore = (field.getModifiers() & Modifier.PUBLIC) == 0 || (field.getModifiers() & Modifier.TRANSIENT) != 0;\n                        fieldInfo.ignore = ((field.getModifiers() & Modifier.PUBLIC) == 0 || (field.getModifiers() & Modifier.TRANSIENT) != 0);\n                        FieldWriter fieldWriter = creteFieldWriter(objectClass, writerFieldFeatures, provider, beanInfo, fieldInfo, field);\n                        if (fieldWriter != null) {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":7128,
        "Patch":"package com.alibaba.fastjson2.issues_2900;\nimport com.alibaba.fastjson2.JSONObject;\nimport com.alibaba.fastjson2.annotation.JSONField;\nimport lombok.AllArgsConstructor;\nimport lombok.Getter;\nimport lombok.Setter;\nimport static org.junit.jupiter.api.Assertions.*;\npublic class Issue2901 {\n    @Setter\n    @Getter\n    @AllArgsConstructor\n    public class User {\n        @JSONField(name = \"user_name\")\n        private String userName;\n        @Override\n        public String toString() {\n            return \"User{\" +\n                    \"userName='\" + userName + '\\'' +\n                    '}';\n        }\n    }\n    void test() {\n        String str1 = \"{\\n\" +\n                \"\\\"user_name\\\":\\\"zs\\\"\\n\" +\n                \"}\";\n        User user = JSONObject.parseObject(str1, User.class);\n        assertNotNull(user);\n        assertEquals(user.getUserName(), \"zs\");\n    }\n}",
        "Stack-trace":"void test() {\n        String str1 = \"{\\n\" +\n                \"\\\"user_name\\\":\\\"zs\\\"\\n\" +\n                \"}\";\n        User user = JSONObject.parseObject(str1, User.class);\n        assertNotNull(user);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":4777,
        "Patch":"public class JSONType_serializeFilters {\n    public void test_for_jsonField() {\n        Model m = new Model();\n        String json = JSON.toJSONString(m);\n        assertEquals(\"{\\\"id\\\":0}\", json);\n        assertEquals(\"{\\\"id\\\":123}\", json);\n    }\n    public static class MyValueFilter\n            implements ValueFilter {\n        @Override\n        public Object apply(Object object, String name, Object value) {\n            if (name.equals(\"id\") && value == null) {\n            if (name.equals(\"id\") && ((Number) value).intValue() == 0) {\n                return 123;",
        "Stack-trace":"public Object apply(Object object, String name, Object value) {\n            if (name.equals(\"id\") && value == null) {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":4970,
        "Patch":"public abstract class ObjectReaderBean<T>\n                String fieldName1 = fieldName.substring(2);\n                long hashCode64LCase = Fnv.hashCode64LCase(fieldName1);\n                FieldReader fieldReader = getFieldReaderLCase(hashCode64LCase);\n                if (fieldReader != null) {\n                if (fieldReader != null && fieldReader.fieldClass == Boolean.class) {\n                    fieldReader.readFieldValue(jsonReader, object);\n                    return;\n                }",
        "Stack-trace":"if (fieldReader != null) {\n                if (fieldReader != null && fieldReader.fieldClass == Boolean.class) {\n                    fieldReader.readFieldValue(jsonReader, object);\n                    return;\n                }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":89,
        "Patch":"public class ServerListManager implements Closeable {\n                        : \"\") + getFixedNameSuffix(serverUrls.toArray(new String[0]));\n            } else {\n                \/\/if use endpoint ,  use endpoint ,content path ,serverlist name\n                serverName = CUSTOM_NAME + \"-\" + String.join(\"_\", endpoint, String.valueOf(endpointPort), contentPath,\n                        serverListName) + (StringUtils.isNotBlank(namespace) ? (\"_\" + StringUtils.trim(namespace))\n                        : \"\");\n                serverName = CUSTOM_NAME + \"-\" + String\n                        .join(\"_\", endpoint, String.valueOf(endpointPort), contentPath, serverListName) + (\n                        StringUtils.isNotBlank(namespace) ? (\"_\" + StringUtils.trim(namespace)) : \"\");\n            }\n        }\n        serverName = serverName.replaceAll(\"\\\\\/\", \"_\");\n@@ -247,8 +260,8 @@ public class ServerListManager implements Closeable {\n            hasQueryString = true;\n        }\n        if (properties != null && properties.containsKey(PropertyKeyConst.ENDPOINT_QUERY_PARAMS)) {\n            addressServerUrlTem.append(\n                    hasQueryString ? \"&\" : \"?\" + properties.getProperty(PropertyKeyConst.ENDPOINT_QUERY_PARAMS));\n            addressServerUrlTem\n                    .append(hasQueryString ? \"&\" : \"?\" + properties.getProperty(PropertyKeyConst.ENDPOINT_QUERY_PARAMS));\n        }",
        "Stack-trace":"if (properties != null && properties.containsKey(PropertyKeyConst.ENDPOINT_QUERY_PARAMS)) {\n            addressServerUrlTem.append(\n                    hasQueryString ? \"&\" : \"?\" + properties.getProperty(PropertyKeyConst.ENDPOINT_QUERY_PARAMS));",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":81,
        "Patch":"public class NacosDiscoveryProperties {\n\t\t\t\t}\n\t\t\t\telse if (ipType == null) {\n\t\t\t\t\tip = inetUtils.findFirstNonLoopbackHostInfo().getIpAddress();\n\t\t\t\t\tmetadata.put(IPV6, inetIPv6Utils.findIPv6Address());\n\t\t\t\t\tString iPv6Address = inetIPv6Utils.findIPv6Address();\n\t\t\t\t\tif (iPv6Address != null) {\n\t\t\t\t\t\tmetadata.put(IPV6, iPv6Address);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new IllegalArgumentException(\n@@ -294,7 +297,7 @@ public class NacosDiscoveryProperties {\n\t\t\t\t\tInetAddress currentAddress = inetAddress.nextElement();\n\t\t\t\t\tif (currentAddress instanceof Inet4Address\n\t\t\t\t\t\t\t|| currentAddress instanceof Inet6Address\n\t\t\t\t\t\t\t\t\t&& !currentAddress.isLoopbackAddress()) {\n\t\t\t\t\t\t\t&& !currentAddress.isLoopbackAddress()) {\n\t\t\t\t\t\tip = currentAddress.getHostAddress();\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}",
        "Stack-trace":"else if (ipType == null) {\n\t\t\t\t\tip = inetUtils.findFirstNonLoopbackHostInfo().getIpAddress();\n\t\t\t\t\tmetadata.put(IPV6, inetIPv6Utils.findIPv6Address());\n\t\t\t\t\tString iPv6Address = inetIPv6Utils.findIPv6Address();\n\t\t\t\t\tif (iPv6Address != null) {\n\t\t\t\t\t\tmetadata.put(IPV6, iPv6Address);\n\t\t\t\t\t}",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":81,
        "Patch":"public class InetIPv6Utils {\n\tprivate InetUtils.HostInfo findFirstValidHostInfo() {\n\t\tInetAddress address = this.findFirstValidIPv6Address();\n\t\treturn this.getHostInfo(address);\n\t\treturn address != null ? this.getHostInfo(address) : null;\n\t}\n\tprivate InetAddress findFirstValidIPv6Address() {\n@@ -92,7 +92,7 @@ public class InetIPv6Utils {\n\tpublic String findIPv6Address() {\n\t\tInetUtils.HostInfo hostInfo = findFirstValidHostInfo();\n\t\treturn normalizeIPv6(hostInfo.getIpAddress());\n\t\treturn hostInfo != null ? normalizeIPv6(hostInfo.getIpAddress()) : null;\n\t}\n\tprivate String normalizeIPv6(String ip) {",
        "Stack-trace":"return address != null ? this.getHostInfo(address) : null;\n\t}\n\tprivate InetAddress findFirstValidIPv6Address() {\n@@ -92,7 +92,7 @@ public class InetIPv6Utils {\n\tpublic String findIPv6Address() {\n\t\tInetUtils.HostInfo hostInfo = findFirstValidHostInfo();\n\t\treturn normalizeIPv6(hostInfo.getIpAddress());\n\t\treturn hostInfo != null ? normalizeIPv6(hostInfo.getIpAddress()) : null;\n\t}",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":3977,
        "Patch":"public static boolean isNoneBlank(final CharSequence... css) {\n    \/**\n     * <p>Checks if all of the CharSequences are empty (\"\"), null or whitespace only.<\/p>\n     *\n     * <\/p>Whitespace is defined by {@link Character#isWhitespace(char)}.<\/p>\n     * <p>Whitespace is defined by {@link Character#isWhitespace(char)}.<\/p>\n     *\n     * <pre>\n     * StringUtils.isAllBlank(null)             = true",
        "Stack-trace":"<p>Checks if all of the CharSequences are empty (\"\"), null or whitespace only.<\/p>\n     *\n     * <\/p>Whitespace is defined by {@link Character#isWhitespace(char)}.<\/p>\n     * <p>Whitespace is defined by {@link Character#isWhitespace(char)}.<\/p>\n     *\n     * <pre>\n     * StringUtils.isAllBlank(null)             = true",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":3993,
        "Patch":"public static String toString(final Object object, final ToStringStyle style, fi\n     *            whether to include transient fields\n     * @param outputStatics\n     *            whether to include static fields\n     * @param excludeNulls\n     * @param excludeNullValues\n     *            whether to exclude fields whose values are null\n     * @param reflectUpToClass\n     *            the superclass to reflect up to (inclusive), may be <code>null<\/code>",
        "Stack-trace":"whether to exclude fields whose values are null\n     * @param reflectUpToClass\n     *            the superclass to reflect up to (inclusive), may be <code>null<\/code>",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":3993,
        "Patch":" public static String toString(final Object object, final ToStringStyle style, fi\n     *             if the Object is <code>null<\/code>\n     * \n     * @see ToStringExclude\n     * @since 2.1\n     * @since 3.6\n     *\/\n    public static <T> String toString(\n            final T object, final ToStringStyle style, final boolean outputTransients,\n            final boolean outputStatics, boolean excludeNulls, final Class<? super T> reflectUpToClass) {\n        return new ReflectionToStringBuilder(object, style, null, reflectUpToClass, outputTransients, outputStatics, excludeNulls)\n            final boolean outputStatics, boolean excludeNullValues, final Class<? super T> reflectUpToClass) {\n        return new ReflectionToStringBuilder(object, style, null, reflectUpToClass, outputTransients, outputStatics, excludeNullValues)\n                .toString();\n    }",
        "Stack-trace":"final boolean outputStatics, boolean excludeNulls, final Class<? super T> reflectUpToClass) {\n        return new ReflectionToStringBuilder(object, style, null, reflectUpToClass, outputTransients, outputStatics, excludeNulls)\n            final boolean outputStatics, boolean excludeNullValues, final Class<? super T> reflectUpToClass) {\n        return new ReflectionToStringBuilder(object, style, null, reflectUpToClass, outputTransients, outputStatics, excludeNullValues)\n                .toString();",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":3993,
        "Patch":"public ReflectionToStringBuilder(final Object object, final ToStringStyle style,\n     *            whether to include static fields\n     * @param excludeNullValues\n     *            whether to exclude fields who value is null\n     * @since 2.1\n     * @since 3.6\n     *\/\n    public <T> ReflectionToStringBuilder(\n            final T object, final ToStringStyle style, final StringBuffer buffer,\n@@ -641,7 +641,7 @@ protected void appendFieldsIn(final Class<?> clazz) {\n                    \/\/ Warning: Field.get(Object) creates wrappers objects\n                    \/\/ for primitive types.\n                    final Object fieldValue = this.getValue(field);\n                    if(!excludeNullValues || fieldValue != null){\n                    if (!excludeNullValues || fieldValue != null) {\n                        this.append(fieldName, fieldValue);\n                    }\n                } catch (final IllegalAccessException ex) {\n@@ -722,6 +722,7 @@ public boolean isAppendTransients() {\n     * <\/p>\n     *\n     * @return Whether or not to append fields whose values are null.",
        "Stack-trace":" if(!excludeNullValues || fieldValue != null){\n                    if (!excludeNullValues || fieldValue != null) {\n                        this.append(fieldName, fieldValue);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":3993,
        "Patch":" public boolean isAppendTransients() {\n     * <\/p>\n     *\n     * @return Whether or not to append fields whose values are null.\n     * @since 3.6\n     *\/\n    public boolean isExcludeNullValues() {\n        return this.excludeNullValues;\n@@ -773,6 +774,7 @@ public void setAppendTransients(final boolean appendTransients) {\n     *\n     * @param excludeNullValues\n     *            Whether or not to append fields whose values are null.\n     * @since 3.6\n     *\/\n    public void setExcludeNullValues(final boolean excludeNullValues) {\n        this.excludeNullValues = excludeNullValues;",
        "Stack-trace":"param excludeNullValues\n     *            Whether or not to append fields whose values are null.\n     * @since 3.6\n     *\/\n    public void setExcludeNullValues(final boolean excludeNullValues) {\n        this.excludeNullValues = excludeNullValues;",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":3993,
        "Patch":"package org.apache.commons.lang3.builder;\nimport static org.junit.Assert.*;\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\nimport org.junit.Test;\npublic class ReflectionToStringBuilderExcludeNullValuesTest {\n    class TestFixture{\n    static class TestFixture {\n        @SuppressWarnings(\"unused\")\n        private Integer testIntegerField;\n        @SuppressWarnings(\"unused\")\n        private String testStringField;\n        public TestFixture(Integer a, String b){\n        public TestFixture(Integer a, String b) {\n            this.testIntegerField = a;\n            this.testStringField = b;\n        }",
        "Stack-trace":"public class ReflectionToStringBuilderExcludeNullValuesTest {\n    class TestFixture{",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":4004,
        "Patch":"private static int distance(final Class<?>[] classArray, final Class<?>[] toClas\n        Validate.isTrue(annotationCls != null, \"The annotation class must not be null\");\n        List<Class<?>> classes = (searchSupers ? getAllSuperclassesAndInterfaces(cls)\n                : new ArrayList<Class<?>>());\n        classes.add(cls);\n        classes.add(0, cls);\n        final List<Method> annotatedMethods = new ArrayList<>();\n        for (Class<?> acls : classes) {\n            final Method[] methods = (ignoreAccess ? acls.getDeclaredMethods() : acls.getMethods());",
        "Stack-trace":"Validate.isTrue(annotationCls != null, \"The annotation class must not be null\");\n        List<Class<?>> classes = (searchSupers ? getAllSuperclassesAndInterfaces(cls)\n                : new ArrayList<Class<?>>());\n        classes.add(cls);\n        classes.add(0, cls);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":4036,
        "Patch":"private static final long serialVersionUID = 4954918890077093841L;\n    \/** \n    \/**\n     * Returns an immutable pair of nulls.\n     * \n     * @return an immutable pair of nulls. \n     *\n     * @param <L> the left element of this pair. Value is {@code null}.\n     * @param <R> the right element of this pair. Value is {@code null}.\n     * @return an immutable pair of nulls.\n     * @since 3.6\n     *\/\n    @SuppressWarnings(\"unchecked\")",
        "Stack-trace":" Returns an immutable pair of nulls.\n     * \n     * @return an immutable pair of nulls. \n     *\n     * @param <L> the left element of this pair. Value is {@code null}.\n     * @param <R> the right element of this pair. Value is {@code null}.\n     * @return an immutable pair of nulls.",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":4036,
        "Patch":"private static final long serialVersionUID = 1L;\n    \/** \n    \/**\n     * Returns an immutable triple of nulls.\n     * \n     *\n     * @param <L> the left element of this triple. Value is {@code null}.\n     * @param <M> the middle element of this triple. Value is {@code null}.\n     * @param <R> the right element of this triple. Value is {@code null}.\n     * @return an immutable triple of nulls. \n     * @since 3.6\n     *\/",
        "Stack-trace":"Returns an immutable triple of nulls.\n     * \n     *\n     * @param <L> the left element of this triple. Value is {@code null}.\n     * @param <M> the middle element of this triple. Value is {@code null}.\n     * @param <R> the right element of this triple. Value is {@code null}.\n     * @return an immutable triple of nulls. \n     * @since 3.6\n     *\/",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":4096,
        "Patch":"public static String removeAll(final String text, final Pattern pattern) {\n     *\n     * <pre>\n     * StringUtils.removeFirst(null, *)      = null\n     * StringUtils.removeFirst(\"any\", null)  = \"any\"\n     * StringUtils.removeFirst(\"any\", (String) null)  = \"any\"\n     * StringUtils.removeFirst(\"any\", \"\")    = \"any\"\n     * StringUtils.removeFirst(\"any\", \".*\")  = \"\"\n     * StringUtils.removeFirst(\"any\", \".+\")  = \"\"\n@@ -5280,7 +5280,7 @@ public static String replaceOnceIgnoreCase(final String text, final String searc\n     *\n     * <pre>\n     * StringUtils.replacePattern(null, *, *)       = null\n     * StringUtils.replacePattern(\"any\", null, *)   = \"any\"\n     * StringUtils.replacePattern(\"any\", (String) null, *)   = \"any\"\n     * StringUtils.replacePattern(\"any\", *, null)   = \"any\"\n     * StringUtils.replacePattern(\"\", \"\", \"zzz\")    = \"zzz\"\n     * StringUtils.replacePattern(\"\", \".*\", \"zzz\")  = \"zzz\"",
        "Stack-trace":"StringUtils.removePattern(null, *)       = null\n     * StringUtils.removePattern(\"any\", null)   = \"any\"\n     * StringUtils.removePattern(\"any\", (String) null)   = \"any\"\n     * StringUtils.removePattern(\"A&lt;__&gt;\\n&lt;__&gt;B\", \"&lt;.*&gt;\")  = \"AB\"\n     * StringUtils.removePattern(\"ABCabc123\", \"[a-z]\")    = \"ABC123\"\n     * <\/pre>",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":4096,
        "Patch":"public static String replaceFirst(final String text, final String regex, final S\n     * @return  the text with the first replacement processed,\n     *              {@code null} if null String input\n     *\n     * @see java.util.regex.Matcher#replaceFirst(String, String)\n     * @see java.util.regex.Matcher#replaceFirst(String)\n     * @see java.util.regex.Pattern\n     * @since 3.8\n     *\/",
        "Stack-trace":"return  the text with the first replacement processed,\n     *              {@code null} if null String input\n     *\n     * @see java.util.regex.Matcher#replaceFirst(String, String)\n     * @see java.util.regex.Matcher#replaceFirst(String)\n     * @see java.util.regex.Pattern\n     * @since 3.8\n     *\/",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1904,
        "Patch":"public abstract class KllDoublesSketch extends KllSketch implements QuantilesDou\n    return KllHelper.toByteArray(this, false);\n  }\n  @Override\n  public String toString(final boolean withSummary, final boolean withData) {\n    KllSketch sketch = this;\n    if (withData && sketchStructure != UPDATABLE) {\n      final Memory mem = getWritableMemory();\n      assert mem != null;\n      sketch = KllDoublesSketch.heapify(getWritableMemory());\n    }\n    return KllHelper.toStringImpl(sketch, withSummary, withData, getSerDe());\n  }\n  @Override\n  public void update(final double item) {\n    if (readOnly) { throw new SketchesArgumentException(TGT_IS_READ_ONLY_MSG); }",
        "Stack-trace":" if (withData && sketchStructure != UPDATABLE) {\n      final Memory mem = getWritableMemory();\n      assert mem != null;\n      sketch = KllFloatsSketch.heapify(getWritableMemory());\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1921,
        "Patch":"public class HessianProtocol extends AbstractProxyProtocol {\n        hessianProxyFactory.setOverloadEnabled(isOverloadEnabled);\n        String client = url.getParameter(Constants.CLIENT_KEY, Constants.DEFAULT_HTTP_CLIENT);\n        if (\"httpclient\".equals(client)) {\n            hessianProxyFactory.setConnectionFactory(new HttpClientConnectionFactory());\n            HessianConnectionFactory factory = new HttpClientConnectionFactory();\n            factory.setHessianProxyFactory(hessianProxyFactory);\n            hessianProxyFactory.setConnectionFactory(factory);\n        } else if (client != null && client.length() > 0 && !Constants.DEFAULT_HTTP_CLIENT.equals(client)) {\n            throw new IllegalStateException(\"Unsupported http protocol client=\\\"\" + client + \"\\\"!\");\n        } else {",
        "Stack-trace":"else if (client != null && client.length() > 0 && !Constants.DEFAULT_HTTP_CLIENT.equals(client)) {\n            throw new IllegalStateException(\"Unsupported http protocol client=\\\"\" + client + \"\\\"!\");\n        } else {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1106,
        "Patch":" public class AbstractClusterInvokerTest {\n    @Test\n    public void testSelect_Invokersize0() throws Exception {\n        LoadBalance l = cluster.initLoadBalance(invokers, invocation);\n        Assert.assertNotNull(\"cluster.initLoadBalance returns null!\", l);\n        {\n            Invoker invoker = cluster.select(null, null, null, null);\n            Invoker invoker = cluster.select(l, null, null, null);\n            Assert.assertEquals(null, invoker);\n        }\n        {\n            invokers.clear();\n            selectedInvokers.clear();\n            Invoker invoker = cluster.select(null, null, invokers, null);\n            Invoker invoker = cluster.select(l, null, invokers, null);\n            Assert.assertEquals(null, invoker);\n        }\n    }",
        "Stack-trace":"Assert.assertNotNull(\"cluster.initLoadBalance returns null!\", l);\n        {\n            Invoker invoker = cluster.select(null, null, null, null);\n            Invoker invoker = cluster.select(l, null, null, null);\n            Assert.assertEquals(null, invoker);\n        }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1107,
        "Patch":"public class AbstractClusterInvokerTest {\n    public void testSelect_Invokersize1() throws Exception {\n        invokers.clear();\n        invokers.add(invoker1);\n        Invoker invoker = cluster.select(null, null, invokers, null);\n        LoadBalance l = cluster.initLoadBalance(invokers, invocation);\n        Assert.assertNotNull(\"cluster.initLoadBalance returns null!\", l);\n        Invoker invoker = cluster.select(l, null, invokers, null);\n        Assert.assertEquals(invoker1, invoker);\n    }\n@@ -186,16 +189,18 @@ public class AbstractClusterInvokerTest {\n        invokers.clear();\n        invokers.add(invoker2);\n        invokers.add(invoker4);\n        LoadBalance l = cluster.initLoadBalance(invokers, invocation);\n        Assert.assertNotNull(\"cluster.initLoadBalance returns null!\", l);\n        {\n            selectedInvokers.clear();\n            selectedInvokers.add(invoker4);\n            Invoker invoker = cluster.select(null, invocation, invokers, selectedInvokers);\n            Invoker invoker = cluster.select(l, invocation, invokers, selectedInvokers);\n            Assert.assertEquals(invoker2, invoker);\n        }\n        {\n            selectedInvokers.clear();\n            selectedInvokers.add(invoker2);\n            Invoker invoker = cluster.select(null, invocation, invokers, selectedInvokers);\n            Invoker invoker = cluster.select(l, invocation, invokers, selectedInvokers);\n            Assert.assertEquals(invoker4, invoker);\n        }\n    }",
        "Stack-trace":"invokers.add(invoker1);\n        Invoker invoker = cluster.select(null, null, invokers, null);\n        LoadBalance l = cluster.initLoadBalance(invokers, invocation);\n        Assert.assertNotNull(\"cluster.initLoadBalance returns null!\", l);\n        Invoker invoker = cluster.select(l, null, invokers, null);\n        Assert.assertEquals(invoker1, invoker);\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1535,
        "Patch":"public class FlinkSchemaUtil {\n      primaryKey = UniqueConstraint.primaryKey(UUID.randomUUID().toString(), columns);\n    }\n    validatePrimaryKey(schemaColumns, primaryKey);\n    if (primaryKey != null) {\n      validatePrimaryKey(schemaColumns, primaryKey);\n    }\n    return new ResolvedSchema(schemaColumns, Collections.emptyList(), primaryKey);\n  }",
        "Stack-trace":"if (primaryKey != null) {\n      validatePrimaryKey(schemaColumns, primaryKey);\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1484,
        "Patch":"public abstract class AbstractSQLAssertTest extends AbstractSQLTest {\n                continue;\n            }\n            for (SqlAssertData each : sqlShardingRule.getData()) {\n                String strategyName = getShardingStrategy().name();\n                if (ShardingTestStrategy.hint.name().equals(strategyName)) {\n                    strategyName = ShardingTestStrategy.db.name();\n                }\n                \/\/ TODO DML?DQL???????DML?XML?????placeholder\n                String expected = null == each.getExpected() ? \"integrate\/dataset\/EmptyTable.xml\"\n                        : String.format(\"integrate\/dataset\/%s\/expect\/\" + each.getExpected(), getShardingStrategy().name(), getShardingStrategy().name());\n                        : String.format(\"integrate\/dataset\/%s\/expect\/\" + each.getExpected(), strategyName, strategyName);\n                URL url = AbstractSQLAssertTest.class.getClassLoader().getResource(expected);\n                if (null == url) {\n                    throw new RuntimeException(\"Wrong expected file:\" + expected);\n@@ -168,6 +172,8 @@ public abstract class AbstractSQLAssertTest extends AbstractSQLTest {\n        }\n        if (result.contains(\"masterslave\")) {\n            result = result.replace(\"masterslave\", \"ms\");\n        } else if (result.contains(\"hint\")) {\n            result = result.replace(\"hint\", \"db\");\n        } else {\n            result = \"dataSource_\" + result;\n        }",
        "Stack-trace":"String expected = null == each.getExpected() ? \"integrate\/dataset\/EmptyTable.xml\"\n                        : String.format(\"integrate\/dataset\/%s\/expect\/\" + each.getExpected(), getShardingStrategy().name(), getShardingStrategy().name());\n                        : String.format(\"integrate\/dataset\/%s\/expect\/\" + each.getExpected(), strategyName, strategyName);\n                URL url = AbstractSQLAssertTest.class.getClassLoader().getResource(expected);\n                if (null == url) {\n                    throw new RuntimeException(\"Wrong expected file:\" + expected);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":176,
        "Patch":"import static org.apache.jackrabbit.oak.plugins.index.IndexConstants.INDEX_DEFIN\nimport static org.apache.jackrabbit.oak.plugins.index.IndexConstants.REINDEX_ASYNC_PROPERTY_NAME;\nimport static org.apache.jackrabbit.oak.plugins.index.IndexConstants.REINDEX_COUNT;\nimport static org.apache.jackrabbit.oak.plugins.index.IndexConstants.REINDEX_PROPERTY_NAME;\nimport static org.apache.jackrabbit.oak.plugins.index.IndexConstants.TYPE_DISABLED;\nimport static org.apache.jackrabbit.oak.plugins.index.IndexConstants.TYPE_PROPERTY_NAME;\nimport static org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.MISSING_NODE;\nimport static org.apache.jackrabbit.oak.spi.commit.CompositeEditor.compose;\n@@ -215,6 +216,12 @@ public class IndexUpdate implements Editor, PathSource {\n            return false;\n        }\n        \/\/Do not attempt reindex of disabled indexes\n        PropertyState type = definition.getProperty(TYPE_PROPERTY_NAME);\n        if (type != null && TYPE_DISABLED.equals(type.getValue(Type.STRING))) {\n            return false;\n        }\n        PropertyState ps = definition.getProperty(REINDEX_PROPERTY_NAME);\n        if (ps != null && ps.getValue(BOOLEAN)) {\n            return !rootState.ignoreReindexFlags;",
        "Stack-trace":"if (type != null && TYPE_DISABLED.equals(type.getValue(Type.STRING))) {\n            return false;\n        }\n        PropertyState ps = definition.getProperty(REINDEX_PROPERTY_NAME);\n        if (ps != null && ps.getValue(BOOLEAN)) {\n            return !rootState.ignoreReindexFlags;",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1818,
        "Patch":"public final class Lucene90HnswVectorsReader extends KnnVectorsReader {\n    @Override\n    public VectorScorer scorer(float[] target) {\n      if (size() == 0) {\n        return null;\n      }\n      OffHeapFloatVectorValues values = this.copy();\n      return new VectorScorer() {\n        @Override",
        "Stack-trace":"if (size == 0) {\n        return null;\n      }\n      OffHeapFloatVectorValues values = this.copy();\n      return new VectorScorer() {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":114,
        "Patch":"class UserInitializer implements WorkspaceInitializer, UserConstants, QueryIndex\n        target.compareAgainstBaseState(base, new ApplyDiff(builder));\n    }\n    private QueryIndexProvider getIndexProvider() {\n        return checkNotNull(queryIndexProvider, \"QueryIndexProvider yet not initialized\");\n    }\n    @Override\n    public void setQueryIndexProvider(QueryIndexProvider provider) {\n        this.queryIndexProvider = provider;",
        "Stack-trace":"private QueryIndexProvider getIndexProvider() {\n        return checkNotNull(queryIndexProvider, \"QueryIndexProvider yet not initialized\");\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":114,
        "Patch":" public class MemoryDocumentStore implements DocumentStore {\n    private final boolean maintainModCount;\n    private static final Key KEY_MODIFIED = new Key(MODIFIED_IN_SECS, null);\n    public MemoryDocumentStore() {\n        this(false);\n    }",
        "Stack-trace":" private static final Key KEY_MODIFIED = new Key(MODIFIED_IN_SECS, null);\n    public MemoryDocumentStore() {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":114,
        "Patch":"public class MemoryDocumentStore implements DocumentStore {\n    }\n    @Override\n    public <T extends Document> int remove(Collection<T> collection,\n                                           Map<String, Map<UpdateOp.Key, Condition>> toRemove) {\n    public <T extends Document> int remove(Collection<T> collection, Map<String, Long> toRemove) {\n        int num = 0;\n        ConcurrentSkipListMap<String, T> map = getMap(collection);\n        for (Map.Entry<String, Map<UpdateOp.Key, Condition>> entry : toRemove.entrySet()) {\n        for (Map.Entry<String, Long> entry : toRemove.entrySet()) {\n            Lock lock = rwLock.writeLock();\n            lock.lock();\n            try {\n                T doc = map.get(entry.getKey());\n                if (doc != null && checkConditions(doc, entry.getValue())) {\n                Condition c = newEqualsCondition(entry.getValue());\n                if (doc != null && checkConditions(doc, Collections.singletonMap(KEY_MODIFIED, c))) {\n                    if (map.remove(entry.getKey()) != null) {\n                        num++;\n                    }",
        "Stack-trace":" if (doc != null && checkConditions(doc, entry.getValue())) {\n                Condition c = newEqualsCondition(entry.getValue());\n                if (doc != null && checkConditions(doc, Collections.singletonMap(KEY_MODIFIED, c))) {\n                    if (map.remove(entry.getKey()) != null) {\n                        num++;",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":114,
        "Patch":" public class MongoDocumentStore implements DocumentStore, RevisionListener {\n    private boolean hasModifiedIdCompoundIndex = true;\n    private static final Key KEY_MODIFIED = new Key(MODIFIED_IN_SECS, null);\n    public MongoDocumentStore(DB db, DocumentMK.Builder builder) {\n        MongoStatus mongoStatus = builder.getMongoStatus();\n        if (mongoStatus == null) {",
        "Stack-trace":"private static final Key KEY_MODIFIED = new Key(MODIFIED_IN_SECS, null);\n    public MongoDocumentStore(DB db, DocumentMK.Builder builder) {\n        MongoStatus mongoStatus = builder.getMongoStatus();\n        if (mongoStatus == null) {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":114,
        "Patch":"public class RDBDocumentStore implements DocumentStore {\n        return numDeleted;\n    }\n    private <T extends Document> int delete(Collection<T> collection,\n                                            Map<String, Map<Key, Condition>> toRemove) {\n    private <T extends Document> int delete(Collection<T> collection, Map<String, Long> toRemove) {\n        int numDeleted = 0;\n        RDBTableMetaData tmd = getTable(collection);\n        Map<String, Map<Key, Condition>> subMap = Maps.newHashMap();\n        Iterator<Entry<String, Map<Key, Condition>>> it = toRemove.entrySet().iterator();\n        Map<String, Long> subMap = Maps.newHashMap();\n        Iterator<Entry<String, Long>> it = toRemove.entrySet().iterator();\n        while (it.hasNext()) {\n            Entry<String, Map<Key, Condition>> entry = it.next();\n            Entry<String, Long> entry = it.next();\n            subMap.put(entry.getKey(), entry.getValue());\n            if (subMap.size() == 64 || !it.hasNext()) {\n                Connection connection = null;",
        "Stack-trace":"if (subMap.size() == 64 || !it.hasNext()) {\n                Connection connection = null;",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":114,
        "Patch":"public class RDBDocumentStoreJDBC {\n        return count;\n    }\n    public int delete(Connection connection, RDBTableMetaData tmd, Map<String, Map<Key, Condition>> toDelete)\n    public int delete(Connection connection, RDBTableMetaData tmd, Map<String, Long> toDelete)\n            throws SQLException, DocumentStoreException {\n        \/\/ sanity check on parameters; see OAK-6789\n        for (Entry<String, Map<Key, Condition>> entry : toDelete.entrySet()) {\n            if (entry.getValue().entrySet().size() != 1) {\n                throw new DocumentStoreException(\"Unsupported number of conditions in : \" + entry.getValue().entrySet());\n            }\n            Entry<Key, Condition> c = entry.getValue().entrySet().iterator().next();\n            if (!c.getKey().getName().equals(MODIFIED) || c.getKey().getRevision() != null\n                    || c.getValue().type != Condition.Type.EQUALS) {\n                throw new DocumentStoreException(\"Unsupported condition: \" + c);\n            }\n        }\n        PreparedStatement stmt = connection.prepareStatement(\"delete from \" + tmd.getName() + \" where ID=? and MODIFIED=?\");\n        UpdateOp.Key MODIFIEDKEY = new UpdateOp.Key(MODIFIED_IN_SECS, null);\n        try {\n            for (Entry<String, Map<Key, Condition>> entry : toDelete.entrySet()) {\n            for (Entry<String, Long> entry : toDelete.entrySet()) {\n                setIdInStatement(tmd, stmt, 1, entry.getKey());\n                stmt.setLong(2, (Long) entry.getValue().get(MODIFIEDKEY).value);\n                stmt.setLong(2, entry.getValue());\n                stmt.addBatch();\n            }\n            int[] rets = stmt.executeBatch();\n+1, -3\u0095oak-store-document\/src\/main\/java\/org\/apache\/jackrabbit\/oak\/plugins\/document\/util\/LeaseCheckDocumentStoreWrapper.java\n@@ -30,8 +30,6 @@ import org.apache.jackrabbit.oak.plugins.document.DocumentStoreException;\nimport org.apache.jackrabbit.oak.plugins.document.RevisionListener;\nimport org.apache.jackrabbit.oak.plugins.document.RevisionVector;\nimport org.apache.jackrabbit.oak.plugins.document.UpdateOp;\nimport org.apache.jackrabbit.oak.plugins.document.UpdateOp.Condition;\nimport org.apache.jackrabbit.oak.plugins.document.UpdateOp.Key;\nimport org.apache.jackrabbit.oak.plugins.document.cache.CacheInvalidationStats;",
        "Stack-trace":" if (!c.getKey().getName().equals(MODIFIED) || c.getKey().getRevision() != null\n                    || c.getValue().type != Condition.Type.EQUALS) {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":114,
        "Patch":"package org.apache.jackrabbit.oak.plugins.document.util;\nimport com.google.common.collect.Lists;\nimport com.google.common.collect.Maps;\nimport org.apache.jackrabbit.oak.plugins.document.Collection;\nimport org.apache.jackrabbit.oak.plugins.document.Document;\nimport org.apache.jackrabbit.oak.plugins.document.DocumentMKBuilderProvider;\nimport org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore;\nimport org.apache.jackrabbit.oak.plugins.document.DocumentStore;\nimport org.apache.jackrabbit.oak.plugins.document.UpdateOp;\nimport org.apache.jackrabbit.oak.plugins.document.memory.MemoryDocumentStore;\nimport org.apache.jackrabbit.oak.spi.commit.CommitInfo;\nimport org.apache.jackrabbit.oak.spi.commit.EmptyHook;\nimport org.apache.jackrabbit.oak.spi.state.NodeBuilder;\nimport org.junit.Assert;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport java.lang.reflect.InvocationHandler;\nimport java.lang.reflect.InvocationTargetException;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Proxy;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Map;\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\npublic class ReadOnlyDocumentStoreWrapperTest {\n    @Rule\n    public DocumentMKBuilderProvider builderProvider = new DocumentMKBuilderProvider();\n    @Test\n    public void testPassthrough() throws NoSuchMethodException, InvocationTargetException, IllegalAccessException {\n        final List<String> disallowedMethods = Lists.newArrayList(\n                \"create\", \"update\", \"remove\", \"createOrUpdate\", \"findAndUpdate\");\n        InvocationHandler handler = new InvocationHandler() {\n            @Override\n            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n                String methodName = method.getName();\n                if (disallowedMethods.contains(methodName)) {\n                    Assert.fail(String.format(\"Invalid passthrough of method (%s) with params %s\", method, Arrays.toString(args)));\n                }\n                if (\"determineServerTimeDifferenceMillis\".equals(methodName)) {\n                    return new Long(0);\n                } else {\n                    return null;\n                }\n            }\n        };\n        DocumentStore proxyStore = (DocumentStore)Proxy.newProxyInstance(DocumentStore.class.getClassLoader(),\n                new Class[]{DocumentStore.class},\n                handler);\n        DocumentStore readOnlyStore = ReadOnlyDocumentStoreWrapperFactory.getInstance(proxyStore);\n        Collection<? extends Document> []collections = new Collection[] {\n                Collection.CLUSTER_NODES, Collection.JOURNAL, Collection.NODES, Collection.SETTINGS\n        };\n        for (Collection collection : collections) {\n            readOnlyStore.find(collection, null);\n            readOnlyStore.find(collection, null, 0);\n            readOnlyStore.query(collection, null, null, 0);\n            readOnlyStore.query(collection, null, null, null, 0, 0);\n            boolean uoeThrown = false;\n            try {\n                readOnlyStore.remove(collection, \"\");\n            } catch (UnsupportedOperationException uoe) {\n                \/\/catch uoe thrown by read only wrapper\n                uoeThrown = true;\n            }\n            assertTrue(\"remove must throw UnsupportedOperationException\", uoeThrown);\n            uoeThrown = false;\n            try {\n                readOnlyStore.remove(collection, Lists.<String>newArrayList());\n            } catch (UnsupportedOperationException uoe) {\n                \/\/catch uoe thrown by read only wrapper\n                uoeThrown = true;\n            }\n            assertTrue(\"remove must throw UnsupportedOperationException\", uoeThrown);\n            uoeThrown = false;\n            try {\n                readOnlyStore.remove(collection, Maps.<String, Map<UpdateOp.Key, UpdateOp.Condition>>newHashMap());\n            } catch (UnsupportedOperationException uoe) {\n                \/\/catch uoe thrown by read only wrapper\n                uoeThrown = true;\n            }\n            assertTrue(\"remove must throw UnsupportedOperationException\", uoeThrown);\n            uoeThrown = false;\n            try {\n                readOnlyStore.create(collection, null);\n            } catch (UnsupportedOperationException uoe) {\n                \/\/catch uoe thrown by read only wrapper\n                uoeThrown = true;\n            }\n            assertTrue(\"create must throw UnsupportedOperationException\", uoeThrown);\n            uoeThrown = false;\n            try {\n                readOnlyStore.update(collection, null, null);\n            } catch (UnsupportedOperationException uoe) {\n                \/\/catch uoe thrown by read only wrapper\n                uoeThrown = true;\n            }\n            assertTrue(\"update must throw UnsupportedOperationException\", uoeThrown);\n            uoeThrown = false;\n            try {\n                readOnlyStore.createOrUpdate(collection, (UpdateOp) null);\n            } catch (UnsupportedOperationException uoe) {\n                \/\/catch uoe thrown by read only wrapper\n                uoeThrown = true;\n            }\n            assertTrue(\"createOrUpdate must throw UnsupportedOperationException\", uoeThrown);\n            uoeThrown = false;\n            try {\n                readOnlyStore.createOrUpdate(collection, Lists.<UpdateOp>newArrayList());\n            } catch (UnsupportedOperationException uoe) {\n                \/\/catch uoe thrown by read only wrapper\n                uoeThrown = true;\n            }\n            assertTrue(\"createOrUpdate must throw UnsupportedOperationException\", uoeThrown);\n            uoeThrown = false;\n            try {\n                readOnlyStore.findAndUpdate(collection, null);\n            } catch (UnsupportedOperationException uoe) {\n                \/\/catch uoe thrown by read only wrapper\n                uoeThrown = true;\n            }\n            assertTrue(\"findAndUpdate must throw UnsupportedOperationException\", uoeThrown);\n            readOnlyStore.invalidateCache(collection, null);\n            readOnlyStore.getIfCached(collection, null);\n        }\n        readOnlyStore.invalidateCache();\n        readOnlyStore.invalidateCache(null);\n        readOnlyStore.dispose();\n        readOnlyStore.setReadWriteMode(null);\n        readOnlyStore.getCacheStats();\n        readOnlyStore.getMetadata();\n        readOnlyStore.determineServerTimeDifferenceMillis();\n    }\n    @Test\n    public void backgroundRead() throws Exception {\n        DocumentStore docStore = new MemoryDocumentStore();\n        DocumentNodeStore store = builderProvider.newBuilder().setAsyncDelay(0)\n                .setDocumentStore(docStore).setClusterId(2).getNodeStore();\n        DocumentNodeStore readOnlyStore = builderProvider.newBuilder().setAsyncDelay(0)\n                .setDocumentStore(docStore).setClusterId(1).setReadOnlyMode().getNodeStore();\n        NodeBuilder builder = store.getRoot().builder();\n        builder.child(\"node\");\n        store.merge(builder, EmptyHook.INSTANCE, CommitInfo.EMPTY);\n        store.runBackgroundOperations();\n        \/\/ at this point node must not be visible\n        assertFalse(readOnlyStore.getRoot().hasChildNode(\"node\"));\n        readOnlyStore.runBackgroundOperations();\n        \/\/ at this point node should get visible\n        assertTrue(readOnlyStore.getRoot().hasChildNode(\"node\"));\n    }\n}",
        "Stack-trace":"readOnlyStore.findAndUpdate(collection, null);\n            } catch (UnsupportedOperationException uoe) {\n                \/\/catch uoe thrown by read only wrapper\n                uoeThrown = true;\n            }\n            assertTrue(\"findAndUpdate must throw UnsupportedOperationException\", uoeThrown);\n            readOnlyStore.invalidateCache(collection, null);\n            readOnlyStore.getIfCached(collection, null);\n        }\n        readOnlyStore.invalidateCache();\n        readOnlyStore.invalidateCache(null);\n        readOnlyStore.dispose();\n        readOnlyStore.setReadWriteMode(null);\n        readOnlyStore.getCacheStats();",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1818,
        "Patch":"public final class Lucene90HnswVectorsReader extends KnnVectorsReader {\n    @Override\n    public VectorScorer scorer(float[] target) {\n      if (size() == 0) {\n        return null;\n      }\n      OffHeapFloatVectorValues values = this.copy();\n      return new VectorScorer() {\n        @Override\n+3\u0095lucene\/backward-codecs\/src\/java\/org\/apache\/lucene\/backward_codecs\/lucene91\/Lucene91HnswVectorsReader.java\n@@ -494,6 +494,9 @@ public final class Lucene91HnswVectorsReader extends KnnVectorsReader {\n    @Override\n    public VectorScorer scorer(float[] target) {\n      if (size == 0) {\n        return null;\n      }\n      OffHeapFloatVectorValues values = this.copy();\n      return new VectorScorer() {\n        @Override",
        "Stack-trace":" if (size() == 0) {\n        return null;\n      }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1947,
        "Patch":"public class ${class.name}\n    }\n    #end\n    #if ( $locationTracking )\n    #if ( $locationTracking && !$class.superClass )\n    \/**\n     * Gets the location of the specified field in the input source.\n     *\/\n    public InputLocation getLocation( Object key )\n    {\n        if ( key instanceof String )\n        {\n            switch ( ( String ) key )\n            {\n      #if ( ! $class.superClass )\n                case \"\":\n                    return location;\n      #end\n      #foreach ( $field in $class.getFields($version) )\n                case \"${field.name}\":\n                    return ${field.name}Location;\n      #end\n            }\n        }\n      #if ( $class.superClass )\n        return super.getLocation( key );\n      #else\n        return locations != null ? locations.get( key ) : null;\n      #end\n    }\n    #end",
        "Stack-trace":"if ( $class.superClass )\n        return super.getLocation( key );\n      #else\n        return locations != null ? locations.get( key ) : null;\n      #end\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1947,
        "Patch":"public class ${class.name}\n            {\n    #foreach ( $field in $class.getFields($version) )\n                this.${field.name} = base.${field.name};\n    #end\n    #if ( $locationTracking )\n                this.locations = base.locations != null ? new HashMap<>(base.locations) : null;\n    #end\n            }\n            else\n@@ -520,10 +481,6 @@ public class ${class.name}\n            if ( this.locations != null )\n            {\n                locations = this.locations;\n                location = locations.remove( \"\" );\n      #foreach ( $field in $allFields )\n                ${field.name}Location = locations.remove( \"${field.name}\" );\n      #end\n            }\n    #end\n            return new ${class.name}(\n@@ -539,13 +496,7 @@ public class ${class.name}\n      #end\n    #end\n    #if ( $locationTracking )\n                locations != null ? locations : ( base != null ? base.locations : null ),\n      #set ( $sep = \"#if(${allFields.size()}>0),#end\" )\n                location != null ? location : ( base != null ? base.location : null )${sep}\n      #foreach ( $field in $allFields )\n        #set ( $sep = \"#if(${locationTracking}&&$field!=${allFields[${allFields.size()} - 1]}),#end\" )\n                ${field.name}Location != null ? ${field.name}Location : ( base != null ? base.${field.name}Location : null )${sep}\n      #end\n                locations != null ? locations : ( base != null ? base.locations : null )\n    #end\n            );\n        }",
        "Stack-trace":"f ( $locationTracking )\n                locations != null ? locations : ( base != null ? base.locations : null ),\n      #set ( $sep = \"#if(${allFields.size()}>0),#end\" )\n                location != null ? location : ( base != null ? base.location : null )${sep}\n      #foreach ( $field in $allFields )\n        #set ( $sep = \"#if(${locationTracking}&&$field!=${allFields[${allFields.size()} - 1]}),#end\" )\n                ${field.name}Location != null ? ${field.name}Location : ( base != null ? base.${field.name}Location : null )${sep}\n      #end\n                locations != null ? locations : ( base != null ? base.locations : null )",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":207,
        "Patch":"public void validate(Settings settings, SettingsProblemCollector problems) {\n            for (int i = 0; i < pluginGroups.size(); i++) {\n                String pluginGroup = pluginGroups.get(i).trim();\n                if (pluginGroup.trim().isEmpty()) {\n                if (pluginGroup.isEmpty()) {\n                    addViolation(\n                            problems, Severity.ERROR, \"pluginGroups.pluginGroup[\" + i + \"]\", null, \"must not be empty\");\n                } else if (!ID_REGEX.matcher(pluginGroup).matches()) {",
        "Stack-trace":" problems, Severity.ERROR, \"pluginGroups.pluginGroup[\" + i + \"]\", null, \"must not be empty\");\n                } else if (!ID_REGEX.matcher(pluginGroup).matches()) {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":760,
        "Patch":" protected void mergePlugin_Executions(\n                    Object key = getPluginExecutionKey(element);\n                    PluginExecution existing = merged.get(key);\n                    if (existing != null) {\n                        mergePluginExecution(existing, element, sourceDominant, context);\n                        mergePluginExecution(existing, element.clone(), sourceDominant, context);\n                    } else {\n                        merged.put(key, element);\n                        merged.put(key, element.clone());\n                    }\n                }",
        "Stack-trace":" if (existing != null) {\n                        mergePluginExecution(existing, element, sourceDominant, context);\n                        mergePluginExecution(existing, element.clone(), sourceDominant, context);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":942,
        "Patch":"private void commands(CliRequest cliRequest) {\n    \/\/ Maybe it's better to move some of those methods to separate class (SoC).\n    void properties(CliRequest cliRequest) throws Exception {\n        Properties paths = new Properties();\n        paths.put(\"session.topdir\", cliRequest.topdir.toString());\n        if (cliRequest.topdir != null) {\n            paths.put(\"session.topdir\", cliRequest.topdir.toString());\n        }\n        if (cliRequest.rootdir != null) {\n            paths.put(\"session.rootdir\", cliRequest.rootdir.toString());\n        }",
        "Stack-trace":"if (cliRequest.topdir != null) {\n            paths.put(\"session.topdir\", cliRequest.topdir.toString());\n        }\n        if (cliRequest.rootdir != null) {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":889,
        "Patch":"param dir the directory to locate the pom for, never {@code null}\n     * @param options possible parsing options, may be {@code null}\n     * @return an optional parsed {@link Model} or {@code null} if none could be found\n     * @throws ModelParserException if the located model cannot be parsd\n     * @throws ModelParserException if the located model cannot be parsed\n     *\/\n    default Optional<Model> locateAndParse(@Nonnull Path dir, @Nullable Map<String, ?> options)\n            throws ModelParserException {",
        "Stack-trace":"default Optional<Model> locateAndParse(@Nonnull Path dir, @Nullable Map<String, ?> options)\n            throws ModelParserException {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":889,
        "Patch":"void transform(Path src, Path dest, Model model) {\n            String packaging = model.getPackaging();\n            if (POM_PACKAGING.equals(packaging)) {\n                ModelParser parser = null;\n                Source source = null;\n                for (ModelParser p : modelParsers.values()) {\n                    source = p.locate(src.getParent()).orElse(null);\n                    if (source != null && src.equals(source.getPath())) {\n                        parser = p;\n                        break;\n                    }\n                Result<? extends org.apache.maven.model.Model> result =\n                        modelBuilder.buildRawModel(src.toFile(), ModelBuildingRequest.VALIDATION_LEVEL_MINIMAL, false);\n                if (result.hasErrors()) {\n                    throw new IllegalStateException(\n                            \"Unable to build POM \" + src,\n                            result.getProblems().iterator().next().getException());\n                }\n                consumer = result.get().getDelegate();\n                if (source != null) {\n                    consumer = parser.parse(source, null);\n                } else {\n                    try (InputStream is = Files.newInputStream(src)) {\n                        consumer = new MavenStaxReader().read(is);\n                    } catch (Exception e) {\n                        throw new IllegalStateException(\"Unable to read POM \" + src, e);\n                    }\n                }\n                \/\/ file to raw transformation\n                org.apache.maven.model.Model m = new org.apache.maven.model.Model(consumer);\n                try {\n                    transformer.transform(src, context, m);\n                } catch (TransformerException e) {\n                    throw new IllegalStateException(\"Unable to transform POM \" + src, e);\n                }\n                consumer = m.getDelegate();\n                \/\/ raw to consumer transform\n                consumer = consumer.withRoot(false).withModules(null);\n                if (consumer.getParent() != null) {",
        "Stack-trace":"if (source != null) {\n                    consumer = parser.parse(source, null);\n                } else {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":889,
        "Patch":"private DefaultModelBuildingResult asDefaultModelBuildingResult(ModelBuildingRes\n                .setValidationLevel(validationLevel)\n                .setLocationTracking(locationTracking)\n                .setModelSource(new FileModelSource(pomFile));\n        final DefaultModelProblemCollector collector =\n                new DefaultModelProblemCollector(new DefaultModelBuildingResult());\n        DefaultModelProblemCollector problems = new DefaultModelProblemCollector(new DefaultModelBuildingResult());\n        try {\n            return newResult(readFileModel(request, collector), collector.getProblems());\n            Model model = readFileModel(request, problems);\n            try {\n                if (transformer != null && request.getTransformerContextBuilder() != null) {\n                    TransformerContext context =\n                            request.getTransformerContextBuilder().initialize(request, problems);\n                    transformer.transform(pomFile.toPath(), context, model);\n                }\n            } catch (TransformerException e) {\n                problems.add(new ModelProblemCollectorRequest(Severity.FATAL, Version.V40).setException(e));\n            }\n            return newResult(model, problems.getProblems());\n        } catch (ModelBuildingException e) {\n            return error(collector.getProblems());\n            return error(problems.getProblems());\n        }\n    }",
        "Stack-trace":"if (transformer != null && request.getTransformerContextBuilder() != null) {\n                    TransformerContext context =\n                            request.getTransformerContextBuilder().initialize(request, problems);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":889,
        "Patch":"if (transformer != null && request.getTransformerContextBuilder() != null) {\n                    TransformerContext context =\n                            request.getTransformerContextBuilder().initialize(request, problems);",
        "Stack-trace":"if (pomFile != null) {\n            Path projectDirectory = pomFile.getParent();\n            \/\/            Path path = pomFile;",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":107,
        "Patch":"public final class ProjectionEngine {\n        return new ParameterMarkerProjection(projectionSegment.getParameterMarkerIndex(), projectionSegment.getParameterMarkerType(), projectionSegment.getAliasName().orElse(null));\n    }",
        "Stack-trace":"new ParameterMarkerProjection(projectionSegment.getParameterMarkerIndex(), projectionSegment.getParameterMarkerType(), projectionSegment.getAliasName().orElse(null));\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":108,
        "Patch":"new ParameterMarkerProjection(projectionSegment.getParameterMarkerIndex(), projectionSegment.getParameterMarkerType(), projectionSegment.getAliasName().orElse(null));\n    }",
        "Stack-trace":"new ParameterMarkerProjection(projectionSegment.getParameterMarkerIndex(), projectionSegment.getParameterMarkerType(), projectionSegment.getAliasName().orElse(null));\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":109,
        "Patch":"new ParameterMarkerProjection(projectionSegment.getParameterMarkerIndex(), projectionSegment.getParameterMarkerType(), projectionSegment.getAliasName().orElse(null));\n    }",
        "Stack-trace":"projectionSegment.getParameterMarkerType(), projectionSegment.getAliasName().orElse(null));\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":110,
        "Patch":"projectionSegment.getParameterMarkerType(), projectionSegment.getAliasName().orElse(null));\n    }",
        "Stack-trace":"projectionSegment.getParameterMarkerType(), projectionSegment.getAliasName().orElse(null));\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":989,
        "Patch":"public Project getProject(MavenProject project) {\n    public abstract ArtifactRepository toArtifactRepository(RemoteRepository repository);\n    public List<org.eclipse.aether.graph.Dependency> toDependencies(Collection<DependencyCoordinate> dependencies) {\n        return dependencies == null ? null : map(dependencies, this::toDependency);\n    public List<org.eclipse.aether.graph.Dependency> toDependencies(\n            Collection<DependencyCoordinate> dependencies, boolean managed) {\n        return dependencies == null ? null : map(dependencies, d -> toDependency(d, managed));\n    }\n    public abstract org.eclipse.aether.graph.Dependency toDependency(DependencyCoordinate dependency);\n    public abstract org.eclipse.aether.graph.Dependency toDependency(DependencyCoordinate dependency, boolean managed);\n    public List<org.eclipse.aether.artifact.Artifact> toArtifacts(Collection<Artifact> artifacts) {\n        return artifacts == null ? null : map(artifacts, this::toArtifact);",
        "Stack-trace":"return dependencies == null ? null : map(dependencies, this::toDependency);\n    public List<org.eclipse.aether.graph.Dependency> toDependencies(\n            Collection<DependencyCoordinate> dependencies, boolean managed) {\n        return dependencies == null ? null : map(dependencies, d -> toDependency(d, managed));",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":986,
        "Patch":"public DependencyCollectorResult collect(@Nonnull DependencyCollectorRequest req\n        Artifact rootArtifact =\n                request.getRootArtifact().map(session::toArtifact).orElse(null);\n        Dependency root = request.getRoot().map(session::toDependency).orElse(null);\n        Dependency root =\n                request.getRoot().map(d -> session.toDependency(d, false)).orElse(null);\n        CollectRequest collectRequest = new CollectRequest()\n                .setRootArtifact(rootArtifact)\n                .setRoot(root)\n                .setDependencies(session.toDependencies(request.getDependencies()))\n                .setManagedDependencies(session.toDependencies(request.getManagedDependencies()))\n                .setDependencies(session.toDependencies(request.getDependencies(), false))\n                .setManagedDependencies(session.toDependencies(request.getManagedDependencies(), true))\n                .setRepositories(session.toRepositories(session.getRemoteRepositories()));\n        RepositorySystemSession systemSession = session.getSession();",
        "Stack-trace":"request.getRootArtifact().map(session::toArtifact).orElse(null);\n        Dependency root = request.getRoot().map(session::toDependency).orElse(null);\n        Dependency root =\n                request.getRoot().map(d -> session.toDependency(d, false)).orElse(null);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":111,
        "Patch":"public ArtifactRepository toArtifactRepository(RemoteRepository repository) {\n        }\n    }\n    public org.eclipse.aether.graph.Dependency toDependency(DependencyCoordinate dependency) {\n    public org.eclipse.aether.graph.Dependency toDependency(DependencyCoordinate dependency, boolean managed) {\n        org.eclipse.aether.graph.Dependency dep;\n        if (dependency instanceof DefaultDependencyCoordinate) {\n            return ((DefaultDependencyCoordinate) dependency).getDependency();\n            dep = ((DefaultDependencyCoordinate) dependency).getDependency();\n        } else {\n            return new org.eclipse.aether.graph.Dependency(\n            dep = new org.eclipse.aether.graph.Dependency(\n                    new org.eclipse.aether.artifact.DefaultArtifact(\n                            dependency.getGroupId(),\n                            dependency.getArtifactId(),\n@@ -294,5 +295,9 @@ public ArtifactRepository toArtifactRepository(RemoteRepository repository) {\n                            null),\n                    dependency.getScope().id());\n        }\n        if (!managed && \"\".equals(dep.getScope())) {\n            dep = dep.setScope(DependencyScope.COMPILE.id());\n        }\n        return dep;\n    }\n}",
        "Stack-trace":"public ArtifactRepository toArtifactRepository(RemoteRepository repository) {\n                            null),\n                    dependency.getScope().id());\n        }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1016,
        "Patch":"private ModelBuilderResult buildModel(RepositorySystemSession session, MavenProj\n        request.transformerContextBuilder(modelBuilder.newTransformerContextBuilder());\n        request.systemProperties(session.getSystemProperties());\n        request.userProperties(session.getUserProperties());\n        ModelCache modelCache = DefaultModelCache.newInstance(session, false);\n        Map map = (Map) session.getCache().get(session, DefaultModelCache.class.getName());\n        System.out.println(\"ModelCache contains \" + map.size());\n        map.keySet().forEach(k -> System.out.println(\"    \" + k));\n        request.modelCache(modelCache);\n        request.modelCache(DefaultModelCache.newInstance(session, false));\n        if (session.getCache() != null) {\n            Map map = (Map) session.getCache().get(session, DefaultModelCache.class.getName());\n            System.out.println(\"ModelCache contains \" + map.size());\n            map.keySet().forEach(k -> System.out.println(\"    \" + k));\n        }\n        return modelBuilder.build(request.build());\n    }",
        "Stack-trace":"if (session.getCache() != null) {\n            Map map = (Map) session.getCache().get(session, DefaultModelCache.class.getName());\n            System.out.println(\"ModelCache contains \" + map.size());",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1096,
        "Patch":"void populateProperties(\n            mavenConf = fileSystem.getPath(systemProperties.getProperty(MAVEN_INSTALLATION_CONF));\n        } else if (systemProperties.getProperty(\"maven.conf\") != null) {\n            mavenConf = fileSystem.getPath(systemProperties.getProperty(\"maven.conf\"));\n        } else {\n        } else if (systemProperties.getProperty(MAVEN_HOME) != null) {\n            mavenConf = fileSystem.getPath(systemProperties.getProperty(MAVEN_HOME), \"conf\");\n        } else {\n            mavenConf = fileSystem.getPath(\"\");\n        }\n        Path propertiesFile = mavenConf.resolve(\"maven.properties\");\n        MavenPropertiesLoader.loadProperties(userProperties, propertiesFile, callback, false);",
        "Stack-trace":"mavenConf = fileSystem.getPath(systemProperties.getProperty(MAVEN_INSTALLATION_CONF));\n        } else if (systemProperties.getProperty(\"maven.conf\") != null) {\n            mavenConf = fileSystem.getPath(systemProperties.getProperty(\"maven.conf\"));\n        } else {\n        } else if (systemProperties.getProperty(MAVEN_HOME) != null) {\n            mavenConf = fileSystem.getPath(systemProperties.getProperty(MAVEN_HOME), \"conf\");",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1068,
        "Patch":"private Model doReadFileModel(\n                String version = parent.getVersion();\n                String path = Optional.ofNullable(parent.getRelativePath()).orElse(\"..\");\n                if (version == null && !path.isEmpty()) {\n                    Path pomFile = model.getPomFile();\n                    Path relativePath = Paths.get(path);\n                    Path pomPath = pomFile.resolveSibling(relativePath).normalize();\n                    if (Files.isDirectory(pomPath)) {\n                        pomPath = getModelProcessor().locateExistingPom(pomPath);\n                    }\n                    if (pomPath != null && Files.isRegularFile(pomPath)) {\n                        ModelBuilderRequest parentRequest =\n                                ModelBuilderRequest.build(request, ModelSource.fromPath(pomPath));\n                        Model parentModel = readFileModel(parentRequest, problems);\n                        if (parentModel != null) {\n                            if ((groupId == null || groupId.equals(parentModel.getGroupId()))\n                                    && (artifactId == null || artifactId.equals(parentModel.getArtifactId()))) {\n                                groupId = getGroupId(parentModel);\n                                artifactId = parentModel.getArtifactId();\n                                version = getVersion(parentModel);\n                            String parentGroupId = getGroupId(parentModel);\n                            String parentArtifactId = parentModel.getArtifactId();\n                            String parentVersion = getVersion(parentModel);\n                            if ((groupId == null || groupId.equals(parentGroupId))\n                                    && (artifactId == null || artifactId.equals(parentArtifactId))) {\n                                model = model.withParent(parent.with()\n                                        .groupId(groupId)\n                                        .artifactId(artifactId)\n                                        .version(version)\n                                        .groupId(parentGroupId)\n                                        .artifactId(parentArtifactId)\n                                        .version(parentVersion)\n                                        .build());\n                            }\n                        }",
        "Stack-trace":"if (pomPath != null && Files.isRegularFile(pomPath)) {\n                        ModelBuilderRequest parentRequest =\n                                ModelBuilderRequest.build(request, ModelSource.fromPath(pomPath));\n                        Model parentModel = readFileModel(parentRequest, problems);\n                        if (parentModel != null) {\n                            if ((groupId == null || groupId.equals(parentModel.getGroupId()))\n                                    && (artifactId == null || artifactId.equals(parentModel.getArtifactId()))) {\n                                groupId = getGroupId(parentModel);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":63,
        "Patch":"Model doReadFileModel() throws ModelBuilderException {\n                    rootDirectory = request.getSession().getRootDirectory();\n                } catch (IllegalStateException ignore) {\n                    rootDirectory = modelSource.getPath();\n                    while (rootDirectory != null && !Files.isDirectory(rootDirectory)) {\n                        rootDirectory = rootDirectory.getParent();\n                    }\n                }\n                try (InputStream is = modelSource.openStream()) {\n                    model = modelProcessor.read(XmlReaderRequest.builder()",
        "Stack-trace":"while (rootDirectory != null && !Files.isDirectory(rootDirectory)) {\n                        rootDirectory = rootDirectory.getParent();\n                    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1684,
        "Patch":"public class SCBEngine {\n  }\n  public Environment getEnvironment() {\n    if (this.applicationContext == null) {\n      \/\/ some test cases\n      return null;\n    }\n    return this.applicationContext.getEnvironment();\n  }",
        "Stack-trace":"if (this.applicationContext == null) {\n      \/\/ some test cases\n      return null;\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1684,
        "Patch":"import org.springframework.core.env.Environment;\npublic class ConfigurationProblemsCollector implements BootUpInformationCollector {\n  @Override\n  public String collect(SCBEngine engine) {\n    if (engine.getEnvironment() == null) {\n      \/\/ some test cases\n      return null;\n    }\n    StringBuilder result = new StringBuilder();\n    collectCsePrefix(engine.getEnvironment(), result);\n    collectServiceDefinition(engine.getEnvironment(), result);",
        "Stack-trace":"public String collect(SCBEngine engine) {\n    if (engine.getEnvironment() == null) {\n      \/\/ some test cases\n      return null;\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":18201,
        "Patch":"public final class MySQLClient {\n        try {\n            running = false;\n            channel.close().sync();\n            eventLoopGroup.shutdownGracefully();\n            if (null != eventLoopGroup) {\n                eventLoopGroup.shutdownGracefully();\n            }\n        } catch (final InterruptedException ex) {\n            log.error(\"close channel interrupted\", ex);\n        }",
        "Stack-trace":"if (null != eventLoopGroup) {\n                eventLoopGroup.shutdownGracefully();\n            }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":19170,
        "Patch":"public final class GovernanceRepositoryAPIImpl implements GovernanceRepositoryAP\n    @Override\n    public Optional<String> getJobItemProgress(final String jobId, final int shardingItem) {\n        return Optional.ofNullable(repository.getDirectly(PipelineMetaDataNode.getJobOffsetItemPath(jobId, shardingItem)));\n        String text = repository.getDirectly(PipelineMetaDataNode.getJobOffsetItemPath(jobId, shardingItem));\n        return Strings.isNullOrEmpty(text) ? Optional.empty() : Optional.of(text);\n    }",
        "Stack-trace":"public Optional<String> getJobItemProgress(final String jobId, final int shardingItem) {\n        return Optional.ofNullable(repository.getDirectly(PipelineMetaDataNode.getJobOffsetItemPath(jobId, shardingItem)));\n        String text = repository.getDirectly(PipelineMetaDataNode.getJobOffsetItemPath(jobId, shardingItem));\n        return Strings.isNullOrEmpty(text) ? Optional.empty() : Optional.of(text);",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":112,
        "Patch":"public final class EncryptCreateTableTokenGenerator implements CollectionSQLToke\n                getColumnProjections(new IdentifierValue(optional, column.getColumnName().getIdentifier().getQuoteCharacter())), lastColumn));\n    }\n    private Collection<ColumnProjection> getColumnProjections(final IdentifierValue columnIdentifier) {\n    private Collection<Projection> getColumnProjections(final IdentifierValue columnIdentifier) {\n        return Collections.singletonList(new ColumnProjection(null, columnIdentifier, null));\n    }\n}",
        "Stack-trace":"private Collection<Projection> getColumnProjections(final IdentifierValue columnIdentifier) {\n        return Collections.singletonList(new ColumnProjection(null, columnIdentifier, null));\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":113,
        "Patch":"public final class EncryptProjectionTokenGenerator implements CollectionSQLToken\n    private void addGenerateSQLTokens(final Collection<SQLToken> result, final SelectStatementContext selectStatementContext) {\n        Map<String, String> columnTableNames = getColumnTableNames(selectStatementContext);\n        for (ProjectionSegment projection : selectStatementContext.getSqlStatement().getProjections().getProjections()) {\n        for (ProjectionSegment each : selectStatementContext.getSqlStatement().getProjections().getProjections()) {\n            SubqueryType subqueryType = selectStatementContext.getSubqueryType();\n            if (projection instanceof ColumnProjectionSegment) {\n                ColumnProjectionSegment columnSegment = (ColumnProjectionSegment) projection;\n            if (each instanceof ColumnProjectionSegment) {\n                ColumnProjectionSegment columnSegment = (ColumnProjectionSegment) each;\n                ColumnProjection columnProjection = buildColumnProjection(columnSegment);\n                String tableName = columnTableNames.get(columnProjection.getExpression());\n                if (null != tableName && encryptRule.findEncryptColumn(tableName, columnProjection.getName()).isPresent()) {\n                if (isEncryptColumn(tableName, columnProjection.getName())) {\n                    result.add(generateSQLToken(tableName, columnSegment, columnProjection, subqueryType));\n                }\n            }\n            if (projection instanceof ShorthandProjectionSegment) {\n                ShorthandProjectionSegment shorthandSegment = (ShorthandProjectionSegment) projection;\n                Collection<ColumnProjection> columnProjections = getShorthandProjection(shorthandSegment, selectStatementContext.getProjectionsContext()).getColumnProjections();\n                if (!columnProjections.isEmpty()) {\n                    result.add(generateSQLToken(shorthandSegment, columnProjections, selectStatementContext, subqueryType, columnTableNames));\n            if (each instanceof ShorthandProjectionSegment) {\n                ShorthandProjectionSegment shorthandSegment = (ShorthandProjectionSegment) each;\n                Collection<Projection> actualColumns = getShorthandProjection(shorthandSegment, selectStatementContext.getProjectionsContext()).getActualColumns();\n                if (!actualColumns.isEmpty()) {\n                    result.add(generateSQLToken(shorthandSegment, actualColumns, selectStatementContext, subqueryType, columnTableNames));\n                }\n            }\n        }\n    }\n    private boolean isEncryptColumn(final String tableName, final String columnName) {\n        return null != tableName && encryptRule.findEncryptColumn(tableName, columnName).isPresent();\n    }\n    private SubstitutableColumnNameToken generateSQLToken(final String tableName, final ColumnProjectionSegment columnSegment,\n                                                          final ColumnProjection columnProjection, final SubqueryType subqueryType) {\n        Collection<ColumnProjection> projections = generateProjections(tableName, columnProjection, subqueryType, false, null);\n        Collection<Projection> projections = generateProjections(tableName, columnProjection, subqueryType, false, null);\n        int startIndex = columnSegment.getColumn().getOwner().isPresent() ? columnSegment.getColumn().getOwner().get().getStopIndex() + 2 : columnSegment.getColumn().getStartIndex();\n        int stopIndex = columnSegment.getStopIndex();\n        return new SubstitutableColumnNameToken(startIndex, stopIndex, projections);\n    }\n    private SubstitutableColumnNameToken generateSQLToken(final ShorthandProjectionSegment segment, final Collection<ColumnProjection> columnProjections,\n    private SubstitutableColumnNameToken generateSQLToken(final ShorthandProjectionSegment segment, final Collection<Projection> actualColumns,\n                                                          final SelectStatementContext selectStatementContext, final SubqueryType subqueryType, final Map<String, String> columnTableNames) {\n        List<ColumnProjection> projections = new LinkedList<>();\n        for (ColumnProjection each : columnProjections) {\n        List<Projection> projections = new LinkedList<>();\n        for (Projection each : actualColumns) {\n            String tableName = columnTableNames.get(each.getExpression());\n            if (null == tableName || !encryptRule.findStandardEncryptor(tableName, each.getName()).isPresent()) {\n                projections.add(new ColumnProjection(each.getOwnerIdentifier(), each.getNameIdentifier(), each.getAlias().isPresent() ? each.getAliasIdentifier() : null));\n            } else {\n                projections.addAll(generateProjections(tableName, each, subqueryType, true, segment));\n            if (!isEncryptColumn(tableName, each.getColumnLabel())) {\n                projections.add(each.getAlias().map(optional -> (Projection) new ColumnProjection(null, optional, null)).orElse(each));\n            } else if (each instanceof ColumnProjection) {\n                projections.addAll(generateProjections(tableName, (ColumnProjection) each, subqueryType, true, segment));\n            }\n        }\n        int startIndex = segment.getOwner().isPresent() ? segment.getOwner().get().getStartIndex() : segment.getStartIndex();\n        previousSQLTokens.removeIf(each -> each.getStartIndex() == startIndex);\n        return new SubstitutableColumnNameToken(startIndex, segment.getStopIndex(), projections, selectStatementContext.getDatabaseType().getQuoteCharacter(),\n                selectStatementContext.getTablesContext().getTableSegments());\n        return new SubstitutableColumnNameToken(startIndex, segment.getStopIndex(), projections, selectStatementContext.getDatabaseType().getQuoteCharacter());\n    }\n    private ColumnProjection buildColumnProjection(final ColumnProjectionSegment segment) {",
        "Stack-trace":"if (null == tableName || !encryptRule.findStandardEncryptor(tableName, each.getName()).isPresent()) {\n                projections.add(new ColumnProjection(each.getOwnerIdentifier(), each.getNameIdentifier(), each.getAlias().isPresent() ? each.getAliasIdentifier() : null));\n            } else {\n                projections.addAll(generateProjections(tableName, each, subqueryType, true, segment));\n            if (!isEncryptColumn(tableName, each.getColumnLabel())) {\n                projections.add(each.getAlias().map(optional -> (Projection) new ColumnProjection(null, optional, null)).orElse(each));\n            } else if (each instanceof ColumnProjection) {",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":114,
        "Patch":"if (null == tableName || !encryptRule.findStandardEncryptor(tableName, each.getName()).isPresent()) {\n                projections.add(new ColumnProjection(each.getOwnerIdentifier(), each.getNameIdentifier(), each.getAlias().isPresent() ? each.getAliasIdentifier() : null));\n            } else {\n                projections.addAll(generateProjections(tableName, each, subqueryType, true, segment));\n            if (!isEncryptColumn(tableName, each.getColumnLabel())) {\n                projections.add(each.getAlias().map(optional -> (Projection) new ColumnProjection(null, optional, null)).orElse(each));\n            } else if (each instanceof ColumnProjection) {",
        "Stack-trace":"null == ctx.assistedQueryColumnDefinition() ? null\n                        : new EncryptColumnItemSegment(getIdentifierValue(ctx.assistedQueryColumnDefinition().assistedQueryColumnName()),\n                                null == ctx.assistedQueryAlgorithm() ? null : (AlgorithmSegment) visit(ctx.assistedQueryAlgorithm().algorithmDefinition())),\n                null == ctx.likeQueryColumnDefinition() ? null\n                        : new EncryptColumnItemSegment(getIdentifierValue(ctx.likeQueryColumnDefinition().likeQueryColumnName()),\n                                null == ctx.likeQueryAlgorithm() ? null : (AlgorithmSegment) visit(ctx.likeQueryAlgorithm().algorithmDefinition())));\n                        : new EncryptColumnItemSegment(getIdentifierValue(ctx.assistedQueryColumnDefinition().assistedQueryColumnName()), getAssistedEncryptor(ctx)),\n                null == ctx.likeQueryColumnDefinition() ? null : new EncryptColumnItemSegment(getIdentifierValue(ctx.likeQueryColumnDefinition().likeQueryColumnName()), getLikeEncryptor(ctx)));\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":115,
        "Patch":"public final class ProjectionEngine {\n        return new ParameterMarkerProjection(projectionSegment.getParameterMarkerIndex(), projectionSegment.getParameterMarkerType(), projectionSegment.getAliasName().orElse(null));\n    }\n    private SubqueryProjection createProjection(final SubqueryProjectionSegment projectionSegment) {\n        return new SubqueryProjection(projectionSegment.getText(), projectionSegment.getAliasName().orElse(null));\n    private SubqueryProjection createProjection(final TableSegment table, final SubqueryProjectionSegment projectionSegment) {\n        Projection subqueryProjection = createProjection(table, projectionSegment.getSubquery().getSelect().getProjections().getProjections().iterator().next())\n                .orElseThrow(() -> new IllegalArgumentException(\"Subquery projection must have at least one projection column.\"));\n        return new SubqueryProjection(projectionSegment.getText(), subqueryProjection, projectionSegment.getAliasName().orElse(null), databaseType);\n    }\n    private ShorthandProjection createProjection(final TableSegment table, final ShorthandProjectionSegment projectionSegment) {",
        "Stack-trace":"public final class ProjectionEngine {\n        return new ParameterMarkerProjection(projectionSegment.getParameterMarkerIndex(), projectionSegment.getParameterMarkerType(), projectionSegment.getAliasName().orElse(null));\n    }",
        "Label":"Null_Pointer_Exception"
    },
    {
        "ID Number":1001,
        "Patch":"private void switchToPerspective( final PlaceRequest place,\n                                               LifecyclePhase.OPEN,\n                                               ex );\n             }\n+            existingWorkbenchActivities.remove( newPerspectiveActivity.getPlace() );\n             activityManager.destroyActivity( newPerspectiveActivity );\n         }\n     }",
        "Stack-trace":"private void switchToPerspective( final PlaceRequest place,\n                                               LifecyclePhase.OPEN,\n                                               ex );\n             }\n+            existingWorkbenchActivities.remove( newPerspectiveActivity.getPlace() );\n             activityManager.destroyActivity( newPerspectiveActivity );\n         }\n     }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1002,
        "Patch":" package org.uberfire.ext.wires.bpmn.api.model;\n \n-import java.util.Set;\n-\n \/**\n- * Mar\n+ * Implementations can provide a copy of themselves.\n  *\/\n public interface Copyable<T> {\n \n-T copy();\n+    T copy();\n \n }",
        "Stack-trace":" package org.uberfire.ext.wires.bpmn.api.model;\n \n-import java.util.Set;\n-\n \/**\n- * Mar\n+ * Implementations can provide a copy of themselves.\n  *\/\n public interface Copyable<T> {\n \n-T copy();\n+    T copy();\n \n }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1003,
        "Patch":"public BatchCommand( final Command... commands ) {\n+        this.commands = Arrays.asList( PortablePreconditions.checkNotNull( \"commands\",\n+                                                                           commands ) );\n+    }\n+\n+    @Override\n+    public Results apply( final RuleManager ruleManager ) {\n+        final Results results = new DefaultResultsImpl();\n+        final Stack<Command> appliedCommands = new Stack<Command>();\n+        for ( Command command : commands ) {\n+            results.getMessages().addAll( command.apply( ruleManager ).getMessages() );\n+            if ( results.contains( ResultType.ERROR ) ) {\n+                for ( Command undo : appliedCommands ) {\n+                    undo.undo( ruleManager );\n+                }\n+                return results;\n+            } else {\n+                appliedCommands.add( command );\n+            }\n+        }\n+        return results;\n+    }\n+\n+    @Override\n+    public Results undo( final RuleManager ruleManager ) {\n+        final Results results = new DefaultResultsImpl();\n+        final Stack<Command> appliedCommands = new Stack<Command>();\n+        for ( Command command : commands ) {\n+            results.getMessages().addAll( command.undo( ruleManager ).getMessages() );\n+            if ( results.contains( ResultType.ERROR ) ) {\n+                for ( Command cmd : appliedCommands ) {\n+                    cmd.apply( ruleManager );\n+                }\n+                return results;\n+            } else {\n+                appliedCommands.add( command );\n+            }\n+        }\n+        return results;\n+    }\n+\n+}",
        "Stack-trace":"public BatchCommand( final Command... commands ) {\n+        this.commands = Arrays.asList( PortablePreconditions.checkNotNull( \"commands\",\n+                                                                           commands ) );\n+    }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1004,
        "Patch":" import java.io.Closeable;\n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.concurrent.atomic.AtomicBoolean;\n@@ -224,10 +225,11 @@ public void processResult(int rc, String path, Object ctx, byte[] data, Stat sta\n           } catch (Exception e) {\n             break;\n           }\n-          zk.getData(path, false, getTaskForExecutionCallback, new String(data));\n+          zk.getData(path, false, getTaskForExecutionCallback,\n+            new String(data, StandardCharsets.UTF_8));\n           break;\n         case OK:\n-          String cmd = new String(data);\n+          String cmd = new String(data, StandardCharsets.UTF_8);\n           LOG.info(\"Executing command : \" + cmd);\n           String status = ChaosConstants.TASK_COMPLETION_STRING;\n           try {",
        "Stack-trace":" import java.io.Closeable;\n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.concurrent.atomic.AtomicBoolean;\n@@ -224,10 +225,11 @@ public void processResult(int rc, String path, Object ctx, byte[] data, Stat sta\n           } catch (Exception e) {\n             break;\n           }\n-          zk.getData(path, false, getTaskForExecutionCallback, new String(data));\n+          zk.getData(path, false, getTaskForExecutionCallback,\n+            new String(data, StandardCharsets.UTF_8));\n           break;\n         case OK:\n-          String cmd = new String(data);\n+          String cmd = new String(data, StandardCharsets.UTF_8);\n           LOG.info(\"Executing command : \" + cmd);\n           String status = ChaosConstants.TASK_COMPLETION_STRING;\n           try {",
        "Label":"Not_Null"
    },
    {
        "ID Number":1005,
        "Patch":"public void testAutoCalcFixedOverhead() throws InterruptedException {\n+    List<Class<?>> classList = Arrays.asList(HFileContext.class, HRegion.class, BlockCacheKey.class,\n+      HFileBlock.class, HStore.class, LruBlockCache.class, StoreContext.class);\n+    for (int i = 0; i < 10; i++) {\n+      \/\/ warm up\n+      calcFixedOverhead(classList);\n+    }\n+    long startNs = System.nanoTime();\n+    long overhead = 0;\n+    for (int i = 0; i < 100; i++) {\n+      overhead += calcFixedOverhead(classList);\n+    }\n+    long costNs = System.nanoTime() - startNs;\n+    LOG.info(\"overhead = {}, cost {} ns\", overhead, costNs);\n+    \/\/ the single computation cost should be less than 5ms\n+    assertThat(costNs, lessThan(TimeUnit.MILLISECONDS.toNanos(5) * classList.size() * 100));\n   }\n }",
        "Stack-trace":"public void testAutoCalcFixedOverhead() throws InterruptedException {\n+    List<Class<?>> classList = Arrays.asList(HFileContext.class, HRegion.class, BlockCacheKey.class,\n+      HFileBlock.class, HStore.class, LruBlockCache.class, StoreContext.class);\n+    for (int i = 0; i < 10; i++) {\n+      \/\/ warm up\n+      calcFixedOverhead(classList);\n+    }\n+    long startNs = System.nanoTime();\n+    long overhead = 0;\n+    for (int i = 0; i < 100; i++) {\n+      overhead += calcFixedOverhead(classList);\n+    }\n+    long costNs = System.nanoTime() - startNs;\n+    LOG.info(\"overhead = {}, cost {} ns\", overhead, costNs);\n+    \/\/ the single computation cost should be less than 5ms\n+    assertThat(costNs, lessThan(TimeUnit.MILLISECONDS.toNanos(5) * classList.size() * 100));\n   }\n }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1006,
        "Patch":" package org.apache.hadoop.metrics2.lib;\n \n+import com.google.errorprone.annotations.RestrictedApi;\n import java.util.Collection;\n import java.util.concurrent.ConcurrentMap;\n import org.apache.hadoop.hbase.metrics.Interns;\n@@ -388,8 +389,9 @@ public void removeMetric(String name) {\n \n   public void removeHistogramMetrics(String baseName) {\n     for (String suffix : histogramSuffixes) {\n-      removeMetric(baseName + suffix);\n+      helper.removeObjectName(baseName + suffix);\n     }\n+    metricsMap.remove(baseName);\n   }\n \n   \/**",
        "Stack-trace":" package org.apache.hadoop.metrics2.lib;\n \n+import com.google.errorprone.annotations.RestrictedApi;\n import java.util.Collection;\n import java.util.concurrent.ConcurrentMap;\n import org.apache.hadoop.hbase.metrics.Interns;\n@@ -388,8 +389,9 @@ public void removeMetric(String name) {\n \n   public void removeHistogramMetrics(String baseName) {\n     for (String suffix : histogramSuffixes) {\n-      removeMetric(baseName + suffix);\n+      helper.removeObjectName(baseName + suffix);\n     }\n+    metricsMap.remove(baseName);\n   }\n \n   \/**",
        "Label":"Not_Null"
    },
    {
        "ID Number":1007,
        "Patch":"public void clearMetrics() {\n     }\n     metricsMap.clear();\n   }\n+\n+  @RestrictedApi(explanation = \"Should only be called in TestMetricsTableMetricsMap\", link = \"\",\n+      allowedOnPath = \".*\/(DynamicMetricsRegistry|TestMetricsTableMetricsMap).java\")\n+  public ConcurrentMap<String, MutableMetric> getMetricsMap() {\n+    return metricsMap;\n+  }\n }",
        "Stack-trace":"public void clearMetrics() {\n     }\n     metricsMap.clear();\n   }\n+\n+  @RestrictedApi(explanation = \"Should only be called in TestMetricsTableMetricsMap\", link = \"\",\n+      allowedOnPath = \".*\/(DynamicMetricsRegistry|TestMetricsTableMetricsMap).java\")\n+  public ConcurrentMap<String, MutableMetric> getMetricsMap() {\n+    return metricsMap;\n+  }\n }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1008,
        "Patch":"public void setRestoredRegion(boolean restoredRegion) {\n   private final int miniBatchSize;\n \n   final ConcurrentHashMap<RegionScanner, Long> scannerReadPoints;\n+  final ReadPointCalculationLock smallestReadPointCalcLock;\n \n   \/**\n    * The sequence ID that was enLongAddered when this region was opened.",
        "Stack-trace":"public void setRestoredRegion(boolean restoredRegion) {\n   private final int miniBatchSize;\n \n   final ConcurrentHashMap<RegionScanner, Long> scannerReadPoints;\n+  final ReadPointCalculationLock smallestReadPointCalcLock;\n \n   \/**\n    * The sequence ID that was enLongAddered when this region was opened.",
        "Label":"Not_Null"
    },
    {
        "ID Number":1009,
        "Patch":" public void setRestoredRegion(boolean restoredRegion) {\n    *         this readPoint, are included in every read operation.\n    *\/\n   public long getSmallestReadPoint() {\n-    long minimumReadPoint;\n     \/\/ We need to ensure that while we are calculating the smallestReadPoint\n     \/\/ no new RegionScanners can grab a readPoint that we are unaware of.\n-    \/\/ We achieve this by synchronizing on the scannerReadPoints object.\n-    synchronized (scannerReadPoints) {\n-      minimumReadPoint = mvcc.getReadPoint();\n+    smallestReadPointCalcLock.lock(ReadPointCalculationLock.LockType.CALCULATION_LOCK);\n+    try {\n+      long minimumReadPoint = mvcc.getReadPoint();\n       for (Long readPoint : this.scannerReadPoints.values()) {\n-        if (readPoint < minimumReadPoint) {\n-          minimumReadPoint = readPoint;\n-        }\n+        minimumReadPoint = Math.min(minimumReadPoint, readPoint);\n       }\n+      return minimumReadPoint;\n+    } finally {\n+      smallestReadPointCalcLock.unlock(ReadPointCalculationLock.LockType.CALCULATION_LOCK);\n     }\n-    return minimumReadPoint;\n   }\n \n   \/*",
        "Stack-trace":" public void setRestoredRegion(boolean restoredRegion) {\n    *         this readPoint, are included in every read operation.\n    *\/\n   public long getSmallestReadPoint() {\n-    long minimumReadPoint;\n     \/\/ We need to ensure that while we are calculating the smallestReadPoint\n     \/\/ no new RegionScanners can grab a readPoint that we are unaware of.\n-    \/\/ We achieve this by synchronizing on the scannerReadPoints object.\n-    synchronized (scannerReadPoints) {\n-      minimumReadPoint = mvcc.getReadPoint();\n+    smallestReadPointCalcLock.lock(ReadPointCalculationLock.LockType.CALCULATION_LOCK);\n+    try {\n+      long minimumReadPoint = mvcc.getReadPoint();\n       for (Long readPoint : this.scannerReadPoints.values()) {\n-        if (readPoint < minimumReadPoint) {\n-          minimumReadPoint = readPoint;\n-        }\n+        minimumReadPoint = Math.min(minimumReadPoint, readPoint);\n       }\n+      return minimumReadPoint;\n+    } finally {\n+      smallestReadPointCalcLock.unlock(ReadPointCalculationLock.LockType.CALCULATION_LOCK);\n     }\n-    return minimumReadPoint;\n   }\n \n   \/*",
        "Label":"Not_Null"
    },
    {
        "ID Number":1010,
        "Patch":"public HRegion(final HRegionFileSystem fs, final WAL wal, final Configuration co\n     }\n     this.rowLockWaitDuration = tmpRowLockDuration;\n \n+    this.smallestReadPointCalcLock = new ReadPointCalculationLock(conf);\n+\n     this.isLoadingCfsOnDemandDefault = conf.getBoolean(LOAD_CFS_ON_DEMAND_CONFIG_KEY, true);\n     this.htableDescriptor = htd;\n     Set<byte[]> families = this.htableDescriptor.getColumnFamilyNames();",
        "Stack-trace":"public HRegion(final HRegionFileSystem fs, final WAL wal, final Configuration co\n     }\n     this.rowLockWaitDuration = tmpRowLockDuration;\n \n+    this.smallestReadPointCalcLock = new ReadPointCalculationLock(conf);\n+\n     this.isLoadingCfsOnDemandDefault = conf.getBoolean(LOAD_CFS_ON_DEMAND_CONFIG_KEY, true);\n     this.htableDescriptor = htd;\n     Set<byte[]> families = this.htableDescriptor.getColumnFamilyNames();",
        "Label":"Not_Null"
    },
    {
        "ID Number":1011,
        "Patch":"private void doAttachReplicateRegionReplicaAction(WALKeyImpl walKey, WALEdit wal\n   \/\/ 1 x RegionSplitPolicy - splitPolicy\n   \/\/ 1 x MetricsRegion - metricsRegion\n   \/\/ 1 x MetricsRegionWrapperImpl - metricsRegionWrapper\n+  \/\/ 1 x ReadPointCalculationLock - smallestReadPointCalcLock\n   public static final long DEEP_OVERHEAD = FIXED_OVERHEAD + ClassSize.OBJECT + \/\/ closeLock\n     (2 * ClassSize.ATOMIC_BOOLEAN) + \/\/ closed, closing\n     (3 * ClassSize.ATOMIC_LONG) + \/\/ numPutsWithoutWAL, dataInMemoryWithoutWAL,",
        "Stack-trace":"private void doAttachReplicateRegionReplicaAction(WALKeyImpl walKey, WALEdit wal\n   \/\/ 1 x RegionSplitPolicy - splitPolicy\n   \/\/ 1 x MetricsRegion - metricsRegion\n   \/\/ 1 x MetricsRegionWrapperImpl - metricsRegionWrapper\n+  \/\/ 1 x ReadPointCalculationLock - smallestReadPointCalcLock\n   public static final long DEEP_OVERHEAD = FIXED_OVERHEAD + ClassSize.OBJECT + \/\/ closeLock\n     (2 * ClassSize.ATOMIC_BOOLEAN) + \/\/ closed, closing\n     (3 * ClassSize.ATOMIC_LONG) + \/\/ numPutsWithoutWAL, dataInMemoryWithoutWAL,",
        "Label":"Not_Null"
    },
    {
        "ID Number":1012,
        "Patch":"public HFileLink(final Path originPath, final Path tempPath, final Path mobPath,\n     this.originPath = originPath;\n     this.mobPath = mobPath;\n     this.archivePath = archivePath;\n-    setLocations(originPath, tempPath, mobPath, archivePath);\n+    setLocations(originPath, archivePath, tempPath, mobPath);\n   }",
        "Stack-trace":"public HFileLink(final Path originPath, final Path tempPath, final Path mobPath,\n     this.originPath = originPath;\n     this.mobPath = mobPath;\n     this.archivePath = archivePath;\n-    setLocations(originPath, tempPath, mobPath, archivePath);\n+    setLocations(originPath, archivePath, tempPath, mobPath);\n   }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1013,
        "Patch":"public void setSocketSendBufSize(int size) {\n \n   @Override\n   public int getNumOpenConnections() {\n-    int channelsCount = allChannels.size();\n-    \/\/ allChannels also contains the server channel, so exclude that from the count.\n-    return channelsCount > 0 ? channelsCount - 1 : channelsCount;\n+    return allChannels.size();\n   }\n \n   private void initSSL(ChannelPipeline p, boolean supportPlaintext)",
        "Stack-trace":"public void setSocketSendBufSize(int size) {\n \n   @Override\n   public int getNumOpenConnections() {\n-    int channelsCount = allChannels.size();\n-    \/\/ allChannels also contains the server channel, so exclude that from the count.\n-    return channelsCount > 0 ? channelsCount - 1 : channelsCount;\n+    return allChannels.size();\n   }\n \n   private void initSSL(ChannelPipeline p, boolean supportPlaintext)",
        "Label":"Not_Null"
    },
    {
        "ID Number":1014,
        "Patch":" private void initializeCellSet(int numOfCells, MemStoreSegmentsIterator iterator\n         offsetInCurentChunk = ChunkCreator.SIZEOF_CHUNK_HEADER;\n       }\n       if (action == MemStoreCompactionStrategy.Action.COMPACT && !alreadyCopied) {\n-        \/\/ for compaction copy cell to the new segment (MSLAB copy)\n-        c = maybeCloneWithAllocator(c, false);\n+\n+        \/\/ For compaction copy cell to the new segment (MSLAB copy),here we set forceCloneOfBigCell\n+        \/\/ to true, because the chunk which the cell is allocated may be freed after the compaction\n+        \/\/ is completed, see HBASE-27464.\n+        c = maybeCloneWithAllocator(c, true);\n       }\n       offsetInCurentChunk = \/\/ add the Cell reference to the index chunk\n         createCellReference((ByteBufferKeyValue) c, chunks[currentChunkIdx].getData(),",
        "Stack-trace":" private void initializeCellSet(int numOfCells, MemStoreSegmentsIterator iterator\n         offsetInCurentChunk = ChunkCreator.SIZEOF_CHUNK_HEADER;\n       }\n       if (action == MemStoreCompactionStrategy.Action.COMPACT && !alreadyCopied) {\n-        \/\/ for compaction copy cell to the new segment (MSLAB copy)\n-        c = maybeCloneWithAllocator(c, false);\n+\n+        \/\/ For compaction copy cell to the new segment (MSLAB copy),here we set forceCloneOfBigCell\n+        \/\/ to true, because the chunk which the cell is allocated may be freed after the compaction\n+        \/\/ is completed, see HBASE-27464.\n+        c = maybeCloneWithAllocator(c, true);\n       }\n       offsetInCurentChunk = \/\/ add the Cell reference to the index chunk\n         createCellReference((ByteBufferKeyValue) c, chunks[currentChunkIdx].getData(),",
        "Label":"Not_Null"
    },
    {
        "ID Number":1015,
        "Patch":"public List<NormalizationTarget> getNormalizationTargets() {\n     return normalizationTargets;\n   }\n \n+  @Override\n+  public long getPlanSizeMb() {\n+    long total = 0;\n+    for (NormalizationTarget target : normalizationTargets) {\n+      total += target.getRegionSizeMb();\n+    }\n+    return total;\n+  }\n+",
        "Stack-trace":"public List<NormalizationTarget> getNormalizationTargets() {\n     return normalizationTargets;\n   }\n \n+  @Override\n+  public long getPlanSizeMb() {\n+    long total = 0;\n+    for (NormalizationTarget target : normalizationTargets) {\n+      total += target.getRegionSizeMb();\n+    }\n+    return total;\n+  }\n+",
        "Label":"Not_Null"
    },
    {
        "ID Number":1016,
        "Patch":"possible configurations would overwhelm and obscure the important.\n     <value>3<\/value>\n     <description>The minimum age for a region to be considered for a merge, in days.<\/description>\n   <\/property>\n-  <property>\n-    <name>hbase.normalizer.merge.min_region_age.days<\/name>\n-    <value>3<\/value>\n-    <description>The minimum age for a region to be considered for a merge, in days.<\/description>\n-  <\/property>\n   <property>\n     <name>hbase.normalizer.merge.min_region_size.mb<\/name>\n     <value>1<\/value>",
        "Stack-trace":"possible configurations would overwhelm and obscure the important.\n     <value>3<\/value>\n     <description>The minimum age for a region to be considered for a merge, in days.<\/description>\n   <\/property>\n-  <property>\n-    <name>hbase.normalizer.merge.min_region_age.days<\/name>\n-    <value>3<\/value>\n-    <description>The minimum age for a region to be considered for a merge, in days.<\/description>\n-  <\/property>\n   <property>\n     <name>hbase.normalizer.merge.min_region_size.mb<\/name>\n     <value>1<\/value>",
        "Label":"Not_Null"
    },
    {
        "ID Number":1017,
        "Patch":" public abstract class CleanerChore<T extends FileCleanerDelegate> extends Schedu\n    *\/\n   public static final String LOG_CLEANER_CHORE_SIZE = \"hbase.log.cleaner.scan.dir.concurrent.size\";\n   static final String DEFAULT_LOG_CLEANER_CHORE_POOL_SIZE = \"1\";\n+  \/**\n+   * Enable the CleanerChore to sort the subdirectories by consumed space and start the cleaning\n+   * with the largest subdirectory. Enabled by default.\n+   *\/\n+  public static final String LOG_CLEANER_CHORE_DIRECTORY_SORTING =\n+    \"hbase.cleaner.directory.sorting\";\n+  static final boolean DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING = true;\n \n   private final DirScanPool pool;\n ",
        "Stack-trace":" public abstract class CleanerChore<T extends FileCleanerDelegate> extends Schedu\n    *\/\n   public static final String LOG_CLEANER_CHORE_SIZE = \"hbase.log.cleaner.scan.dir.concurrent.size\";\n   static final String DEFAULT_LOG_CLEANER_CHORE_POOL_SIZE = \"1\";\n+  \/**\n+   * Enable the CleanerChore to sort the subdirectories by consumed space and start the cleaning\n+   * with the largest subdirectory. Enabled by default.\n+   *\/\n+  public static final String LOG_CLEANER_CHORE_DIRECTORY_SORTING =\n+    \"hbase.cleaner.directory.sorting\";\n+  static final boolean DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING = true;\n \n   private final DirScanPool pool;\n ",
        "Label":"Not_Null"
    },
    {
        "ID Number":1018,
        "Patch":"public abstract class CleanerChore<T extends FileCleanerDelegate> extends Schedu\n   protected List<String> excludeDirs;\n   private CompletableFuture<Boolean> future;\n   private boolean forceRun;\n+  private boolean sortDirectories;\n \n   public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Configuration conf,\n     FileSystem fs, Path oldFileDir, String confKey, DirScanPool pool) {\n@@ -123,6 +131,8 @@ public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Confi\n     if (excludeDirs != null) {\n       LOG.info(\"Cleaner {} excludes sub dirs: {}\", name, excludeDirs);\n     }\n+    sortDirectories = conf.getBoolean(LOG_CLEANER_CHORE_DIRECTORY_SORTING,\n+      DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING);\n     initCleanerChain(confKey);\n   }",
        "Stack-trace":"public abstract class CleanerChore<T extends FileCleanerDelegate> extends Schedu\n   protected List<String> excludeDirs;\n   private CompletableFuture<Boolean> future;\n   private boolean forceRun;\n+  private boolean sortDirectories;\n \n   public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Configuration conf,\n     FileSystem fs, Path oldFileDir, String confKey, DirScanPool pool) {\n@@ -123,6 +131,8 @@ public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Confi\n     if (excludeDirs != null) {\n       LOG.info(\"Cleaner {} excludes sub dirs: {}\", name, excludeDirs);\n     }\n+    sortDirectories = conf.getBoolean(LOG_CLEANER_CHORE_DIRECTORY_SORTING,\n+      DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING);\n     initCleanerChain(confKey);\n   }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1019,
        "Patch":" public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Confi\n     if (excludeDirs != null) {\n       LOG.info(\"Cleaner {} excludes sub dirs: {}\", name, excludeDirs);\n     }\n+    sortDirectories = conf.getBoolean(LOG_CLEANER_CHORE_DIRECTORY_SORTING,\n+      DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING);\n     initCleanerChain(confKey);\n   }",
        "Stack-trace":" public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Confi\n     if (excludeDirs != null) {\n       LOG.info(\"Cleaner {} excludes sub dirs: {}\", name, excludeDirs);\n     }\n+    sortDirectories = conf.getBoolean(LOG_CLEANER_CHORE_DIRECTORY_SORTING,\n+      DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING);\n     initCleanerChain(confKey);\n   }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1020,
        "Patch":" private void traverseAndDelete(Path dir, boolean root, CompletableFuture<Boolean\n       \/\/ Step.3: Start to traverse and delete the sub-directories.\n       List<CompletableFuture<Boolean>> futures = new ArrayList<>();\n       if (!subDirs.isEmpty()) {\n-        sortByConsumedSpace(subDirs);\n+        if (sortDirectories) {\n+          sortByConsumedSpace(subDirs);\n+        }\n         \/\/ Submit the request of sub-directory deletion.\n         subDirs.forEach(subDir -> {\n           if (!shouldExclude(subDir)) {",
        "Stack-trace":" private void traverseAndDelete(Path dir, boolean root, CompletableFuture<Boolean\n       \/\/ Step.3: Start to traverse and delete the sub-directories.\n       List<CompletableFuture<Boolean>> futures = new ArrayList<>();\n       if (!subDirs.isEmpty()) {\n-        sortByConsumedSpace(subDirs);\n+        if (sortDirectories) {\n+          sortByConsumedSpace(subDirs);\n+        }\n         \/\/ Submit the request of sub-directory deletion.\n         subDirs.forEach(subDir -> {\n           if (!shouldExclude(subDir)) {",
        "Label":"Not_Null"
    },
    {
        "ID Number":1021,
        "Patch":"public class AsyncConnectionImpl implements AsyncConnection {\n \n   private final AtomicBoolean closed = new AtomicBoolean(false);\n \n+  private final String metricsScope;\n   private final Optional<MetricsConnection> metrics;\n \n   private final ClusterStatusListener clusterStatusListener;",
        "Stack-trace":"public class AsyncConnectionImpl implements AsyncConnection {\n \n   private final AtomicBoolean closed = new AtomicBoolean(false);\n \n+  private final String metricsScope;\n   private final Optional<MetricsConnection> metrics;\n \n   private final ClusterStatusListener clusterStatusListener;",
        "Label":"Not_Null"
    },
    {
        "ID Number":1022,
        "Patch":"public AsyncConnectionImpl(Configuration conf, ConnectionRegistry registry, Stri\n     SocketAddress localAddress, User user) {\n     this.conf = conf;\n     this.user = user;\n+    this.metricsScope = MetricsConnection.getScope(conf, clusterId, this);\n \n     if (user.isLoginFromKeytab()) {\n       spawnRenewalChore(user.getUGI());\n     }\n     this.connConf = new AsyncConnectionConfiguration(conf);\n     this.registry = registry;\n     if (conf.getBoolean(CLIENT_SIDE_METRICS_ENABLED_KEY, false)) {\n-      String scope = MetricsConnection.getScope(conf, clusterId, this);\n-      this.metrics = Optional.of(new MetricsConnection(scope, () -> null, () -> null));\n+      this.metrics =\n+        Optional.of(MetricsConnection.getMetricsConnection(metricsScope, () -> null, () -> null));\n     } else {\n       this.metrics = Optional.empty();\n     }",
        "Stack-trace":"public AsyncConnectionImpl(Configuration conf, ConnectionRegistry registry, Stri\n     SocketAddress localAddress, User user) {\n     this.conf = conf;\n     this.user = user;\n+    this.metricsScope = MetricsConnection.getScope(conf, clusterId, this);\n \n     if (user.isLoginFromKeytab()) {\n       spawnRenewalChore(user.getUGI());\n     }\n     this.connConf = new AsyncConnectionConfiguration(conf);\n     this.registry = registry;\n     if (conf.getBoolean(CLIENT_SIDE_METRICS_ENABLED_KEY, false)) {\n-      String scope = MetricsConnection.getScope(conf, clusterId, this);\n-      this.metrics = Optional.of(new MetricsConnection(scope, () -> null, () -> null));\n+      this.metrics =\n+        Optional.of(MetricsConnection.getMetricsConnection(metricsScope, () -> null, () -> null));\n     } else {\n       this.metrics = Optional.empty();\n     }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1023,
        "Patch":"public abstract class RpcServer implements RpcServerInterface, ConfigurationObse\n   private static final String MULTI_SERVICE_CALLS = \"multi.service_calls\";\n \n   private final boolean authorize;\n-  private final boolean isOnlineLogProviderEnabled;\n+  private volatile boolean isOnlineLogProviderEnabled;\n   protected boolean isSecurityEnabled;\n \n   public static final byte CURRENT_VERSION = 0;",
        "Stack-trace":"public abstract class RpcServer implements RpcServerInterface, ConfigurationObse\n   private static final String MULTI_SERVICE_CALLS = \"multi.service_calls\";\n \n   private final boolean authorize;\n-  private final boolean isOnlineLogProviderEnabled;\n+  private volatile boolean isOnlineLogProviderEnabled;\n   protected boolean isSecurityEnabled;\n \n   public static final byte CURRENT_VERSION = 0;",
        "Label":"Not_Null"
    },
    {
        "ID Number":1024,
        "Patch":"public abstract class RpcServer implements RpcServerInterface, ConfigurationObse\n   protected static final Gson GSON = GsonUtil.createGsonWithDisableHtmlEscaping().create();\n \n   protected final int maxRequestSize;\n-  protected final int warnResponseTime;\n-  protected final int warnResponseSize;\n+  protected volatile int warnResponseTime;\n+  protected volatile int warnResponseSize;\n \n   protected final int minClientRequestTimeout;\n \n@@ -275,8 +275,8 @@ public RpcServer(final Server server, final String name,\n     this.maxQueueSizeInBytes =\n       this.conf.getLong(\"hbase.ipc.server.max.callqueue.size\", DEFAULT_MAX_CALLQUEUE_SIZE);\n \n-    this.warnResponseTime = conf.getInt(WARN_RESPONSE_TIME, DEFAULT_WARN_RESPONSE_TIME);\n-    this.warnResponseSize = conf.getInt(WARN_RESPONSE_SIZE, DEFAULT_WARN_RESPONSE_SIZE);\n+    this.warnResponseTime = getWarnResponseTime(conf);\n+    this.warnResponseSize = getWarnResponseSize(conf);\n     this.minClientRequestTimeout =\n       conf.getInt(MIN_CLIENT_REQUEST_TIMEOUT, DEFAULT_MIN_CLIENT_REQUEST_TIMEOUT);\n     this.maxRequestSize = conf.getInt(MAX_REQUEST_SIZE, DEFAULT_MAX_REQUEST_SIZE);\n@@ -297,8 +297,7 @@ public RpcServer(final Server server, final String name,\n       saslProps = Collections.emptyMap();\n     }\n \n-    this.isOnlineLogProviderEnabled = conf.getBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY,\n-      HConstants.DEFAULT_ONLINE_LOG_PROVIDER_ENABLED);\n+    this.isOnlineLogProviderEnabled = getIsOnlineLogProviderEnabled(conf);\n     this.scheduler = scheduler;\n   }\n \n@@ -311,6 +310,35 @@ public void onConfigurationChange(Configuration newConf) {\n     if (authorize) {\n       refreshAuthManager(newConf, new HBasePolicyProvider());\n     }\n+    refreshSlowLogConfiguration(newConf);\n+  }\n+\n+  private void refreshSlowLogConfiguration(Configuration newConf) {\n+    boolean newIsOnlineLogProviderEnabled = getIsOnlineLogProviderEnabled(newConf);\n+    if (isOnlineLogProviderEnabled != newIsOnlineLogProviderEnabled) {\n+      isOnlineLogProviderEnabled = newIsOnlineLogProviderEnabled;\n+    }\n+    int newWarnResponseTime = getWarnResponseTime(newConf);\n+    if (warnResponseTime != newWarnResponseTime) {\n+      warnResponseTime = newWarnResponseTime;\n+    }\n+    int newWarnResponseSize = getWarnResponseSize(newConf);\n+    if (warnResponseSize != newWarnResponseSize) {\n+      warnResponseSize = newWarnResponseSize;\n+    }\n+  }\n+\n+  private static boolean getIsOnlineLogProviderEnabled(Configuration conf) {\n+    return conf.getBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY,\n+      HConstants.DEFAULT_ONLINE_LOG_PROVIDER_ENABLED);\n+  }\n+\n+  private static int getWarnResponseTime(Configuration conf) {\n+    return conf.getInt(WARN_RESPONSE_TIME, DEFAULT_WARN_RESPONSE_TIME);\n+  }\n+\n+  private static int getWarnResponseSize(Configuration conf) {\n+    return conf.getInt(WARN_RESPONSE_SIZE, DEFAULT_WARN_RESPONSE_SIZE);\n   }\n \n   protected void initReconfigurable(Configuration confToLoad) {",
        "Stack-trace":"public abstract class RpcServer implements RpcServerInterface, ConfigurationObse\n   protected static final Gson GSON = GsonUtil.createGsonWithDisableHtmlEscaping().create();\n \n   protected final int maxRequestSize;\n-  protected final int warnResponseTime;\n-  protected final int warnResponseSize;\n+  protected volatile int warnResponseTime;\n+  protected volatile int warnResponseSize;\n \n   protected final int minClientRequestTimeout;\n \n@@ -275,8 +275,8 @@ public RpcServer(final Server server, final String name,\n     this.maxQueueSizeInBytes =\n       this.conf.getLong(\"hbase.ipc.server.max.callqueue.size\", DEFAULT_MAX_CALLQUEUE_SIZE);\n \n-    this.warnResponseTime = conf.getInt(WARN_RESPONSE_TIME, DEFAULT_WARN_RESPONSE_TIME);\n-    this.warnResponseSize = conf.getInt(WARN_RESPONSE_SIZE, DEFAULT_WARN_RESPONSE_SIZE);\n+    this.warnResponseTime = getWarnResponseTime(conf);\n+    this.warnResponseSize = getWarnResponseSize(conf);\n     this.minClientRequestTimeout =\n       conf.getInt(MIN_CLIENT_REQUEST_TIMEOUT, DEFAULT_MIN_CLIENT_REQUEST_TIMEOUT);\n     this.maxRequestSize = conf.getInt(MAX_REQUEST_SIZE, DEFAULT_MAX_REQUEST_SIZE);\n@@ -297,8 +297,7 @@ public RpcServer(final Server server, final String name,\n       saslProps = Collections.emptyMap();\n     }\n \n-    this.isOnlineLogProviderEnabled = conf.getBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY,\n-      HConstants.DEFAULT_ONLINE_LOG_PROVIDER_ENABLED);\n+    this.isOnlineLogProviderEnabled = getIsOnlineLogProviderEnabled(conf);\n     this.scheduler = scheduler;\n   }\n \n@@ -311,6 +310,35 @@ public void onConfigurationChange(Configuration newConf) {\n     if (authorize) {\n       refreshAuthManager(newConf, new HBasePolicyProvider());\n     }\n+    refreshSlowLogConfiguration(newConf);\n+  }\n+\n+  private void refreshSlowLogConfiguration(Configuration newConf) {\n+    boolean newIsOnlineLogProviderEnabled = getIsOnlineLogProviderEnabled(newConf);\n+    if (isOnlineLogProviderEnabled != newIsOnlineLogProviderEnabled) {\n+      isOnlineLogProviderEnabled = newIsOnlineLogProviderEnabled;\n+    }\n+    int newWarnResponseTime = getWarnResponseTime(newConf);\n+    if (warnResponseTime != newWarnResponseTime) {\n+      warnResponseTime = newWarnResponseTime;\n+    }\n+    int newWarnResponseSize = getWarnResponseSize(newConf);\n+    if (warnResponseSize != newWarnResponseSize) {\n+      warnResponseSize = newWarnResponseSize;\n+    }\n+  }\n+\n+  private static boolean getIsOnlineLogProviderEnabled(Configuration conf) {\n+    return conf.getBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY,\n+      HConstants.DEFAULT_ONLINE_LOG_PROVIDER_ENABLED);\n+  }\n+\n+  private static int getWarnResponseTime(Configuration conf) {\n+    return conf.getInt(WARN_RESPONSE_TIME, DEFAULT_WARN_RESPONSE_TIME);\n+  }\n+\n+  private static int getWarnResponseSize(Configuration conf) {\n+    return conf.getInt(WARN_RESPONSE_SIZE, DEFAULT_WARN_RESPONSE_SIZE);\n   }\n \n   protected void initReconfigurable(Configuration confToLoad) {",
        "Label":"Not_Null"
    },
    {
        "ID Number":1025,
        "Patch":"public static CloseRegionRequest buildCloseRegionRequest(ServerName server, byte\n   }\n \n   public static CloseRegionRequest buildCloseRegionRequest(ServerName server, byte[] regionName,\n+    ServerName destinationServer, long closeProcId) {\n+    return ProtobufUtil.getBuilder(server, regionName, destinationServer, closeProcId).build();\n+  }\n+\n+  public static CloseRegionRequest buildCloseRegionRequest(ServerName server, byte[] regionName,\n+    ServerName destinationServer, long closeProcId, boolean evictCache) {\n+    CloseRegionRequest.Builder builder =\n+      getBuilder(server, regionName, destinationServer, closeProcId);\n+    builder.setEvictCache(evictCache);\n+    return builder.build();\n+  }\n+\n+  public static CloseRegionRequest.Builder getBuilder(ServerName server, byte[] regionName,\n     ServerName destinationServer, long closeProcId) {\n     CloseRegionRequest.Builder builder = CloseRegionRequest.newBuilder();\n     RegionSpecifier region =",
        "Stack-trace":"public static CloseRegionRequest buildCloseRegionRequest(ServerName server, byte\n   }\n \n   public static CloseRegionRequest buildCloseRegionRequest(ServerName server, byte[] regionName,\n+    ServerName destinationServer, long closeProcId) {\n+    return ProtobufUtil.getBuilder(server, regionName, destinationServer, closeProcId).build();\n+  }\n+\n+  public static CloseRegionRequest buildCloseRegionRequest(ServerName server, byte[] regionName,\n+    ServerName destinationServer, long closeProcId, boolean evictCache) {\n+    CloseRegionRequest.Builder builder =\n+      getBuilder(server, regionName, destinationServer, closeProcId);\n+    builder.setEvictCache(evictCache);\n+    return builder.build();\n+  }\n+\n+  public static CloseRegionRequest.Builder getBuilder(ServerName server, byte[] regionName,\n     ServerName destinationServer, long closeProcId) {\n     CloseRegionRequest.Builder builder = CloseRegionRequest.newBuilder();\n     RegionSpecifier region =",
        "Label":"Not_Null"
    },
    {
        "ID Number":1026,
        "Patch":" public static CloseRegionRequest buildCloseRegionRequest(ServerName server, byte\n       builder.setServerStartCode(server.getStartcode());\n     }\n     builder.setCloseProcId(closeProcId);\n-    return builder.build();\n+    return builder;\n   }\n \n   public static ProcedureDescription buildProcedureDescription(String signature, String instance,",
        "Stack-trace":" public static CloseRegionRequest buildCloseRegionRequest(ServerName server, byte\n       builder.setServerStartCode(server.getStartcode());\n     }\n     builder.setCloseProcId(closeProcId);\n-    return builder.build();\n+    return builder;\n   }\n \n   public static ProcedureDescription buildProcedureDescription(String signature, String instance,",
        "Label":"Not_Null"
    },
    {
        "ID Number":1017,
        "Patch":" public abstract class CleanerChore<T extends FileCleanerDelegate> extends Schedu\n    *\/\n   public static final String LOG_CLEANER_CHORE_SIZE = \"hbase.log.cleaner.scan.dir.concurrent.size\";\n   static final String DEFAULT_LOG_CLEANER_CHORE_POOL_SIZE = \"1\";\n+  \/**\n+   * Enable the CleanerChore to sort the subdirectories by consumed space and start the cleaning\n+   * with the largest subdirectory. Enabled by default.\n+   *\/\n+  public static final String LOG_CLEANER_CHORE_DIRECTORY_SORTING =\n+    \"hbase.cleaner.directory.sorting\";\n+  static final boolean DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING = true;\n \n   private final DirScanPool pool;\n ",
        "Stack-trace":" public abstract class CleanerChore<T extends FileCleanerDelegate> extends Schedu\n    *\/\n   public static final String LOG_CLEANER_CHORE_SIZE = \"hbase.log.cleaner.scan.dir.concurrent.size\";\n   static final String DEFAULT_LOG_CLEANER_CHORE_POOL_SIZE = \"1\";\n+  \/**\n+   * Enable the CleanerChore to sort the subdirectories by consumed space and start the cleaning\n+   * with the largest subdirectory. Enabled by default.\n+   *\/\n+  public static final String LOG_CLEANER_CHORE_DIRECTORY_SORTING =\n+    \"hbase.cleaner.directory.sorting\";\n+  static final boolean DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING = true;\n \n   private final DirScanPool pool;\n ",
        "Label":"Not_Null"
    },
    {
        "ID Number":1018,
        "Patch":"public abstract class CleanerChore<T extends FileCleanerDelegate> extends Schedu\n   protected List<String> excludeDirs;\n   private CompletableFuture<Boolean> future;\n   private boolean forceRun;\n+  private boolean sortDirectories;\n \n   public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Configuration conf,\n     FileSystem fs, Path oldFileDir, String confKey, DirScanPool pool) {\n@@ -123,6 +131,8 @@ public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Confi\n     if (excludeDirs != null) {\n       LOG.info(\"Cleaner {} excludes sub dirs: {}\", name, excludeDirs);\n     }\n+    sortDirectories = conf.getBoolean(LOG_CLEANER_CHORE_DIRECTORY_SORTING,\n+      DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING);\n     initCleanerChain(confKey);\n   }",
        "Stack-trace":"public abstract class CleanerChore<T extends FileCleanerDelegate> extends Schedu\n   protected List<String> excludeDirs;\n   private CompletableFuture<Boolean> future;\n   private boolean forceRun;\n+  private boolean sortDirectories;\n \n   public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Configuration conf,\n     FileSystem fs, Path oldFileDir, String confKey, DirScanPool pool) {\n@@ -123,6 +131,8 @@ public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Confi\n     if (excludeDirs != null) {\n       LOG.info(\"Cleaner {} excludes sub dirs: {}\", name, excludeDirs);\n     }\n+    sortDirectories = conf.getBoolean(LOG_CLEANER_CHORE_DIRECTORY_SORTING,\n+      DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING);\n     initCleanerChain(confKey);\n   }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1019,
        "Patch":" public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Confi\n     if (excludeDirs != null) {\n       LOG.info(\"Cleaner {} excludes sub dirs: {}\", name, excludeDirs);\n     }\n+    sortDirectories = conf.getBoolean(LOG_CLEANER_CHORE_DIRECTORY_SORTING,\n+      DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING);\n     initCleanerChain(confKey);\n   }",
        "Stack-trace":" public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Confi\n     if (excludeDirs != null) {\n       LOG.info(\"Cleaner {} excludes sub dirs: {}\", name, excludeDirs);\n     }\n+    sortDirectories = conf.getBoolean(LOG_CLEANER_CHORE_DIRECTORY_SORTING,\n+      DEFAULT_LOG_CLEANER_CHORE_DIRECTORY_SORTING);\n     initCleanerChain(confKey);\n   }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1020,
        "Patch":" private void traverseAndDelete(Path dir, boolean root, CompletableFuture<Boolean\n       \/\/ Step.3: Start to traverse and delete the sub-directories.\n       List<CompletableFuture<Boolean>> futures = new ArrayList<>();\n       if (!subDirs.isEmpty()) {\n-        sortByConsumedSpace(subDirs);\n+        if (sortDirectories) {\n+          sortByConsumedSpace(subDirs);\n+        }\n         \/\/ Submit the request of sub-directory deletion.\n         subDirs.forEach(subDir -> {\n           if (!shouldExclude(subDir)) {",
        "Stack-trace":" private void traverseAndDelete(Path dir, boolean root, CompletableFuture<Boolean\n       \/\/ Step.3: Start to traverse and delete the sub-directories.\n       List<CompletableFuture<Boolean>> futures = new ArrayList<>();\n       if (!subDirs.isEmpty()) {\n-        sortByConsumedSpace(subDirs);\n+        if (sortDirectories) {\n+          sortByConsumedSpace(subDirs);\n+        }\n         \/\/ Submit the request of sub-directory deletion.\n         subDirs.forEach(subDir -> {\n           if (!shouldExclude(subDir)) {",
        "Label":"Not_Null"
    },
    {
        "ID Number":1021,
        "Patch":"public class AsyncConnectionImpl implements AsyncConnection {\n \n   private final AtomicBoolean closed = new AtomicBoolean(false);\n \n+  private final String metricsScope;\n   private final Optional<MetricsConnection> metrics;\n \n   private final ClusterStatusListener clusterStatusListener;",
        "Stack-trace":"public class AsyncConnectionImpl implements AsyncConnection {\n \n   private final AtomicBoolean closed = new AtomicBoolean(false);\n \n+  private final String metricsScope;\n   private final Optional<MetricsConnection> metrics;\n \n   private final ClusterStatusListener clusterStatusListener;",
        "Label":"Not_Null"
    },
    {
        "ID Number":1027,
        "Patch":"static String getScope(Configuration conf, String clusterId, Object connectionOb\n   }\n \n   private static final String CNT_BASE = \"rpcCount_\";\n+  private static final String FAILURE_CNT_BASE = \"rpcFailureCount_\";\n   private static final String DRTN_BASE = \"rpcCallDurationMs_\";\n   private static final String REQ_BASE = \"rpcCallRequestSizeBytes_\";\n   private static final String RESP_BASE = \"rpcCallResponseSizeBytes_\";",
        "Stack-trace":"static String getScope(Configuration conf, String clusterId, Object connectionOb\n   }\n \n   private static final String CNT_BASE = \"rpcCount_\";\n+  private static final String FAILURE_CNT_BASE = \"rpcFailureCount_\";\n   private static final String DRTN_BASE = \"rpcCallDurationMs_\";\n   private static final String REQ_BASE = \"rpcCallRequestSizeBytes_\";\n   private static final String RESP_BASE = \"rpcCallResponseSizeBytes_\";",
        "Label":"Not_Null"
    },
    {
        "ID Number":1028,
        "Patch":"private void shutdown() {\n   }\n \n   \/** Report RPC context to metrics system. *\/\n-  public void updateRpc(MethodDescriptor method, Message param, CallStats stats) {\n+  public void updateRpc(MethodDescriptor method, Message param, CallStats stats, boolean failed) {\n     int callsPerServer = stats.getConcurrentCallsPerServer();\n     if (callsPerServer > 0) {\n       concurrentCallsPerServerHist.update(callsPerServer);\n     }\n     \/\/ Update the counter that tracks RPCs by type.\n     final String methodName = method.getService().getName() + \"_\" + method.getName();\n     getMetric(CNT_BASE + methodName, rpcCounters, counterFactory).inc();\n+    if (failed) {\n+      getMetric(FAILURE_CNT_BASE + methodName, rpcCounters, counterFactory).inc();\n+    }\n     \/\/ this implementation is tied directly to protobuf implementation details. would be better\n     \/\/ if we could dispatch based on something static, ie, request Message type.\n     if (method.getService() == ClientService.getDescriptor()) {",
        "Stack-trace":"private void shutdown() {\n   }\n \n   \/** Report RPC context to metrics system. *\/\n-  public void updateRpc(MethodDescriptor method, Message param, CallStats stats) {\n+  public void updateRpc(MethodDescriptor method, Message param, CallStats stats, boolean failed) {\n     int callsPerServer = stats.getConcurrentCallsPerServer();\n     if (callsPerServer > 0) {\n       concurrentCallsPerServerHist.update(callsPerServer);\n     }\n     \/\/ Update the counter that tracks RPCs by type.\n     final String methodName = method.getService().getName() + \"_\" + method.getName();\n     getMetric(CNT_BASE + methodName, rpcCounters, counterFactory).inc();\n+    if (failed) {\n+      getMetric(FAILURE_CNT_BASE + methodName, rpcCounters, counterFactory).inc();\n+    }\n     \/\/ this implementation is tied directly to protobuf implementation details. would be better\n     \/\/ if we could dispatch based on something static, ie, request Message type.\n     if (method.getService() == ClientService.getDescriptor()) {",
        "Label":"Not_Null"
    },
    {
        "ID Number":1029,
        "Patch":"public HFileBlock getMetaBlock(String metaBlockName, boolean cacheBlock) throws\n   }\n \n   \/**\n-   * If expected block is data block, we'll allocate the ByteBuff of block from\n-   * {@link org.apache.hadoop.hbase.io.ByteBuffAllocator} and it's usually an off-heap one,\n-   * otherwise it will allocate from heap.\n+   * Whether we use heap or not depends on our intent to cache the block. We want to avoid\n+   * allocating to off-heap if we intend to cache into the on-heap L1 cache. Otherwise, it's more\n+   * efficient to allocate to off-heap since we can control GC ourselves for those. So our decision\n+   * here breaks down as follows: <br>\n+   * If block cache is disabled, don't use heap. If we're not using the CombinedBlockCache, use heap\n+   * unless caching is disabled for the request. Otherwise, only use heap if caching is enabled and\n+   * the expected block type is not DATA (which goes to off-heap L2 in combined cache).\n    * @see org.apache.hadoop.hbase.io.hfile.HFileBlock.FSReader#readBlockData(long, long, boolean,\n    *      boolean, boolean)\n    *\/\n-  private boolean shouldUseHeap(BlockType expectedBlockType) {\n+  private boolean shouldUseHeap(BlockType expectedBlockType, boolean cacheBlock) {\n     if (!cacheConf.getBlockCache().isPresent()) {\n       return false;\n-    } else if (!cacheConf.isCombinedBlockCache()) {\n-      \/\/ Block to cache in LruBlockCache must be an heap one. So just allocate block memory from\n-      \/\/ heap for saving an extra off-heap to heap copying.\n-      return true;\n     }\n-    return expectedBlockType != null && !expectedBlockType.isData();\n+\n+    \/\/ we only cache a block if cacheBlock is true and caching-on-read is enabled in CacheConfig\n+    \/\/ we can really only check for that if have an expectedBlockType\n+    if (expectedBlockType != null) {\n+      cacheBlock &= cacheConf.shouldCacheBlockOnRead(expectedBlockType.getCategory());\n+    }\n+\n+    if (!cacheConf.isCombinedBlockCache()) {\n+      \/\/ Block to cache in LruBlockCache must be an heap one, if caching enabled. So just allocate\n+      \/\/ block memory from heap for saving an extra off-heap to heap copying in that case.\n+      return cacheBlock;\n+    }",
        "Stack-trace":"public HFileBlock getMetaBlock(String metaBlockName, boolean cacheBlock) throws\n   }\n \n   \/**\n-   * If expected block is data block, we'll allocate the ByteBuff of block from\n-   * {@link org.apache.hadoop.hbase.io.ByteBuffAllocator} and it's usually an off-heap one,\n-   * otherwise it will allocate from heap.\n+   * Whether we use heap or not depends on our intent to cache the block. We want to avoid\n+   * allocating to off-heap if we intend to cache into the on-heap L1 cache. Otherwise, it's more\n+   * efficient to allocate to off-heap since we can control GC ourselves for those. So our decision\n+   * here breaks down as follows: <br>\n+   * If block cache is disabled, don't use heap. If we're not using the CombinedBlockCache, use heap\n+   * unless caching is disabled for the request. Otherwise, only use heap if caching is enabled and\n+   * the expected block type is not DATA (which goes to off-heap L2 in combined cache).\n    * @see org.apache.hadoop.hbase.io.hfile.HFileBlock.FSReader#readBlockData(long, long, boolean,\n    *      boolean, boolean)\n    *\/\n-  private boolean shouldUseHeap(BlockType expectedBlockType) {\n+  private boolean shouldUseHeap(BlockType expectedBlockType, boolean cacheBlock) {\n     if (!cacheConf.getBlockCache().isPresent()) {\n       return false;\n-    } else if (!cacheConf.isCombinedBlockCache()) {\n-      \/\/ Block to cache in LruBlockCache must be an heap one. So just allocate block memory from\n-      \/\/ heap for saving an extra off-heap to heap copying.\n-      return true;\n     }\n-    return expectedBlockType != null && !expectedBlockType.isData();\n+\n+    \/\/ we only cache a block if cacheBlock is true and caching-on-read is enabled in CacheConfig\n+    \/\/ we can really only check for that if have an expectedBlockType\n+    if (expectedBlockType != null) {\n+      cacheBlock &= cacheConf.shouldCacheBlockOnRead(expectedBlockType.getCategory());\n+    }\n+\n+    if (!cacheConf.isCombinedBlockCache()) {\n+      \/\/ Block to cache in LruBlockCache must be an heap one, if caching enabled. So just allocate\n+      \/\/ block memory from heap for saving an extra off-heap to heap copying in that case.\n+      return cacheBlock;\n+    }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1030,
        "Patch":"public HFileBlock readBlock(long dataBlockOffset, long onDiskBlockSize, final bo\n         span.addEvent(\"block cache miss\", attributes);\n         \/\/ Load block from filesystem.\n         HFileBlock hfileBlock = fsBlockReader.readBlockData(dataBlockOffset, onDiskBlockSize, pread,\n-          !isCompaction, shouldUseHeap(expectedBlockType));\n-        validateBlockType(hfileBlock, expectedBlockType);\n+          !isCompaction, shouldUseHeap(expectedBlockType, cacheable));\n+        try {\n+          validateBlockType(hfileBlock, expectedBlockType);\n+        } catch (IOException e) {\n+          hfileBlock.release();\n+          throw e;\n+        }\n         BlockType.BlockCategory category = hfileBlock.getBlockType().getCategory();\n         final boolean cacheCompressed = cacheConf.shouldCacheCompressed(category);\n         final boolean cacheOnRead = cacheConf.shouldCacheBlockOnRead(category);",
        "Stack-trace":"public HFileBlock readBlock(long dataBlockOffset, long onDiskBlockSize, final bo\n         span.addEvent(\"block cache miss\", attributes);\n         \/\/ Load block from filesystem.\n         HFileBlock hfileBlock = fsBlockReader.readBlockData(dataBlockOffset, onDiskBlockSize, pread,\n-          !isCompaction, shouldUseHeap(expectedBlockType));\n-        validateBlockType(hfileBlock, expectedBlockType);\n+          !isCompaction, shouldUseHeap(expectedBlockType, cacheable));\n+        try {\n+          validateBlockType(hfileBlock, expectedBlockType);\n+        } catch (IOException e) {\n+          hfileBlock.release();\n+          throw e;\n+        }\n         BlockType.BlockCategory category = hfileBlock.getBlockType().getCategory();\n         final boolean cacheCompressed = cacheConf.shouldCacheCompressed(category);\n         final boolean cacheOnRead = cacheConf.shouldCacheBlockOnRead(category);",
        "Label":"Not_Null"
    },
    {
        "ID Number":1031,
        "Patch":"public class ZNodePaths {\n   \/\/ znode used for log splitting work assignment\n   public final String splitLogZNode;\n   \/\/ znode containing the state of the load balancer\n+  \/**\n+   * @deprecated Since 2.6.0, will be removed in 4.0.0. We use master local region to store this\n+   *             state.\n+   *\/\n+  @Deprecated\n   public final String balancerZNode;\n   \/\/ znode containing the state of region normalizer\n+  \/**\n+   * @deprecated Since 2.6.0, will be removed in 4.0.0. We use master local region to store this\n+   *             state.\n+   *\/\n+  @Deprecated\n   public final String regionNormalizerZNode;\n   \/\/ znode containing the state of all switches, currently there are split and merge child node.\n+  \/**\n+   * @deprecated Since 2.6.0, will be removed in 4.0.0. We use master local region to store this\n+   *             state.\n+   *\/\n+  @Deprecated\n   public final String switchZNode;\n   \/\/ znode of indicating master maintenance mode\n   public final String masterMaintZNode;",
        "Stack-trace":"public class ZNodePaths {\n   \/\/ znode used for log splitting work assignment\n   public final String splitLogZNode;\n   \/\/ znode containing the state of the load balancer\n+  \/**\n+   * @deprecated Since 2.6.0, will be removed in 4.0.0. We use master local region to store this\n+   *             state.\n+   *\/\n+  @Deprecated\n   public final String balancerZNode;\n   \/\/ znode containing the state of region normalizer\n+  \/**\n+   * @deprecated Since 2.6.0, will be removed in 4.0.0. We use master local region to store this\n+   *             state.\n+   *\/\n+  @Deprecated\n   public final String regionNormalizerZNode;\n   \/\/ znode containing the state of all switches, currently there are split and merge child node.\n+  \/**\n+   * @deprecated Since 2.6.0, will be removed in 4.0.0. We use master local region to store this\n+   *             state.\n+   *\/\n+  @Deprecated\n   public final String switchZNode;\n   \/\/ znode of indicating master maintenance mode\n   public final String masterMaintZNode;",
        "Label":"Not_Null"
    },
    {
        "ID Number":1032,
        "Patch":"  public final String regionNormalizerZNode;\n   \/\/ znode containing the state of all switches, currently there are split and merge child node.\n+  \/**\n+   * @deprecated Since 2.6.0, will be removed in 4.0.0. We use master local region to store this\n+   *             state.\n+   *\/\n+  @Deprecated\n   public final String switchZNode;\n   \/\/ znode of indicating master maintenance mode\n   public final String masterMaintZNode;\n public class ZNodePaths {\n   \/\/ znode containing queues of hfile references to be replicated\n   public final String hfileRefsZNode;\n   \/\/ znode containing the state of the snapshot auto-cleanup\n-  final String snapshotCleanupZNode;\n+  \/**\n+   * @deprecated Since 2.6.0, will be removed in 4.0.0. We use master local region to store this\n+   *             state.\n+   *\/\n+  @Deprecated\n+  public final String snapshotCleanupZNode;\n \n   public ZNodePaths(Configuration conf) {\n     baseZNode = conf.get(ZOOKEEPER_ZNODE_PARENT, DEFAULT_ZOOKEEPER_ZNODE_PARENT);",
        "Stack-trace":"  public final String regionNormalizerZNode;\n   \/\/ znode containing the state of all switches, currently there are split and merge child node.\n+  \/**\n+   * @deprecated Since 2.6.0, will be removed in 4.0.0. We use master local region to store this\n+   *             state.\n+   *\/\n+  @Deprecated\n   public final String switchZNode;\n   \/\/ znode of indicating master maintenance mode\n   public final String masterMaintZNode;\n public class ZNodePaths {\n   \/\/ znode containing queues of hfile references to be replicated\n   public final String hfileRefsZNode;\n   \/\/ znode containing the state of the snapshot auto-cleanup\n-  final String snapshotCleanupZNode;\n+  \/**\n+   * @deprecated Since 2.6.0, will be removed in 4.0.0. We use master local region to store this\n+   *             state.\n+   *\/\n+  @Deprecated\n+  public final String snapshotCleanupZNode;\n \n   public ZNodePaths(Configuration conf) {\n     baseZNode = conf.get(ZOOKEEPER_ZNODE_PARENT, DEFAULT_ZOOKEEPER_ZNODE_PARENT);",
        "Label":"Not_Null"
    },
    {
        "ID Number":1033,
        "Patch":" public static String parseHostFromOldLog(Path p) {\n+    \/\/ Skip master wals\n+    if (p.getName().endsWith(MasterRegionFactory.ARCHIVED_WAL_SUFFIX)) {\n+      return null;\n+    }\n     try {\n       String n = p.getName();\n       int idx = n.lastIndexOf(LOGNAME_SEPARATOR);",
        "Stack-trace":" public static String parseHostFromOldLog(Path p) {\n+    \/\/ Skip master wals\n+    if (p.getName().endsWith(MasterRegionFactory.ARCHIVED_WAL_SUFFIX)) {\n+      return null;\n+    }\n     try {\n       String n = p.getName();\n       int idx = n.lastIndexOf(LOGNAME_SEPARATOR);",
        "Label":"Not_Null"
    },
    {
        "ID Number":1034,
        "Patch":"logger.MetricsConfig.level = WARN\n logger.MetricsSinkAdapter.name = org.apache.hadoop.metrics2.impl.MetricsSinkAdapter\n logger.MetricsSinkAdapter.level = WARN\n \n+# These two settings are workarounds against spurious logs from the minicluster.\n+# See HBASE-4709\n logger.MetricsSystemImpl.name = org.apache.hadoop.metrics2.impl.MetricsSystemImpl\n-logger.MetricsSystemImpl.level = WARN\n+logger.MetricsSystemImpl.level = ERROR\n \n logger.MBeans.name = org.apache.hadoop.metrics2.util.MBeans\n-logger.MBeans.level = WARN\n+logger.MBeans.level = ERROR\n \n logger.directory.name = org.apache.directory\n logger.directory.level = WARN",
        "Stack-trace":"logger.MetricsConfig.level = WARN\n logger.MetricsSinkAdapter.name = org.apache.hadoop.metrics2.impl.MetricsSinkAdapter\n logger.MetricsSinkAdapter.level = WARN\n \n+# These two settings are workarounds against spurious logs from the minicluster.\n+# See HBASE-4709\n logger.MetricsSystemImpl.name = org.apache.hadoop.metrics2.impl.MetricsSystemImpl\n-logger.MetricsSystemImpl.level = WARN\n+logger.MetricsSystemImpl.level = ERROR\n \n logger.MBeans.name = org.apache.hadoop.metrics2.util.MBeans\n-logger.MBeans.level = WARN\n+logger.MBeans.level = ERROR\n \n logger.directory.name = org.apache.directory\n logger.directory.level = WARN",
        "Label":"Not_Null"
    },
    {
        "ID Number":1035,
        "Patch":"logger.RSRpcServices.level = DEBUG\n \n logger.TestJul2Slf4j.name = org.apache.hadoop.hbase.logging.TestJul2Slf4j\n logger.TestJul2Slf4j.level = DEBUG\n+\n+# Avoid log flooded with chore execution time, see HBASE-24646 for more details.\n+logger.ScheduledChore.name = org.apache.hadoop.hbase.ScheduledChore\n+logger.ScheduledChore.level = INFO",
        "Stack-trace":"logger.RSRpcServices.level = DEBUG\n \n logger.TestJul2Slf4j.name = org.apache.hadoop.hbase.logging.TestJul2Slf4j\n logger.TestJul2Slf4j.level = DEBUG\n+\n+# Avoid log flooded with chore execution time, see HBASE-24646 for more details.\n+logger.ScheduledChore.name = org.apache.hadoop.hbase.ScheduledChore\n+logger.ScheduledChore.level = INFO",
        "Label":"Not_Null"
    },
    {
        "ID Number":1036,
        "Patch":" private RegionInfo metaTableConsistencyCheck(Result metaTableRow) {\n           addOverlap(this.highestEndKeyRegionInfo, ri);\n         }\n       }\n+      this.previous = ri;\n+      this.highestEndKeyRegionInfo =\n+        MetaFixer.getRegionInfoWithLargestEndKey(this.highestEndKeyRegionInfo, ri);\n     }\n-    this.previous = ri;\n-    this.highestEndKeyRegionInfo =\n-      MetaFixer.getRegionInfoWithLargestEndKey(this.highestEndKeyRegionInfo, ri);\n     return ri;\n   }\n ",
        "Stack-trace":" private RegionInfo metaTableConsistencyCheck(Result metaTableRow) {\n           addOverlap(this.highestEndKeyRegionInfo, ri);\n         }\n       }\n+      this.previous = ri;\n+      this.highestEndKeyRegionInfo =\n+        MetaFixer.getRegionInfoWithLargestEndKey(this.highestEndKeyRegionInfo, ri);\n     }\n-    this.previous = ri;\n-    this.highestEndKeyRegionInfo =\n-      MetaFixer.getRegionInfoWithLargestEndKey(this.highestEndKeyRegionInfo, ri);\n     return ri;\n   }\n ",
        "Label":"Not_Null"
    },
    {
        "ID Number":1037,
        "Patch":"public class ChoreService {\n    *\/\n   @InterfaceAudience.Private\n   public final static int MIN_CORE_POOL_SIZE = 1;\n+  \/**\n+   * The initial number of threads in the core pool for the {@link ChoreService}.\n+   *\/\n+  public static final String CHORE_SERVICE_INITIAL_POOL_SIZE =\n+    \"hbase.choreservice.initial.pool.size\";\n+  public static final int DEFAULT_CHORE_SERVICE_INITIAL_POOL_SIZE = 1;\n \n   \/**\n    * This thread pool is used to schedule all of the Chores",
        "Stack-trace":"public class ChoreService {\n    *\/\n   @InterfaceAudience.Private\n   public final static int MIN_CORE_POOL_SIZE = 1;\n+  \/**\n+   * The initial number of threads in the core pool for the {@link ChoreService}.\n+   *\/\n+  public static final String CHORE_SERVICE_INITIAL_POOL_SIZE =\n+    \"hbase.choreservice.initial.pool.size\";\n+  public static final int DEFAULT_CHORE_SERVICE_INITIAL_POOL_SIZE = 1;\n \n   \/**\n    * This thread pool is used to schedule all of the Chores",
        "Label":"Not_Null"
    },
    {
        "ID Number":1038,
        "Patch":" public class PolicyBasedChaosMonkey extends ChaosMonkey {\n+  private static final Logger LOG = LoggerFactory.getLogger(PolicyBasedChaosMonkey.class);\n \n   private static final long ONE_SEC = 1000;\n   private static final long ONE_MIN = 60 * ONE_SEC;",
        "Stack-trace":" public class PolicyBasedChaosMonkey extends ChaosMonkey {\n+  private static final Logger LOG = LoggerFactory.getLogger(PolicyBasedChaosMonkey.class);\n \n   private static final long ONE_SEC = 1000;\n   private static final long ONE_MIN = 60 * ONE_SEC;",
        "Label":"Not_Null"
    },
    {
        "ID Number":1039,
        "Patch":"public static <T> T selectWeightedRandomItem(List<Pair<T, Integer>> items) {\n \n   \/** Selects and returns ceil(ratio * items.length) random items from the given array *\/\n   public static <T> List<T> selectRandomItems(T[] items, float ratio) {\n-    int selectedNumber = (int) Math.ceil(items.length * ratio);\n-\n-    List<T> originalItems = Arrays.asList(items);\n-    Collections.shuffle(originalItems);\n-\n-    int startIndex = ThreadLocalRandom.current().nextInt(items.length - selectedNumber);\n-    return originalItems.subList(startIndex, startIndex + selectedNumber);\n+    \/\/ clamp ratio to [0.0,1.0]\n+    ratio = Math.max(Math.min(ratio, 1.0f), 0.0f);\n+    final int selectedNumber = (int) Math.ceil(items.length * ratio);\n+    final ReservoirSample<T> sample = new ReservoirSample<>(selectedNumber);\n+    sample.add(Arrays.stream(items));\n+    final List<T> shuffledItems = sample.getSamplingResult();\n+    Collections.shuffle(shuffledItems);\n+    return shuffledItems;\n   }",
        "Stack-trace":"public static <T> T selectWeightedRandomItem(List<Pair<T, Integer>> items) {\n \n   \/** Selects and returns ceil(ratio * items.length) random items from the given array *\/\n   public static <T> List<T> selectRandomItems(T[] items, float ratio) {\n-    int selectedNumber = (int) Math.ceil(items.length * ratio);\n-\n-    List<T> originalItems = Arrays.asList(items);\n-    Collections.shuffle(originalItems);\n-\n-    int startIndex = ThreadLocalRandom.current().nextInt(items.length - selectedNumber);\n-    return originalItems.subList(startIndex, startIndex + selectedNumber);\n+    \/\/ clamp ratio to [0.0,1.0]\n+    ratio = Math.max(Math.min(ratio, 1.0f), 0.0f);\n+    final int selectedNumber = (int) Math.ceil(items.length * ratio);\n+    final ReservoirSample<T> sample = new ReservoirSample<>(selectedNumber);\n+    sample.add(Arrays.stream(items));\n+    final List<T> shuffledItems = sample.getSamplingResult();\n+    Collections.shuffle(shuffledItems);\n+    return shuffledItems;\n   }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1040,
        "Patch":"public void waitForStop() throws InterruptedException {\n-    monkeyThreadPool.awaitTermination(1, TimeUnit.MINUTES);\n+    if (!monkeyThreadPool.awaitTermination(1, TimeUnit.MINUTES)) {\n+      LOG.warn(\"Some pool threads failed to terminate. Forcing. {}\", monkeyThreadPool);\n+      monkeyThreadPool.shutdownNow();\n+    }\n   }\n ",
        "Stack-trace":"public void waitForStop() throws InterruptedException {\n-    monkeyThreadPool.awaitTermination(1, TimeUnit.MINUTES);\n+    if (!monkeyThreadPool.awaitTermination(1, TimeUnit.MINUTES)) {\n+      LOG.warn(\"Some pool threads failed to terminate. Forcing. {}\", monkeyThreadPool);\n+      monkeyThreadPool.shutdownNow();\n+    }\n   }\n ",
        "Label":"Not_Null"
    },
    {
        "ID Number":1041,
        "Patch":" public class MasterRegistry extends AbstractRpcBasedConnectionRegistry {\n \n   private static final String MASTER_ADDRS_CONF_SEPARATOR = \",\";\n \n+  \/**\n+   * Supplies the default master port we should use given the provided configuration.\n+   * @param conf Configuration to parse from.\n+   *\/\n+  private static int getDefaultMasterPort(Configuration conf) {\n+    final int port = conf.getInt(HConstants.MASTER_PORT, HConstants.DEFAULT_MASTER_PORT);\n+    if (port == 0) {\n+      \/\/ Master port may be set to 0. We should substitute the default port in that case.\n+      return HConstants.DEFAULT_MASTER_PORT;\n+    }\n+    return port;\n+  }\n+\n   \/**\n    * Parses the list of master addresses from the provided configuration. Supported format is comma\n    * separated host[:port] values. If no port number if specified, default master port is assumed.\n    * @param conf Configuration to parse from.\n    *\/\n   public static Set<ServerName> parseMasterAddrs(Configuration conf) throws UnknownHostException {\n-    Set<ServerName> masterAddrs = new HashSet<>();\n-    String configuredMasters = getMasterAddr(conf);\n+    final int defaultPort = getDefaultMasterPort(conf);\n+    final Set<ServerName> masterAddrs = new HashSet<>();\n+    final String configuredMasters = getMasterAddr(conf);\n     for (String masterAddr : Splitter.onPattern(MASTER_ADDRS_CONF_SEPARATOR)\n       .split(configuredMasters)) {\n-      HostAndPort masterHostPort =\n-        HostAndPort.fromString(masterAddr.trim()).withDefaultPort(HConstants.DEFAULT_MASTER_PORT);\n+      final HostAndPort masterHostPort =\n+        HostAndPort.fromString(masterAddr.trim()).withDefaultPort(defaultPort);\n       masterAddrs.add(ServerName.valueOf(masterHostPort.toString(), ServerName.NON_STARTCODE));\n     }\n     Preconditions.checkArgument(!masterAddrs.isEmpty(), \"At least one master address is needed\");",
        "Stack-trace":" public class MasterRegistry extends AbstractRpcBasedConnectionRegistry {\n \n   private static final String MASTER_ADDRS_CONF_SEPARATOR = \",\";\n \n+  \/**\n+   * Supplies the default master port we should use given the provided configuration.\n+   * @param conf Configuration to parse from.\n+   *\/\n+  private static int getDefaultMasterPort(Configuration conf) {\n+    final int port = conf.getInt(HConstants.MASTER_PORT, HConstants.DEFAULT_MASTER_PORT);\n+    if (port == 0) {\n+      \/\/ Master port may be set to 0. We should substitute the default port in that case.\n+      return HConstants.DEFAULT_MASTER_PORT;\n+    }\n+    return port;\n+  }\n+\n   \/**\n    * Parses the list of master addresses from the provided configuration. Supported format is comma\n    * separated host[:port] values. If no port number if specified, default master port is assumed.\n    * @param conf Configuration to parse from.\n    *\/\n   public static Set<ServerName> parseMasterAddrs(Configuration conf) throws UnknownHostException {\n-    Set<ServerName> masterAddrs = new HashSet<>();\n-    String configuredMasters = getMasterAddr(conf);\n+    final int defaultPort = getDefaultMasterPort(conf);\n+    final Set<ServerName> masterAddrs = new HashSet<>();\n+    final String configuredMasters = getMasterAddr(conf);\n     for (String masterAddr : Splitter.onPattern(MASTER_ADDRS_CONF_SEPARATOR)\n       .split(configuredMasters)) {\n-      HostAndPort masterHostPort =\n-        HostAndPort.fromString(masterAddr.trim()).withDefaultPort(HConstants.DEFAULT_MASTER_PORT);\n+      final HostAndPort masterHostPort =\n+        HostAndPort.fromString(masterAddr.trim()).withDefaultPort(defaultPort);\n       masterAddrs.add(ServerName.valueOf(masterHostPort.toString(), ServerName.NON_STARTCODE));\n     }\n     Preconditions.checkArgument(!masterAddrs.isEmpty(), \"At least one master address is needed\");",
        "Label":"Not_Null"
    },
    {
        "ID Number":1042,
        "Patch":" public interface RestoreJob extends Configurable {\n    * Run restore operation\n    * @param dirPaths          path array of WAL log directories\n    * @param fromTables        from tables\n+   * @param restoreRootDir    output file system\n    * @param toTables          to tables\n    * @param fullBackupRestore full backup restore\n    * @throws IOException if running the job fails\n    *\/\n-  void run(Path[] dirPaths, TableName[] fromTables, TableName[] toTables, boolean fullBackupRestore)\n-    throws IOException;\n+  void run(Path[] dirPaths, TableName[] fromTables, Path restoreRootDir, TableName[] toTables,\n+    boolean fullBackupRestore) throws IOException;\n }",
        "Stack-trace":" public interface RestoreJob extends Configurable {\n    * Run restore operation\n    * @param dirPaths          path array of WAL log directories\n    * @param fromTables        from tables\n+   * @param restoreRootDir    output file system\n    * @param toTables          to tables\n    * @param fullBackupRestore full backup restore\n    * @throws IOException if running the job fails\n    *\/\n-  void run(Path[] dirPaths, TableName[] fromTables, TableName[] toTables, boolean fullBackupRestore)\n-    throws IOException;\n+  void run(Path[] dirPaths, TableName[] fromTables, Path restoreRootDir, TableName[] toTables,\n+    boolean fullBackupRestore) throws IOException;\n }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1043,
        "Patch":"private void checkIsDead(final ServerName serverName, final String what)\n    * Assumes onlineServers is locked.\n    * @return ServerName with matching hostname and port.\n    *\/\n-  private ServerName findServerWithSameHostnamePortWithLock(final ServerName serverName) {\n+  public ServerName findServerWithSameHostnamePortWithLock(final ServerName serverName) {\n     ServerName end =\n       ServerName.valueOf(serverName.getHostname(), serverName.getPort(), Long.MAX_VALUE);\n ",
        "Stack-trace":"private void checkIsDead(final ServerName serverName, final String what)\n    * Assumes onlineServers is locked.\n    * @return ServerName with matching hostname and port.\n    *\/\n-  private ServerName findServerWithSameHostnamePortWithLock(final ServerName serverName) {\n+  public ServerName findServerWithSameHostnamePortWithLock(final ServerName serverName) {\n     ServerName end =\n       ServerName.valueOf(serverName.getHostname(), serverName.getPort(), Long.MAX_VALUE);\n ",
        "Label":"Not_Null"
    },
    {
        "ID Number":1044,
        "Patch":"function hbaseprotoc_rebuild\n   # Need to run 'install' instead of 'compile' because shading plugin\n   # is hooked-up to 'install'; else hbase-protocol-shaded is left with\n   # half of its process done.\n-  modules_workers patch hbaseprotoc install -DskipTests -X -DHBasePatchProcess\n+  modules_workers patch hbaseprotoc install -DskipTests -DHBasePatchProcess\n \n   # shellcheck disable=SC2153\n   until [[ $i -eq \"${#MODULE[@]}\" ]]; do",
        "Stack-trace":"function hbaseprotoc_rebuild\n   # Need to run 'install' instead of 'compile' because shading plugin\n   # is hooked-up to 'install'; else hbase-protocol-shaded is left with\n   # half of its process done.\n-  modules_workers patch hbaseprotoc install -DskipTests -X -DHBasePatchProcess\n+  modules_workers patch hbaseprotoc install -DskipTests -DHBasePatchProcess\n \n   # shellcheck disable=SC2153\n   until [[ $i -eq \"${#MODULE[@]}\" ]]; do",
        "Label":"Not_Null"
    },
    {
        "ID Number":1045,
        "Patch":"public static Class<? extends StoreFileTracker> getTrackerClass(String trackerNa\n   public static StoreFileTracker create(Configuration conf, boolean isPrimaryReplica,\n     StoreContext ctx) {\n     Class<? extends StoreFileTracker> tracker = getTrackerClass(conf);\n-    LOG.info(\"instantiating StoreFileTracker impl {}\", tracker.getName());\n+    LOG.debug(\"instantiating StoreFileTracker impl {}\", tracker.getName());\n     return ReflectionUtils.newInstance(tracker, conf, isPrimaryReplica, ctx);\n   }",
        "Stack-trace":"public static Class<? extends StoreFileTracker> getTrackerClass(String trackerNa\n   public static StoreFileTracker create(Configuration conf, boolean isPrimaryReplica,\n     StoreContext ctx) {\n     Class<? extends StoreFileTracker> tracker = getTrackerClass(conf);\n-    LOG.info(\"instantiating StoreFileTracker impl {}\", tracker.getName());\n+    LOG.debug(\"instantiating StoreFileTracker impl {}\", tracker.getName());\n     return ReflectionUtils.newInstance(tracker, conf, isPrimaryReplica, ctx);\n   }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1046,
        "Patch":" static StoreFileTrackerBase createForMigration(Configuration conf, String config\n       throw new IllegalArgumentException(\"Should not specify \" + configName + \" as \"\n         + Trackers.MIGRATION + \" because it can not be nested\");\n     }\n-    LOG.info(\"instantiating StoreFileTracker impl {} as {}\", tracker.getName(), configName);\n+    LOG.debug(\"instantiating StoreFileTracker impl {} as {}\", tracker.getName(), configName);\n     return ReflectionUtils.newInstance(tracker, conf, isPrimaryReplica, ctx);\n   }\n ",
        "Stack-trace":" static StoreFileTrackerBase createForMigration(Configuration conf, String config\n       throw new IllegalArgumentException(\"Should not specify \" + configName + \" as \"\n         + Trackers.MIGRATION + \" because it can not be nested\");\n     }\n-    LOG.info(\"instantiating StoreFileTracker impl {} as {}\", tracker.getName(), configName);\n+    LOG.debug(\"instantiating StoreFileTracker impl {} as {}\", tracker.getName(), configName);\n     return ReflectionUtils.newInstance(tracker, conf, isPrimaryReplica, ctx);\n   }\n ",
        "Label":"Not_Null"
    },
    {
        "ID Number":1047,
        "Patch":"public static HRegion openReadOnlyFileSystemHRegion(final Configuration conf, fi\n     return r.openHRegion(null);\n   }\n \n-  public static void warmupHRegion(final RegionInfo info, final TableDescriptor htd, final WAL wal,\n-    final Configuration conf, final RegionServerServices rsServices,\n+  public static HRegion warmupHRegion(final RegionInfo info, final TableDescriptor htd,\n+    final WAL wal, final Configuration conf, final RegionServerServices rsServices,\n     final CancelableProgressable reporter) throws IOException {\n \n     Objects.requireNonNull(info, \"RegionInfo cannot be null\");",
        "Stack-trace":"public static HRegion openReadOnlyFileSystemHRegion(final Configuration conf, fi\n     return r.openHRegion(null);\n   }\n \n-  public static void warmupHRegion(final RegionInfo info, final TableDescriptor htd, final WAL wal,\n-    final Configuration conf, final RegionServerServices rsServices,\n+  public static HRegion warmupHRegion(final RegionInfo info, final TableDescriptor htd,\n+    final WAL wal, final Configuration conf, final RegionServerServices rsServices,\n     final CancelableProgressable reporter) throws IOException {\n \n     Objects.requireNonNull(info, \"RegionInfo cannot be null\");",
        "Label":"Not_Null"
    },
    {
        "ID Number":1048,
        "Patch":" public void filterRowCells(List<Cell> ignored) throws IOException {\n   }\n \n   \/**\n-   * Fitlers that never filter by modifying the returned List of Cells can inherit this\n+   * Filters that never filter by modifying the returned List of Cells can inherit this\n    * implementation that does nothing. {@inheritDoc}\n    *\/\n   @Override",
        "Stack-trace":" public void filterRowCells(List<Cell> ignored) throws IOException {\n   }\n \n   \/**\n-   * Fitlers that never filter by modifying the returned List of Cells can inherit this\n+   * Filters that never filter by modifying the returned List of Cells can inherit this\n    * implementation that does nothing. {@inheritDoc}\n    *\/\n   @Override",
        "Label":"Not_Null"
    },
    {
        "ID Number":1049,
        "Patch":"public void close() {\n       public void shipped() throws IOException {\n         this.delegate.shipped();\n       }\n+\n+      @Override\n+      public void recordBlockSize(IntConsumer blockSizeConsumer) {\n+        this.delegate.recordBlockSize(blockSizeConsumer);\n+      }\n     };\n   }\n ",
        "Stack-trace":"public void close() {\n       public void shipped() throws IOException {\n         this.delegate.shipped();\n       }\n+\n+      @Override\n+      public void recordBlockSize(IntConsumer blockSizeConsumer) {\n+        this.delegate.recordBlockSize(blockSizeConsumer);\n+      }\n     };\n   }\n ",
        "Label":"Not_Null"
    },
    {
        "ID Number":1050,
        "Patch":"public void run() {\n               threadGroup.setDaemon(false);\n             }\n           }\n+        } catch (NoSuchFieldException e) {\n+          LOG.debug(\"NoSuchFieldException: \" + e.getMessage()\n+            + \"; It might because your Hadoop version > 3.2.3 or 3.3.4, \"\n+            + \"See HBASE-27595 for details.\");\n         } catch (Exception e) {\n           LOG.warn(\"failed to reset thread pool timeout for FsDatasetAsyncDiskService\", e);\n         }",
        "Stack-trace":"public void run() {\n               threadGroup.setDaemon(false);\n             }\n           }\n+        } catch (NoSuchFieldException e) {\n+          LOG.debug(\"NoSuchFieldException: \" + e.getMessage()\n+            + \"; It might because your Hadoop version > 3.2.3 or 3.3.4, \"\n+            + \"See HBASE-27595 for details.\");\n         } catch (Exception e) {\n           LOG.warn(\"failed to reset thread pool timeout for FsDatasetAsyncDiskService\", e);\n         }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1051,
        "Patch":" final public class OnlineLogRecord extends LogEntry {\n   private final int processingTime;\n   private final int queueTime;\n   private final long responseSize;\n+  private final long blockBytesScanned;\n   private final String clientAddress;\n   private final String serverClass;\n   private final String methodName;",
        "Stack-trace":" final public class OnlineLogRecord extends LogEntry {\n   private final int processingTime;\n   private final int queueTime;\n   private final long responseSize;\n+  private final long blockBytesScanned;\n   private final String clientAddress;\n   private final String serverClass;\n   private final String methodName;",
        "Label":"Not_Null"
    },
    {
        "ID Number":1052,
        "Patch":" public long getResponseSize() {\n     return responseSize;\n   }\n \n+  \/**\n+   * Return the amount of block bytes scanned to retrieve the response cells.\n+   *\/\n+  public long getBlockBytesScanned() {\n+    return blockBytesScanned;\n+  }\n+\n   public String getClientAddress() {\n     return clientAddress;\n   }",
        "Stack-trace":" public long getResponseSize() {\n     return responseSize;\n   }\n \n+  \/**\n+   * Return the amount of block bytes scanned to retrieve the response cells.\n+   *\/\n+  public long getBlockBytesScanned() {\n+    return blockBytesScanned;\n+  }\n+\n   public String getClientAddress() {\n     return clientAddress;\n   }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1053,
        "Patch":" public int getMultiServiceCalls() {\n   }\n \n   private OnlineLogRecord(final long startTime, final int processingTime, final int queueTime,\n-    final long responseSize, final String clientAddress, final String serverClass,\n-    final String methodName, final String callDetails, final String param, final String regionName,\n-    final String userName, final int multiGetsCount, final int multiMutationsCount,\n-    final int multiServiceCalls) {\n+    final long responseSize, final long blockBytesScanned, final String clientAddress,\n+    final String serverClass, final String methodName, final String callDetails, final String param,\n+    final String regionName, final String userName, final int multiGetsCount,\n+    final int multiMutationsCount, final int multiServiceCalls) {\n     this.startTime = startTime;\n     this.processingTime = processingTime;\n     this.queueTime = queueTime;\n     this.responseSize = responseSize;\n+    this.blockBytesScanned = blockBytesScanned;\n     this.clientAddress = clientAddress;\n     this.serverClass = serverClass;\n     this.methodName = methodName;",
        "Stack-trace":" public int getMultiServiceCalls() {\n   }\n \n   private OnlineLogRecord(final long startTime, final int processingTime, final int queueTime,\n-    final long responseSize, final String clientAddress, final String serverClass,\n-    final String methodName, final String callDetails, final String param, final String regionName,\n-    final String userName, final int multiGetsCount, final int multiMutationsCount,\n-    final int multiServiceCalls) {\n+    final long responseSize, final long blockBytesScanned, final String clientAddress,\n+    final String serverClass, final String methodName, final String callDetails, final String param,\n+    final String regionName, final String userName, final int multiGetsCount,\n+    final int multiMutationsCount, final int multiServiceCalls) {\n     this.startTime = startTime;\n     this.processingTime = processingTime;\n     this.queueTime = queueTime;\n     this.responseSize = responseSize;\n+    this.blockBytesScanned = blockBytesScanned;\n     this.clientAddress = clientAddress;\n     this.serverClass = serverClass;\n     this.methodName = methodName;",
        "Label":"Not_Null"
    },
    {
        "ID Number":1054,
        "Patch":" public static class OnlineLogRecordBuilder {\n     private int processingTime;\n     private int queueTime;\n     private long responseSize;\n+    private long blockBytesScanned;\n     private String clientAddress;\n     private String serverClass;\n     private String methodName;\n@@ -185,6 +195,14 @@ public OnlineLogRecordBuilder setResponseSize(long responseSize) {\n       return this;\n     }\n \n+    \/**\n+     * Sets the amount of block bytes scanned to retrieve the response cells.\n+     *\/\n+    public OnlineLogRecordBuilder setBlockBytesScanned(long blockBytesScanned) {\n+      this.blockBytesScanned = blockBytesScanned;\n+      return this;\n+    }\n+\n     public OnlineLogRecordBuilder setClientAddress(String clientAddress) {\n       this.clientAddress = clientAddress;\n       return this;",
        "Stack-trace":" public static class OnlineLogRecordBuilder {\n     private int processingTime;\n     private int queueTime;\n     private long responseSize;\n+    private long blockBytesScanned;\n     private String clientAddress;\n     private String serverClass;\n     private String methodName;\n@@ -185,6 +195,14 @@ public OnlineLogRecordBuilder setResponseSize(long responseSize) {\n       return this;\n     }\n \n+    \/**\n+     * Sets the amount of block bytes scanned to retrieve the response cells.\n+     *\/\n+    public OnlineLogRecordBuilder setBlockBytesScanned(long blockBytesScanned) {\n+      this.blockBytesScanned = blockBytesScanned;\n+      return this;\n+    }\n+\n     public OnlineLogRecordBuilder setClientAddress(String clientAddress) {\n       this.clientAddress = clientAddress;\n       return this;",
        "Label":"Not_Null"
    },
    {
        "ID Number":1055,
        "Patch":" public OnlineLogRecordBuilder setResponseSize(long responseSize) {\n       return this;\n     }\n \n+    \/**\n+     * Sets the amount of block bytes scanned to retrieve the response cells.\n+     *\/\n+    public OnlineLogRecordBuilder setBlockBytesScanned(long blockBytesScanned) {\n+      this.blockBytesScanned = blockBytesScanned;\n+      return this;\n+    }\n+\n     public OnlineLogRecordBuilder setClientAddress(String clientAddress) {\n       this.clientAddress = clientAddress;\n       return this;",
        "Stack-trace":" public OnlineLogRecordBuilder setResponseSize(long responseSize) {\n       return this;\n     }\n \n+    \/**\n+     * Sets the amount of block bytes scanned to retrieve the response cells.\n+     *\/\n+    public OnlineLogRecordBuilder setBlockBytesScanned(long blockBytesScanned) {\n+      this.blockBytesScanned = blockBytesScanned;\n+      return this;\n+    }\n+\n     public OnlineLogRecordBuilder setClientAddress(String clientAddress) {\n       this.clientAddress = clientAddress;\n       return this;",
        "Label":"Not_Null"
    },
    {
        "ID Number":1056,
        "Patch":"public OnlineLogRecordBuilder setMultiServiceCalls(int multiServiceCalls) {\n     }\n \n     public OnlineLogRecord build() {\n-      return new OnlineLogRecord(startTime, processingTime, queueTime, responseSize, clientAddress,\n-        serverClass, methodName, callDetails, param, regionName, userName, multiGetsCount,\n-        multiMutationsCount, multiServiceCalls);\n+      return new OnlineLogRecord(startTime, processingTime, queueTime, responseSize,\n+        blockBytesScanned, clientAddress, serverClass, methodName, callDetails, param, regionName,\n+        userName, multiGetsCount, multiMutationsCount, multiServiceCalls);\n     }\n   }\n ",
        "Stack-trace":"public OnlineLogRecordBuilder setMultiServiceCalls(int multiServiceCalls) {\n     }\n \n     public OnlineLogRecord build() {\n-      return new OnlineLogRecord(startTime, processingTime, queueTime, responseSize, clientAddress,\n-        serverClass, methodName, callDetails, param, regionName, userName, multiGetsCount,\n-        multiMutationsCount, multiServiceCalls);\n+      return new OnlineLogRecord(startTime, processingTime, queueTime, responseSize,\n+        blockBytesScanned, clientAddress, serverClass, methodName, callDetails, param, regionName,\n+        userName, multiGetsCount, multiMutationsCount, multiServiceCalls);\n     }\n   }\n ",
        "Label":"Not_Null"
    },
    {
        "ID Number":1057,
        "Patch":" public boolean equals(Object o) {\n \n     return new EqualsBuilder().append(startTime, that.startTime)\n       .append(processingTime, that.processingTime).append(queueTime, that.queueTime)\n-      .append(responseSize, that.responseSize).append(multiGetsCount, that.multiGetsCount)\n+      .append(responseSize, that.responseSize).append(blockBytesScanned, that.blockBytesScanned)\n+      .append(multiGetsCount, that.multiGetsCount)\n       .append(multiMutationsCount, that.multiMutationsCount)\n       .append(multiServiceCalls, that.multiServiceCalls).append(clientAddress, that.clientAddress)\n       .append(serverClass, that.serverClass).append(methodName, that.methodName)",
        "Stack-trace":" public boolean equals(Object o) {\n \n     return new EqualsBuilder().append(startTime, that.startTime)\n       .append(processingTime, that.processingTime).append(queueTime, that.queueTime)\n-      .append(responseSize, that.responseSize).append(multiGetsCount, that.multiGetsCount)\n+      .append(responseSize, that.responseSize).append(blockBytesScanned, that.blockBytesScanned)\n+      .append(multiGetsCount, that.multiGetsCount)\n       .append(multiMutationsCount, that.multiMutationsCount)\n       .append(multiServiceCalls, that.multiServiceCalls).append(clientAddress, that.clientAddress)\n       .append(serverClass, that.serverClass).append(methodName, that.methodName)",
        "Label":"Not_Null"
    },
    {
        "ID Number":1058,
        "Patch":" public int hashCode() {\n     return new HashCodeBuilder(17, 37).append(startTime).append(processingTime).append(queueTime)\n-      .append(responseSize).append(clientAddress).append(serverClass).append(methodName)\n-      .append(callDetails).append(param).append(regionName).append(userName).append(multiGetsCount)\n-      .append(multiMutationsCount).append(multiServiceCalls).toHashCode();\n+      .append(responseSize).append(blockBytesScanned).append(clientAddress).append(serverClass)\n+      .append(methodName).append(callDetails).append(param).append(regionName).append(userName)\n+      .append(multiGetsCount).append(multiMutationsCount).append(multiServiceCalls).toHashCode();\n   }\n ",
        "Stack-trace":" public int hashCode() {\n     return new HashCodeBuilder(17, 37).append(startTime).append(processingTime).append(queueTime)\n-      .append(responseSize).append(clientAddress).append(serverClass).append(methodName)\n-      .append(callDetails).append(param).append(regionName).append(userName).append(multiGetsCount)\n-      .append(multiMutationsCount).append(multiServiceCalls).toHashCode();\n+      .append(responseSize).append(blockBytesScanned).append(clientAddress).append(serverClass)\n+      .append(methodName).append(callDetails).append(param).append(regionName).append(userName)\n+      .append(multiGetsCount).append(multiMutationsCount).append(multiServiceCalls).toHashCode();\n   }\n ",
        "Label":"Not_Null"
    },
    {
        "ID Number":1059,
        "Patch":" public String toJsonPrettyPrint() {\n   public String toString() {\n     return new ToStringBuilder(this).append(\"startTime\", startTime)\n       .append(\"processingTime\", processingTime).append(\"queueTime\", queueTime)\n-      .append(\"responseSize\", responseSize).append(\"clientAddress\", clientAddress)\n-      .append(\"serverClass\", serverClass).append(\"methodName\", methodName)\n-      .append(\"callDetails\", callDetails).append(\"param\", param).append(\"regionName\", regionName)\n-      .append(\"userName\", userName).append(\"multiGetsCount\", multiGetsCount)\n-      .append(\"multiMutationsCount\", multiMutationsCount)\n+      .append(\"responseSize\", responseSize).append(\"blockBytesScanned\", blockBytesScanned)\n+      .append(\"clientAddress\", clientAddress).append(\"serverClass\", serverClass)\n+      .append(\"methodName\", methodName).append(\"callDetails\", callDetails).append(\"param\", param)\n+      .append(\"regionName\", regionName).append(\"userName\", userName)\n+      .append(\"multiGetsCount\", multiGetsCount).append(\"multiMutationsCount\", multiMutationsCount)\n       .append(\"multiServiceCalls\", multiServiceCalls).toString();\n   }\n ",
        "Stack-trace":" public String toJsonPrettyPrint() {\n   public String toString() {\n     return new ToStringBuilder(this).append(\"startTime\", startTime)\n       .append(\"processingTime\", processingTime).append(\"queueTime\", queueTime)\n-      .append(\"responseSize\", responseSize).append(\"clientAddress\", clientAddress)\n-      .append(\"serverClass\", serverClass).append(\"methodName\", methodName)\n-      .append(\"callDetails\", callDetails).append(\"param\", param).append(\"regionName\", regionName)\n-      .append(\"userName\", userName).append(\"multiGetsCount\", multiGetsCount)\n-      .append(\"multiMutationsCount\", multiMutationsCount)\n+      .append(\"responseSize\", responseSize).append(\"blockBytesScanned\", blockBytesScanned)\n+      .append(\"clientAddress\", clientAddress).append(\"serverClass\", serverClass)\n+      .append(\"methodName\", methodName).append(\"callDetails\", callDetails).append(\"param\", param)\n+      .append(\"regionName\", regionName).append(\"userName\", userName)\n+      .append(\"multiGetsCount\", multiGetsCount).append(\"multiMutationsCount\", multiMutationsCount)\n       .append(\"multiServiceCalls\", multiServiceCalls).toString();\n   }\n ",
        "Label":"Not_Null"
    },
    {
        "ID Number":1050,
        "Patch":"public void run() {\n               threadGroup.setDaemon(false);\n             }\n           }\n+        } catch (NoSuchFieldException e) {\n+          LOG.debug(\"NoSuchFieldException: \" + e.getMessage()\n+            + \"; It might because your Hadoop version > 3.2.3 or 3.3.4, \"\n+            + \"See HBASE-27595 for details.\");\n         } catch (Exception e) {\n           LOG.warn(\"failed to reset thread pool timeout for FsDatasetAsyncDiskService\", e);\n         }",
        "Stack-trace":"public void run() {\n               threadGroup.setDaemon(false);\n             }\n           }\n+        } catch (NoSuchFieldException e) {\n+          LOG.debug(\"NoSuchFieldException: \" + e.getMessage()\n+            + \"; It might because your Hadoop version > 3.2.3 or 3.3.4, \"\n+            + \"See HBASE-27595 for details.\");\n         } catch (Exception e) {\n           LOG.warn(\"failed to reset thread pool timeout for FsDatasetAsyncDiskService\", e);\n         }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1051,
        "Patch":" final public class OnlineLogRecord extends LogEntry {\n   private final int processingTime;\n   private final int queueTime;\n   private final long responseSize;\n+  private final long blockBytesScanned;\n   private final String clientAddress;\n   private final String serverClass;\n   private final String methodName;",
        "Stack-trace":" final public class OnlineLogRecord extends LogEntry {\n   private final int processingTime;\n   private final int queueTime;\n   private final long responseSize;\n+  private final long blockBytesScanned;\n   private final String clientAddress;\n   private final String serverClass;\n   private final String methodName;",
        "Label":"Not_Null"
    },
    {
        "ID Number":1052,
        "Patch":" public long getResponseSize() {\n     return responseSize;\n   }\n \n+  \/**\n+   * Return the amount of block bytes scanned to retrieve the response cells.\n+   *\/\n+  public long getBlockBytesScanned() {\n+    return blockBytesScanned;\n+  }\n+\n   public String getClientAddress() {\n     return clientAddress;\n   }",
        "Stack-trace":" public long getResponseSize() {\n     return responseSize;\n   }\n \n+  \/**\n+   * Return the amount of block bytes scanned to retrieve the response cells.\n+   *\/\n+  public long getBlockBytesScanned() {\n+    return blockBytesScanned;\n+  }\n+\n   public String getClientAddress() {\n     return clientAddress;\n   }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1053,
        "Patch":" public int getMultiServiceCalls() {\n   }\n \n   private OnlineLogRecord(final long startTime, final int processingTime, final int queueTime,\n-    final long responseSize, final String clientAddress, final String serverClass,\n-    final String methodName, final String callDetails, final String param, final String regionName,\n-    final String userName, final int multiGetsCount, final int multiMutationsCount,\n-    final int multiServiceCalls) {\n+    final long responseSize, final long blockBytesScanned, final String clientAddress,\n+    final String serverClass, final String methodName, final String callDetails, final String param,\n+    final String regionName, final String userName, final int multiGetsCount,\n+    final int multiMutationsCount, final int multiServiceCalls) {\n     this.startTime = startTime;\n     this.processingTime = processingTime;\n     this.queueTime = queueTime;\n     this.responseSize = responseSize;\n+    this.blockBytesScanned = blockBytesScanned;\n     this.clientAddress = clientAddress;\n     this.serverClass = serverClass;\n     this.methodName = methodName;",
        "Stack-trace":" public int getMultiServiceCalls() {\n   }\n \n   private OnlineLogRecord(final long startTime, final int processingTime, final int queueTime,\n-    final long responseSize, final String clientAddress, final String serverClass,\n-    final String methodName, final String callDetails, final String param, final String regionName,\n-    final String userName, final int multiGetsCount, final int multiMutationsCount,\n-    final int multiServiceCalls) {\n+    final long responseSize, final long blockBytesScanned, final String clientAddress,\n+    final String serverClass, final String methodName, final String callDetails, final String param,\n+    final String regionName, final String userName, final int multiGetsCount,\n+    final int multiMutationsCount, final int multiServiceCalls) {\n     this.startTime = startTime;\n     this.processingTime = processingTime;\n     this.queueTime = queueTime;\n     this.responseSize = responseSize;\n+    this.blockBytesScanned = blockBytesScanned;\n     this.clientAddress = clientAddress;\n     this.serverClass = serverClass;\n     this.methodName = methodName;",
        "Label":"Not_Null"
    },
    {
        "ID Number":1060,
        "Patch":" public class TestTooLargeLog {\n \n   @BeforeClass\n   public static void setUpBeforeClass() throws Exception {\n+    \/\/ Slow log needs to be enabled initially to spin up the SlowLogQueueService\n     TEST_UTIL.getConfiguration().setBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY, true);\n-    TEST_UTIL.getConfiguration().setInt(\"hbase.ipc.warn.response.size\", 100);\n+    TEST_UTIL.getConfiguration().setInt(\"hbase.ipc.warn.response.size\",\n+      HConstants.DEFAULT_BLOCKSIZE \/ 2);\n     TEST_UTIL.startMiniCluster(1);\n     ADMIN = TEST_UTIL.getAdmin();\n   }",
        "Stack-trace":" public class TestTooLargeLog {\n \n   @BeforeClass\n   public static void setUpBeforeClass() throws Exception {\n+    \/\/ Slow log needs to be enabled initially to spin up the SlowLogQueueService\n     TEST_UTIL.getConfiguration().setBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY, true);\n-    TEST_UTIL.getConfiguration().setInt(\"hbase.ipc.warn.response.size\", 100);\n+    TEST_UTIL.getConfiguration().setInt(\"hbase.ipc.warn.response.size\",\n+      HConstants.DEFAULT_BLOCKSIZE \/ 2);\n     TEST_UTIL.startMiniCluster(1);\n     ADMIN = TEST_UTIL.getAdmin();\n   }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1061,
        "Patch":"public static void afterClass() throws Exception {\n+    TEST_UTIL.shutdownMiniCluster();\n+  }\n+\n   \/**\n    * Tests that we can trigger based on blocks scanned, and also that we properly pass the block\n    * bytes scanned value through to the client.\n    *\/\n   @Test\n-  public void testLogLargeBlockBytesScanned() throws IOException, InterruptedException {\n+  public void testLogLargeBlockBytesScanned() throws IOException {\n+    \/\/ Turn off slow log buffer for initial loadTable, because we were seeing core dump\n+    \/\/ issues coming from that slow log entry. We will re-enable below.\n+    HRegionServer regionServer = TEST_UTIL.getHBaseCluster().getRegionServer(0);\n+    regionServer.getConfiguration().setBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY, false);\n+    regionServer.updateConfiguration();\n+\n     byte[] family = Bytes.toBytes(\"0\");\n     Table table = TEST_UTIL.createTable(TableName.valueOf(\"testLogLargeBlockBytesScanned\"), family);\n     TEST_UTIL.loadTable(table, family);\n     TEST_UTIL.flush(table.getName());\n \n-    Set<ServerName> server =\n-      Collections.singleton(TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName());\n+    Set<ServerName> server = Collections.singleton(regionServer.getServerName());\n     Admin admin = TEST_UTIL.getAdmin();\n-    admin.clearSlowLogResponses(server);\n+\n+    \/\/ Turn on slow log so we capture large scan below\n+    regionServer.getConfiguration().setBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY, true);\n+    regionServer.updateConfiguration();\n \n     Scan scan = new Scan();\n     scan.setCaching(1);",
        "Stack-trace":"public static void afterClass() throws Exception {\n+    TEST_UTIL.shutdownMiniCluster();\n+  }\n+\n   \/**\n    * Tests that we can trigger based on blocks scanned, and also that we properly pass the block\n    * bytes scanned value through to the client.\n    *\/\n   @Test\n-  public void testLogLargeBlockBytesScanned() throws IOException, InterruptedException {\n+  public void testLogLargeBlockBytesScanned() throws IOException {\n+    \/\/ Turn off slow log buffer for initial loadTable, because we were seeing core dump\n+    \/\/ issues coming from that slow log entry. We will re-enable below.\n+    HRegionServer regionServer = TEST_UTIL.getHBaseCluster().getRegionServer(0);\n+    regionServer.getConfiguration().setBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY, false);\n+    regionServer.updateConfiguration();\n+\n     byte[] family = Bytes.toBytes(\"0\");\n     Table table = TEST_UTIL.createTable(TableName.valueOf(\"testLogLargeBlockBytesScanned\"), family);\n     TEST_UTIL.loadTable(table, family);\n     TEST_UTIL.flush(table.getName());\n \n-    Set<ServerName> server =\n-      Collections.singleton(TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName());\n+    Set<ServerName> server = Collections.singleton(regionServer.getServerName());\n     Admin admin = TEST_UTIL.getAdmin();\n-    admin.clearSlowLogResponses(server);\n+\n+    \/\/ Turn on slow log so we capture large scan below\n+    regionServer.getConfiguration().setBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY, true);\n+    regionServer.updateConfiguration();\n \n     Scan scan = new Scan();\n     scan.setCaching(1);",
        "Label":"Not_Null"
    },
    {
        "ID Number":1062,
        "Patch":" public void testLogLargeBlockBytesScanned() throws IOException, InterruptedExcep\n       scanner.next();\n     }\n \n-    List<LogEntry> entries =\n-      admin.getLogEntries(server, \"LARGE_LOG\", ServerType.REGION_SERVER, 1, Collections.emptyMap());\n+    List<LogEntry> entries = admin.getLogEntries(server, \"LARGE_LOG\", ServerType.REGION_SERVER, 100,\n+      Collections.emptyMap());\n \n     assertEquals(1, entries.size());\n \n     OnlineLogRecord record = (OnlineLogRecord) entries.get(0);\n-    System.out.println(record.toJsonPrettyPrint());\n \n     assertTrue(\"expected \" + record.getBlockBytesScanned() + \" to be >= 100\",\n       record.getBlockBytesScanned() >= 100);",
        "Stack-trace":" public void testLogLargeBlockBytesScanned() throws IOException, InterruptedExcep\n       scanner.next();\n     }\n \n-    List<LogEntry> entries =\n-      admin.getLogEntries(server, \"LARGE_LOG\", ServerType.REGION_SERVER, 1, Collections.emptyMap());\n+    List<LogEntry> entries = admin.getLogEntries(server, \"LARGE_LOG\", ServerType.REGION_SERVER, 100,\n+      Collections.emptyMap());\n \n     assertEquals(1, entries.size());\n \n     OnlineLogRecord record = (OnlineLogRecord) entries.get(0);\n-    System.out.println(record.toJsonPrettyPrint());\n \n     assertTrue(\"expected \" + record.getBlockBytesScanned() + \" to be >= 100\",\n       record.getBlockBytesScanned() >= 100);",
        "Label":"Not_Null"
    },
    {
        "ID Number":1063,
        "Patch":" public class TestHFileCleaner {\n \n   private static DirScanPool POOL;\n \n+  private static String MOCK_ARCHIVED_HFILE_DIR =\n+    HConstants.HFILE_ARCHIVE_DIRECTORY + \"\/namespace\/table\/region\";\n+\n   @BeforeClass\n   public static void setupCluster() throws Exception {\n     \/\/ have to use a minidfs cluster because the localfs doesn't modify file times correctly",
        "Stack-trace":" public class TestHFileCleaner {\n \n   private static DirScanPool POOL;\n \n+  private static String MOCK_ARCHIVED_HFILE_DIR =\n+    HConstants.HFILE_ARCHIVE_DIRECTORY + \"\/namespace\/table\/region\";\n+\n   @BeforeClass\n   public static void setupCluster() throws Exception {\n     \/\/ have to use a minidfs cluster because the localfs doesn't modify file times correctly",
        "Label":"Not_Null"
    },
    {
        "ID Number":1064,
        "Patch":"public byte[] compress(byte[] valueArray, int valueOffset, int valueLength) thro\n       return compressed;\n     }\n \n-    public int decompress(InputStream in, int inLength, byte[] outArray, int outOffset,\n+    public void decompress(InputStream in, int inLength, byte[] outArray, int outOffset,\n       int outLength) throws IOException {\n \n       \/\/ Our input is a sequence of bounded byte ranges (call them segments), with",
        "Stack-trace":"public byte[] compress(byte[] valueArray, int valueOffset, int valueLength) thro\n       return compressed;\n     }\n \n-    public int decompress(InputStream in, int inLength, byte[] outArray, int outOffset,\n+    public void decompress(InputStream in, int inLength, byte[] outArray, int outOffset,\n       int outLength) throws IOException {\n \n       \/\/ Our input is a sequence of bounded byte ranges (call them segments), with",
        "Label":"Not_Null"
    },
    {
        "ID Number":1065,
        "Patch":"public int decompress(InputStream in, int inLength, byte[] outArray, int outOffs\n       } else {\n         lowerIn.setDelegate(in, inLength);\n       }\n-\n-      \/\/ Caller must handle short reads.\n-      \/\/ With current Hadoop compression codecs all 'outLength' bytes are read in here, so not\n-      \/\/ an issue for now.\n-      return compressedIn.read(outArray, outOffset, outLength);\n+      if (outLength == 0) {\n+        \/\/ The BufferedInputStream will return earlier and skip reading anything if outLength == 0,\n+        \/\/ but in fact for an empty value, the compressed output still contains some metadata so the\n+        \/\/ compressed size is not 0, so here we need to manually skip inLength bytes otherwise the\n+        \/\/ next read on this stream will start from an invalid position and cause critical problem,\n+        \/\/ such as data loss when splitting wal or replicating wal.\n+        IOUtils.skipFully(in, inLength);\n+      } else {\n+        IOUtils.readFully(compressedIn, outArray, outOffset, outLength);\n+      }\n     }\n \n     public void clear() {",
        "Stack-trace":"public int decompress(InputStream in, int inLength, byte[] outArray, int outOffs\n       } else {\n         lowerIn.setDelegate(in, inLength);\n       }\n-\n-      \/\/ Caller must handle short reads.\n-      \/\/ With current Hadoop compression codecs all 'outLength' bytes are read in here, so not\n-      \/\/ an issue for now.\n-      return compressedIn.read(outArray, outOffset, outLength);\n+      if (outLength == 0) {\n+        \/\/ The BufferedInputStream will return earlier and skip reading anything if outLength == 0,\n+        \/\/ but in fact for an empty value, the compressed output still contains some metadata so the\n+        \/\/ compressed size is not 0, so here we need to manually skip inLength bytes otherwise the\n+        \/\/ next read on this stream will start from an invalid position and cause critical problem,\n+        \/\/ such as data loss when splitting wal or replicating wal.\n+        IOUtils.skipFully(in, inLength);\n+      } else {\n+        IOUtils.readFully(compressedIn, outArray, outOffset, outLength);\n+      }\n     }\n \n     public void clear() {",
        "Label":"Not_Null"
    },
    {
        "ID Number":1066,
        "Patch":" public synchronized V putIfAbsent(K key, V value) {\n     if (index < 0) {\n       COWEntry<K, V> newEntry = new COWEntry<>(key, value);\n       this.holder = current.insert(-(index + 1), newEntry);\n-      return value;\n+      \/\/ putIfAbsent contract requires returning null if no previous entry exists\n+      return null;\n     }\n     return current.entries[index].getValue();\n   }",
        "Stack-trace":" public synchronized V putIfAbsent(K key, V value) {\n     if (index < 0) {\n       COWEntry<K, V> newEntry = new COWEntry<>(key, value);\n       this.holder = current.insert(-(index + 1), newEntry);\n-      return value;\n+      \/\/ putIfAbsent contract requires returning null if no previous entry exists\n+      return null;\n     }\n     return current.entries[index].getValue();\n   }",
        "Label":"Not_Null"
    },
    {
        "ID Number":1067,
        "Patch":"private WriterLength getNewWriter(byte[] tableName, byte[] family, Configuration\n         HFileContextBuilder contextBuilder = new HFileContextBuilder().withCompression(compression)\n           .withDataBlockEncoding(encoding).withChecksumType(StoreUtils.getChecksumType(conf))\n           .withBytesPerCheckSum(StoreUtils.getBytesPerChecksum(conf)).withBlockSize(blockSize)\n-          .withColumnFamily(family).withTableName(tableName);\n+          .withColumnFamily(family).withTableName(tableName)\n+          .withCreateTime(EnvironmentEdgeManager.currentTime());\n \n         if (HFile.getFormatVersion(conf) >= HFile.MIN_FORMAT_VERSION_WITH_TAGS) {\n           contextBuilder.withIncludesTags(true);",
        "Stack-trace":"private WriterLength getNewWriter(byte[] tableName, byte[] family, Configuration\n         HFileContextBuilder contextBuilder = new HFileContextBuilder().withCompression(compression)\n           .withDataBlockEncoding(encoding).withChecksumType(StoreUtils.getChecksumType(conf))\n           .withBytesPerCheckSum(StoreUtils.getBytesPerChecksum(conf)).withBlockSize(blockSize)\n-          .withColumnFamily(family).withTableName(tableName);\n+          .withColumnFamily(family).withTableName(tableName)\n+          .withCreateTime(EnvironmentEdgeManager.currentTime());\n \n         if (HFile.getFormatVersion(conf) >= HFile.MIN_FORMAT_VERSION_WITH_TAGS) {\n           contextBuilder.withIncludesTags(true);",
        "Label":"Not_Null"
    },
    {
        "ID Number":1061,
        "Patch":"public static void afterClass() throws Exception {\n+    TEST_UTIL.shutdownMiniCluster();\n+  }\n+\n   \/**\n    * Tests that we can trigger based on blocks scanned, and also that we properly pass the block\n    * bytes scanned value through to the client.\n    *\/\n   @Test\n-  public void testLogLargeBlockBytesScanned() throws IOException, InterruptedException {\n+  public void testLogLargeBlockBytesScanned() throws IOException {\n+    \/\/ Turn off slow log buffer for initial loadTable, because we were seeing core dump\n+    \/\/ issues coming from that slow log entry. We will re-enable below.\n+    HRegionServer regionServer = TEST_UTIL.getHBaseCluster().getRegionServer(0);\n+    regionServer.getConfiguration().setBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY, false);\n+    regionServer.updateConfiguration();\n+\n     byte[] family = Bytes.toBytes(\"0\");\n     Table table = TEST_UTIL.createTable(TableName.valueOf(\"testLogLargeBlockBytesScanned\"), family);\n     TEST_UTIL.loadTable(table, family);\n     TEST_UTIL.flush(table.getName());\n \n-    Set<ServerName> server =\n-      Collections.singleton(TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName());\n+    Set<ServerName> server = Collections.singleton(regionServer.getServerName());\n     Admin admin = TEST_UTIL.getAdmin();\n-    admin.clearSlowLogResponses(server);\n+\n+    \/\/ Turn on slow log so we capture large scan below\n+    regionServer.getConfiguration().setBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY, true);\n+    regionServer.updateConfiguration();\n \n     Scan scan = new Scan();\n     scan.setCaching(1);",
        "Stack-trace":"public static void afterClass() throws Exception {\n+    TEST_UTIL.shutdownMiniCluster();\n+  }\n+\n   \/**\n    * Tests that we can trigger based on blocks scanned, and also that we properly pass the block\n    * bytes scanned value through to the client.\n    *\/\n   @Test\n-  public void testLogLargeBlockBytesScanned() throws IOException, InterruptedException {\n+  public void testLogLargeBlockBytesScanned() throws IOException {\n+    \/\/ Turn off slow log buffer for initial loadTable, because we were seeing core dump\n+    \/\/ issues coming from that slow log entry. We will re-enable below.\n+    HRegionServer regionServer = TEST_UTIL.getHBaseCluster().getRegionServer(0);\n+    regionServer.getConfiguration().setBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY, false);\n+    regionServer.updateConfiguration();\n+\n     byte[] family = Bytes.toBytes(\"0\");\n     Table table = TEST_UTIL.createTable(TableName.valueOf(\"testLogLargeBlockBytesScanned\"), family);\n     TEST_UTIL.loadTable(table, family);\n     TEST_UTIL.flush(table.getName());\n \n-    Set<ServerName> server =\n-      Collections.singleton(TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName());\n+    Set<ServerName> server = Collections.singleton(regionServer.getServerName());\n     Admin admin = TEST_UTIL.getAdmin();\n-    admin.clearSlowLogResponses(server);\n+\n+    \/\/ Turn on slow log so we capture large scan below\n+    regionServer.getConfiguration().setBoolean(HConstants.SLOW_LOG_BUFFER_ENABLED_KEY, true);\n+    regionServer.updateConfiguration();\n \n     Scan scan = new Scan();\n     scan.setCaching(1);",
        "Label":"Not_Null"
    },
    {
        "ID Number":1062,
        "Patch":" public void testLogLargeBlockBytesScanned() throws IOException, InterruptedExcep\n       scanner.next();\n     }\n \n-    List<LogEntry> entries =\n-      admin.getLogEntries(server, \"LARGE_LOG\", ServerType.REGION_SERVER, 1, Collections.emptyMap());\n+    List<LogEntry> entries = admin.getLogEntries(server, \"LARGE_LOG\", ServerType.REGION_SERVER, 100,\n+      Collections.emptyMap());\n \n     assertEquals(1, entries.size());\n \n     OnlineLogRecord record = (OnlineLogRecord) entries.get(0);\n-    System.out.println(record.toJsonPrettyPrint());\n \n     assertTrue(\"expected \" + record.getBlockBytesScanned() + \" to be >= 100\",\n       record.getBlockBytesScanned() >= 100);",
        "Stack-trace":" public void testLogLargeBlockBytesScanned() throws IOException, InterruptedExcep\n       scanner.next();\n     }\n \n-    List<LogEntry> entries =\n-      admin.getLogEntries(server, \"LARGE_LOG\", ServerType.REGION_SERVER, 1, Collections.emptyMap());\n+    List<LogEntry> entries = admin.getLogEntries(server, \"LARGE_LOG\", ServerType.REGION_SERVER, 100,\n+      Collections.emptyMap());\n \n     assertEquals(1, entries.size());\n \n     OnlineLogRecord record = (OnlineLogRecord) entries.get(0);\n-    System.out.println(record.toJsonPrettyPrint());\n \n     assertTrue(\"expected \" + record.getBlockBytesScanned() + \" to be >= 100\",\n       record.getBlockBytesScanned() >= 100);",
        "Label":"Not_Null"
    },
    {
        "ID Number":1063,
        "Patch":" public class TestHFileCleaner {\n \n   private static DirScanPool POOL;\n \n+  private static String MOCK_ARCHIVED_HFILE_DIR =\n+    HConstants.HFILE_ARCHIVE_DIRECTORY + \"\/namespace\/table\/region\";\n+\n   @BeforeClass\n   public static void setupCluster() throws Exception {\n     \/\/ have to use a minidfs cluster because the localfs doesn't modify file times correctly",
        "Stack-trace":" public class TestHFileCleaner {\n \n   private static DirScanPool POOL;\n \n+  private static String MOCK_ARCHIVED_HFILE_DIR =\n+    HConstants.HFILE_ARCHIVE_DIRECTORY + \"\/namespace\/table\/region\";\n+\n   @BeforeClass\n   public static void setupCluster() throws Exception {\n     \/\/ have to use a minidfs cluster because the localfs doesn't modify file times correctly",
        "Label":"Not_Null"
    },
    {
        "ID Number":1064,
        "Patch":"public byte[] compress(byte[] valueArray, int valueOffset, int valueLength) thro\n       return compressed;\n     }\n \n-    public int decompress(InputStream in, int inLength, byte[] outArray, int outOffset,\n+    public void decompress(InputStream in, int inLength, byte[] outArray, int outOffset,\n       int outLength) throws IOException {\n \n       \/\/ Our input is a sequence of bounded byte ranges (call them segments), with",
        "Stack-trace":"public byte[] compress(byte[] valueArray, int valueOffset, int valueLength) thro\n       return compressed;\n     }\n \n-    public int decompress(InputStream in, int inLength, byte[] outArray, int outOffset,\n+    public void decompress(InputStream in, int inLength, byte[] outArray, int outOffset,\n       int outLength) throws IOException {\n \n       \/\/ Our input is a sequence of bounded byte ranges (call them segments), with",
        "Label":"Not_Null"
    },
    {
        "ID Number":1068,
        "Patch":"public HFileContextBuilder(final HFileContext hfc) {\n     this.columnFamily = hfc.getColumnFamily();\n     this.tableName = hfc.getTableName();\n     this.cellComparator = hfc.getCellComparator();\n+    this.indexBlockEncoding = hfc.getIndexBlockEncoding();\n   }\n \n   public HFileContextBuilder withHBaseCheckSum(boolean useHBaseCheckSum) {",
        "Stack-trace":"public HFileContextBuilder(final HFileContext hfc) {\n     this.columnFamily = hfc.getColumnFamily();\n     this.tableName = hfc.getTableName();\n     this.cellComparator = hfc.getCellComparator();\n+    this.indexBlockEncoding = hfc.getIndexBlockEncoding();\n   }\n \n   public HFileContextBuilder withHBaseCheckSum(boolean useHBaseCheckSum) {",
        "Label":"Not_Null"
    },
    {
        "ID Number":1046,
        "Patch":" static StoreFileTrackerBase createForMigration(Configuration conf, String config\n       throw new IllegalArgumentException(\"Should not specify \" + configName + \" as \"\n         + Trackers.MIGRATION + \" because it can not be nested\");\n     }\n-    LOG.info(\"instantiating StoreFileTracker impl {} as {}\", tracker.getName(), configName);\n+    LOG.debug(\"instantiating StoreFileTracker impl {} as {}\", tracker.getName(), configName);\n     return ReflectionUtils.newInstance(tracker, conf, isPrimaryReplica, ctx);\n   }\n ",
        "Stack-trace":" static StoreFileTrackerBase createForMigration(Configuration conf, String config\n       throw new IllegalArgumentException(\"Should not specify \" + configName + \" as \"\n         + Trackers.MIGRATION + \" because it can not be nested\");\n     }\n-    LOG.info(\"instantiating StoreFileTracker impl {} as {}\", tracker.getName(), configName);\n+    LOG.debug(\"instantiating StoreFileTracker impl {} as {}\", tracker.getName(), configName);\n     return ReflectionUtils.newInstance(tracker, conf, isPrimaryReplica, ctx);\n   }\n ",
        "Label":"Not_Null"
    },
    {
        "ID Number":1047,
        "Patch":"public static HRegion openReadOnlyFileSystemHRegion(final Configuration conf, fi\n     return r.openHRegion(null);\n   }\n \n-  public static void warmupHRegion(final RegionInfo info, final TableDescriptor htd, final WAL wal,\n-    final Configuration conf, final RegionServerServices rsServices,\n+  public static HRegion warmupHRegion(final RegionInfo info, final TableDescriptor htd,\n+    final WAL wal, final Configuration conf, final RegionServerServices rsServices,\n     final CancelableProgressable reporter) throws IOException {\n \n     Objects.requireNonNull(info, \"RegionInfo cannot be null\");",
        "Stack-trace":"public static HRegion openReadOnlyFileSystemHRegion(final Configuration conf, fi\n     return r.openHRegion(null);\n   }\n \n-  public static void warmupHRegion(final RegionInfo info, final TableDescriptor htd, final WAL wal,\n-    final Configuration conf, final RegionServerServices rsServices,\n+  public static HRegion warmupHRegion(final RegionInfo info, final TableDescriptor htd,\n+    final WAL wal, final Configuration conf, final RegionServerServices rsServices,\n     final CancelableProgressable reporter) throws IOException {\n \n     Objects.requireNonNull(info, \"RegionInfo cannot be null\");",
        "Label":"Not_Null"
    },
    {
        "ID Number":1048,
        "Patch":" public void filterRowCells(List<Cell> ignored) throws IOException {\n   }\n \n   \/**\n-   * Fitlers that never filter by modifying the returned List of Cells can inherit this\n+   * Filters that never filter by modifying the returned List of Cells can inherit this\n    * implementation that does nothing. {@inheritDoc}\n    *\/\n   @Override",
        "Stack-trace":" public void filterRowCells(List<Cell> ignored) throws IOException {\n   }\n \n   \/**\n-   * Fitlers that never filter by modifying the returned List of Cells can inherit this\n+   * Filters that never filter by modifying the returned List of Cells can inherit this\n    * implementation that does nothing. {@inheritDoc}\n    *\/\n   @Override",
        "Label":"Not_Null"
    },
    {
        "ID Number":1049,
        "Patch":"public void close() {\n       public void shipped() throws IOException {\n         this.delegate.shipped();\n       }\n+\n+      @Override\n+      public void recordBlockSize(IntConsumer blockSizeConsumer) {\n+        this.delegate.recordBlockSize(blockSizeConsumer);\n+      }\n     };\n   }\n ",
        "Stack-trace":"public void close() {\n       public void shipped() throws IOException {\n         this.delegate.shipped();\n       }\n+\n+      @Override\n+      public void recordBlockSize(IntConsumer blockSizeConsumer) {\n+        this.delegate.recordBlockSize(blockSizeConsumer);\n+      }\n     };\n   }\n ",
        "Label":"Not_Null"
    }
]
