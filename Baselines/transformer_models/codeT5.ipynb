{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix\n",
    ")\n",
    "from transformers import (\n",
    "    RobertaTokenizer, RobertaForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback, \n",
    "    TrainerCallback\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"DATA_PATH\": \"/root/workspace/npe_project/Dataset/NPEPatches.json\",\n",
    "    \"MODEL_PATH\": \"/root/workspace/npe_project/Salesforce_CodeT5_base\",\n",
    "    \"OUTPUT_DIR\": \"./results\",\n",
    "    \"LOGS_DIR\": \"./logs\",\n",
    "    \"MAX_LENGTH\": 512,\n",
    "    \"TEST_SIZE\": 0.2,\n",
    "    \"RANDOM_STATE\": 42,\n",
    "    \"LABEL_MAPPING\": {'NPE': 1, 'Non-NPE': 0}\n",
    "}\n",
    "\n",
    "# Training Arguments\n",
    "TRAINING_CONFIG = {\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"fp16\": True,\n",
    "    \"save_total_limit\": 1\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('npe_training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NPECommitDataset(Dataset):\n",
    "    \"\"\"Dataset class for NPE commit classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: list, labels: list, tokenizer, max_len: int = CONFIG[\"MAX_LENGTH\"]):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class MetricsLoggerCallback(TrainerCallback):\n",
    "    \"\"\"Callback for logging training metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics_history = {\n",
    "            \"accuracy\": [], \"precision\": [], \"recall\": [],\n",
    "            \"f1_score\": [], \"fpr\": [], \"fnr\": []\n",
    "        }\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"Log metrics after evaluation.\"\"\"\n",
    "        if metrics is not None:\n",
    "            for metric in self.metrics_history.keys():\n",
    "                self.metrics_history[metric].append(\n",
    "                    metrics.get(f\"eval_{metric}\", 0)\n",
    "                )\n",
    "\n",
    "def compute_metrics(eval_pred) -> dict:\n",
    "    \"\"\"Compute classification metrics.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average=\"binary\"),\n",
    "        \"recall\": recall_score(labels, predictions, average=\"binary\"),\n",
    "        \"f1_score\": f1_score(labels, predictions, average=\"binary\"),\n",
    "        \"fpr\": fp / (fp + tn),\n",
    "        \"fnr\": fn / (fn + tp)\n",
    "    }\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"Load and prepare data for training.\"\"\"\n",
    "    logger.info(\"Loading and preparing data...\")\n",
    "    \n",
    "    # Load and clean data\n",
    "    data = pd.read_csv(CONFIG[\"DATA_PATH\"])\n",
    "    data = data.drop_duplicates(subset=[\"Patch\"])\n",
    "    data = data.dropna(subset=[\"Category\"])\n",
    "    \n",
    "    # Map labels\n",
    "    data[\"Category\"] = data[\"Category\"].map(CONFIG[\"LABEL_MAPPING\"])\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data[\"Patch\"], data[\"Category\"],\n",
    "        test_size=CONFIG[\"TEST_SIZE\"],\n",
    "        random_state=CONFIG[\"RANDOM_STATE\"]\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    try:\n",
    "        # Prepare data\n",
    "        X_train, X_test, y_train, y_test = prepare_data()\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(CONFIG[\"MODEL_PATH\"])\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "            CONFIG[\"MODEL_PATH\"], \n",
    "            num_labels=2\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = NPECommitDataset(\n",
    "            X_train.tolist(), \n",
    "            y_train.tolist(), \n",
    "            tokenizer\n",
    "        )\n",
    "        test_dataset = NPECommitDataset(\n",
    "            X_test.tolist(), \n",
    "            y_test.tolist(), \n",
    "            tokenizer\n",
    "        )\n",
    "        \n",
    "        # Setup training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=CONFIG[\"OUTPUT_DIR\"],\n",
    "            logging_dir=CONFIG[\"LOGS_DIR\"],\n",
    "            **TRAINING_CONFIG,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_steps=100,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\"\n",
    "        )\n",
    "        \n",
    "        # Initialize callbacks\n",
    "        metrics_logger = MetricsLoggerCallback()\n",
    "        early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "        \n",
    "        # Setup trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[early_stopping, metrics_logger]\n",
    "        )\n",
    "        \n",
    "        # Train and evaluate\n",
    "        logger.info(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        eval_results = trainer.evaluate()\n",
    "        logger.info(\"Training completed. Computing final metrics...\")\n",
    "        \n",
    "        # Calculate and display average metrics\n",
    "        avg_metrics = {\n",
    "            metric: sum(values) / len(values)\n",
    "            for metric, values in metrics_logger.metrics_history.items()\n",
    "        }\n",
    "        \n",
    "        print(\"\\nAverage Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        for metric, value in avg_metrics.items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "        \n",
    "        return eval_results, avg_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
