{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import EarlyStoppingCallback\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "DATA_PATH = \"/root/workspace/npe_project/Dataset/NPEPatches.json\"\n",
    "MODEL_PATH = \"microsoft/codereviewer\"\n",
    "OUTPUT_DIR = f\"./results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.best_metrics = {}  # Initialize as empty dict instead of None\n",
    "        self.current_best_f1 = 0.0\n",
    "\n",
    "    def update(self, metrics: Dict[str, Any]) -> None:\n",
    "        try:\n",
    "            processed_metrics = {k: float(v) if isinstance(v, (np.float64, float)) else v \n",
    "                               for k, v in metrics.items()}\n",
    "            \n",
    "            self.history.append(processed_metrics)\n",
    "            \n",
    "            current_f1 = processed_metrics.get('eval_f1_score', 0.0)\n",
    "            if current_f1 > self.current_best_f1:\n",
    "                self.current_best_f1 = current_f1\n",
    "                self.best_metrics = processed_metrics\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in metrics update: {e}\")\n",
    "\n",
    "    def get_metrics_report(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'history': self.history,\n",
    "            'best_metrics': self.best_metrics if self.best_metrics else {},\n",
    "            'final_metrics': self.history[-1] if self.history else {}\n",
    "        }\n",
    "\n",
    "class NPEDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    try:\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1).flatten()\n",
    "        labels = labels.flatten()\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
    "        \n",
    "        metrics = {\n",
    "            'eval_accuracy': float(accuracy_score(labels, predictions)),\n",
    "            'eval_precision': float(precision_score(labels, predictions)),\n",
    "            'eval_recall': float(recall_score(labels, predictions)),\n",
    "            'eval_f1_score': float(f1_score(labels, predictions)),\n",
    "            'eval_fpr': float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0,\n",
    "            'eval_fnr': float(fn / (fn + tp)) if (fn + tp) > 0 else 0.0\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in compute_metrics: {e}\")\n",
    "        return {\n",
    "            'eval_accuracy': 0.0,\n",
    "            'eval_precision': 0.0,\n",
    "            'eval_recall': 0.0,\n",
    "            'eval_f1_score': 0.0,\n",
    "            'eval_fpr': 0.0,\n",
    "            'eval_fnr': 0.0\n",
    "        }\n",
    "\n",
    "class MetricsCallback(EarlyStoppingCallback):\n",
    "    def __init__(self, metrics_tracker, early_stopping_patience=2):\n",
    "        super().__init__(early_stopping_patience=early_stopping_patience)\n",
    "        self.metrics_tracker = metrics_tracker\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            self.metrics_tracker.update(metrics)\n",
    "        super().on_evaluate(args, state, control, metrics, **kwargs)\n",
    "\n",
    "def save_results(output_dir: str, metrics_tracker: MetricsTracker, final_results: Dict[str, Any]):\n",
    "    try:\n",
    "        # Save detailed metrics report\n",
    "        report_path = os.path.join(output_dir, 'metrics_report.txt')\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"=== Final Evaluation Metrics ===\\n\\n\")\n",
    "            for metric, value in final_results.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "\n",
    "            if metrics_tracker.best_metrics:\n",
    "                f.write(\"\\n=== Best Metrics During Training ===\\n\\n\")\n",
    "                for metric, value in metrics_tracker.best_metrics.items():\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "\n",
    "        # Save JSON results\n",
    "        results = metrics_tracker.get_metrics_report()\n",
    "        with open(os.path.join(output_dir, 'results.json'), 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {e}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        metrics_tracker = MetricsTracker()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        logger.info(\"Loading dataset...\")\n",
    "        data = pd.read_csv(DATA_PATH, encoding='latin1')\n",
    "        data['Patch'] = data['Patch'].fillna('').astype(str)\n",
    "        data = data.drop_duplicates(subset=['Patch'])\n",
    "        \n",
    "        label_mapping = {'NPE-Fixes': 1, 'Not-NPE': 0}\n",
    "        data['Category'] = data['Category'].map(label_mapping)\n",
    "\n",
    "        X = data['Patch'].values\n",
    "        y = data['Category'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            num_labels=2\n",
    "        )\n",
    "\n",
    "        train_dataset = NPEDataset(X_train, y_train, tokenizer)\n",
    "        test_dataset = NPEDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            num_train_epochs=5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_f1_score\",\n",
    "            greater_is_better=True,\n",
    "            learning_rate=3e-5,\n",
    "            logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "            logging_steps=100,\n",
    "            save_total_limit=2,\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[MetricsCallback(metrics_tracker, early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        final_results = trainer.evaluate()\n",
    "        \n",
    "        # Save results and model\n",
    "        save_results(OUTPUT_DIR, metrics_tracker, final_results)\n",
    "        trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "        logger.info(f\"Training completed. Results saved to {OUTPUT_DIR}\")\n",
    "        \n",
    "        # Display final metrics\n",
    "        print(\"\\nFinal Evaluation Metrics:\")\n",
    "        for metric, value in final_results.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
